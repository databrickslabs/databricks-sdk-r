[{"path":[]},{"path":"https://databrickslabs.github.io/databricks-sdk-r/CHANGELOG.html","id":"id_040","dir":"","previous_headings":"","what":"0.4.0","title":"Version changelog","text":"Regenerated latest OpenAPI spec (#29). API Changes: Changed catalogsList() function require request . Changed connectionsCreate() function new required argument order. Changed connectionsDelete() function new required argument order. Changed connectionsGet() function new required argument order. Changed connectionsUpdate() function new required argument order. Changed externalLocationsList() function require request . Changed functionsCreate() function . New request type . Changed metastoresCreate() function new required argument order. Removed metastoresEnableOptimization() function. Changed storageCredentialsList() function require request . Added tablesExists() function. Changed volumesCreate() function new required argument order. Changed volumesDelete() function new required argument order. Changed volumesRead() function new required argument order. Changed volumesUpdate() function new required argument order. Added workspaceBindingsGetBindings() function. Added workspaceBindingsUpdateBindings() function. Removed clusterPoliciesGetClusterPolicyPermissionLevels() function. Removed clusterPoliciesGetClusterPolicyPermissions() function. Removed clusterPoliciesSetClusterPolicyPermissions() function. Removed clusterPoliciesUpdateClusterPolicyPermissions() function. Added clusterPoliciesGetPermissionLevels() function. Added clusterPoliciesGetPermissions() function. Added clusterPoliciesSetPermissions() function. Added clusterPoliciesUpdatePermissions() function. Removed clustersGetClusterPermissionLevels() function. Removed clustersGetClusterPermissions() function. Removed clustersSetClusterPermissions() function. Removed clustersUpdateClusterPermissions() function. Added clustersGetPermissionLevels() function. Added clustersGetPermissions() function. Added clustersSetPermissions() function. Added clustersUpdatePermissions() function. Removed instancePoolsGetInstancePoolPermissionLevels() function. Removed instancePoolsGetInstancePoolPermissions() function. Removed instancePoolsSetInstancePoolPermissions() function. Removed instancePoolsUpdateInstancePoolPermissions() function. Added instancePoolsGetPermissionLevels() function. Added instancePoolsGetPermissions() function. Added instancePoolsSetPermissions() function. Added instancePoolsUpdatePermissions() function. Removed usersGetPasswordPermissionLevels() function. Removed usersGetPasswordPermissions() function. Removed usersSetPasswordPermissions() function. Removed usersUpdatePasswordPermissions() function. Added usersGetPermissionLevels() function. Added usersGetPermissions() function. Added usersSetPermissions() function. Added usersUpdatePermissions() function. Changed jobsCancelAllRuns() function new required argument order. Removed jobsGetJobPermissionLevels() function. Removed jobsGetJobPermissions() function. Removed jobsSetJobPermissions() function. Removed jobsUpdateJobPermissions() function. Added jobsGetPermissionLevels() function. Added jobsGetPermissions() function. Added jobsSetPermissions() function. Added jobsUpdatePermissions() function. Changed experimentsGetByName() function return . Changed experimentsGetExperiment() function return . Removed experimentsGetExperimentPermissionLevels() function. Removed experimentsGetExperimentPermissions() function. Removed experimentsSetExperimentPermissions() function. Removed experimentsUpdateExperimentPermissions() function. Added experimentsDeleteRuns() function. Added experimentsGetPermissionLevels() function. Added experimentsGetPermissions() function. Added experimentsRestoreRuns() function. Added experimentsSetPermissions() function. Added experimentsUpdatePermissions() function. Removed modelRegistryGetRegisteredModelPermissionLevels() function. Removed modelRegistryGetRegisteredModelPermissions() function. Removed modelRegistrySetRegisteredModelPermissions() function. Removed modelRegistryUpdateRegisteredModelPermissions() function. Added modelRegistryGetPermissionLevels() function. Added modelRegistryGetPermissions() function. Added modelRegistrySetPermissions() function. Added modelRegistryUpdatePermissions() function. Removed pipelinesGetPipelinePermissionLevels() function. Removed pipelinesGetPipelinePermissions() function. Removed pipelinesReset() function. Removed pipelinesSetPipelinePermissions() function. Removed pipelinesUpdatePipelinePermissions() function. Added pipelinesGetPermissionLevels() function. Added pipelinesGetPermissions() function. Added pipelinesSetPermissions() function. Added pipelinesUpdatePermissions() function. Removed servingEndpointsGetServingEndpointPermissionLevels() function. Removed servingEndpointsGetServingEndpointPermissions() function. Changed servingEndpointsQuery() function . New request type . Removed servingEndpointsSetServingEndpointPermissions() function. Changed servingEndpointsUpdateConfig() function new required argument order. Removed servingEndpointsUpdateServingEndpointPermissions() function. Added servingEndpointsGetPermissionLevels() function. Added servingEndpointsGetPermissions() function. Added servingEndpointsPatch() function. Added servingEndpointsPut() function. Added servingEndpointsSetPermissions() function. Added servingEndpointsUpdatePermissions() function. Changed ipAccessListsCreate() function new required argument order. Changed ipAccessListsGet() function . New request type . Changed ipAccessListsList() function return . Changed ipAccessListsReplace() function new required argument order. Changed ipAccessListsUpdate() function new required argument order. Changed tokenManagementCreateOboToken() function new required argument order. Changed tokenManagementGet() function return . Removed tokenManagementGetTokenPermissionLevels() function. Removed tokenManagementGetTokenPermissions() function. Removed tokenManagementSetTokenPermissions() function. Removed tokenManagementUpdateTokenPermissions() function. Added tokenManagementGetPermissionLevels() function. Added tokenManagementGetPermissions() function. Added tokenManagementSetPermissions() function. Added tokenManagementUpdatePermissions() function. Changed tokensList() function return . Changed cleanRoomsDelete() function new required argument order. Changed cleanRoomsGet() function new required argument order. Changed cleanRoomsList() function require request . Changed cleanRoomsUpdate() function new required argument order. Changed dashboardsCreate() function . New request type . Added dashboardsUpdate() function. Changed statementExecutionExecuteStatement() function new required argument order. Removed warehousesGetWarehousePermissionLevels() function. Removed warehousesGetWarehousePermissions() function. Removed warehousesSetWarehousePermissions() function. Removed warehousesUpdateWarehousePermissions() function. Added warehousesGetPermissionLevels() function. Added warehousesGetPermissions() function. Added warehousesSetPermissions() function. Added warehousesUpdatePermissions() function. Removed reposGetRepoPermissionLevels() function. Removed reposGetRepoPermissions() function. Removed reposSetRepoPermissions() function. Removed reposUpdateRepoPermissions() function. Added reposGetPermissionLevels() function. Added reposGetPermissions() function. Added reposSetPermissions() function. Added reposUpdatePermissions() function. Added secretsGetSecret() function. Removed workspaceGetWorkspaceObjectPermissionLevels() function. Removed workspaceGetWorkspaceObjectPermissions() function. Removed workspaceSetWorkspaceObjectPermissions() function. Removed workspaceUpdateWorkspaceObjectPermissions() function. Added workspaceGetPermissionLevels() function. Added workspaceGetPermissions() function. Added workspaceSetPermissions() function. Added workspaceUpdatePermissions() function. OpenAPI SHA: 93763b0d7ae908520c229c786fff28b8fd623261, Date: 2024-03-20","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/CHANGELOG.html","id":"id_031","dir":"","previous_headings":"","what":"0.3.1","title":"Version changelog","text":"Added tokei.rs badge (#27). Fix handling bare DATABRICKS_HOST URLs. (#25).","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/CHANGELOG.html","id":"id_030","dir":"","previous_headings":"","what":"0.3.0","title":"Version changelog","text":"Added github pages (#19). Added pkgdown website (#20). API Changes: Added clusterPoliciesClusterPolicyId() function. Added clusterPoliciesPermissionLevels() function. Added clustersClusterId() function. Added clustersPermissionLevels() function. Added instancePoolsInstancePoolId() function. Added instancePoolsPermissionLevels() function. Changed permissionsSet() function start returning . Changed permissionsUpdate() function start returning . Added usersGetPasswordPermissionLevels() function. Added usersGetPasswordPermissions() function. Added usersSetPasswordPermissions() function. Added usersUpdatePasswordPermissions() function. Added jobsGetJobPermissionLevels() function. Added jobsGetJobPermissions() function. Added jobsSetJobPermissions() function. Added jobsUpdateJobPermissions() function. Added experimentsGetExperimentPermissionLevels() function. Added experimentsGetExperimentPermissions() function. Added experimentsSetExperimentPermissions() function. Added experimentsUpdateExperimentPermissions() function. Added modelRegistryGetRegisteredModelPermissionLevels() function. Added modelRegistryGetRegisteredModelPermissions() function. Added modelRegistrySetRegisteredModelPermissions() function. Added modelRegistryUpdateRegisteredModelPermissions() function. Added pipelinesGetPipelinePermissionLevels() function. Added pipelinesGetPipelinePermissions() function. Added pipelinesSetPipelinePermissions() function. Added pipelinesUpdatePipelinePermissions() function. Added servingEndpointsGetServingEndpointPermissionLevels() function. Added servingEndpointsGetServingEndpointPermissions() function. Added servingEndpointsSetServingEndpointPermissions() function. Added servingEndpointsUpdateServingEndpointPermissions() function. Added tokenManagementGetTokenPermissionLevels() function. Added tokenManagementGetTokenPermissions() function. Added tokenManagementSetTokenPermissions() function. Added tokenManagementUpdateTokenPermissions() function. Added warehousesGetWarehousePermissionLevels() function. Added warehousesGetWarehousePermissions() function. Added warehousesSetWarehousePermissions() function. Added warehousesUpdateWarehousePermissions() function. Added reposGetRepoPermissionLevels() function. Added reposGetRepoPermissions() function. Added reposSetRepoPermissions() function. Added reposUpdateRepoPermissions() function. Added workspacePermissionLevels() function. Added workspaceWorkspaceObjectId() function. OpenAPI SHA: ae082ae8b1bcc0bd41468e5f07810054e05b3dc7, Date: 2023-08-01","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/CHANGELOG.html","id":"id_020","dir":"","previous_headings":"","what":"0.2.0","title":"Version changelog","text":"Adding @export Roxygen, improving visibility (#16). API Changes: OpenAPI SHA: 2ff01e4fb3c2799518dfaff00c986f6737a4a742, Date: 2023-07-19","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/CHANGELOG.html","id":"id_010","dir":"","previous_headings":"","what":"0.1.0","title":"Version changelog","text":"Work towards getting R CMD check passing (#4). Drop renv (#11). Drop @include (#12). Embrace R namespaces (#13). Fix .NeedsOffsetDedupe error (#14). API Changes: Removed metastoresMaintenance() function. Added metastoresEnableOptimization() function. Added tablesUpdate() function. Changed clustersGet() function return . OpenAPI SHA: 0a1949ba96f71680dad30e06973eaae85b1307bb, Date: 2023-07-18","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/CHANGELOG.html","id":"id_001","dir":"","previous_headings":"","what":"0.0.1","title":"Version changelog","text":"Initial rollout","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/databricks-sdk-r/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Developing SDK","title":"Developing SDK","text":"Please read Writing R Extensions manual case difficulties make check.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/CONTRIBUTING.html","id":"installing-tools","dir":"","previous_headings":"","what":"Installing tools","title":"Developing SDK","text":"Start make deps ’re running clean checkout. set dev workstation, install devtools package","code":"pip3 install -U radian  brew install r fribidi libgit2 install.packages(\"devtools\")"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/CONTRIBUTING.html","id":"devloop","dir":"","previous_headings":"","what":"Devloop","title":"Developing SDK","text":"console, execute get error #include <fribidi.h>, brew install fribidi. HTTP client devloop:","code":"library(devtools); load_all(\".\"); load_all(); DatabricksClient(profile = 'demo')$do('GET', '/api/2.0/scim/Me')"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/CONTRIBUTING.html","id":"debugging","dir":"","previous_headings":"Devloop","what":"Debugging","title":"Developing SDK","text":"insert browser() whenever necessary use Q quit interactiv debugging see https://adv-r.hadley.nz/debugging.html","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Serge Smertin. Author, maintainer. Databricks. Copyright holder, funder.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Smertin S (2024). databricks: Databricks SDK R (Experimental). R package version 0.4.2, https://databrickslabs.github.io/databricks-sdk-r/.","code":"@Manual{,   title = {databricks: Databricks SDK for R (Experimental)},   author = {Serge Smertin},   year = {2024},   note = {R package version 0.4.2},   url = {https://databrickslabs.github.io/databricks-sdk-r/}, }"},{"path":[]},{"path":"https://databrickslabs.github.io/databricks-sdk-r/index.html","id":"authentication","dir":"","previous_headings":"","what":"Authentication","title":"Databricks SDK for R (Experimental)","text":"’s recommended authenticate via .Renviron file using DATABRICKS_HOST DATABRICKS_TOKEN environment variables. can also use Databricks CLI Configuration Profiles DATABRICKS_CONFIG_FILE DATABRICKS_CONFIG_PROFILE environment variables, PAT Authentication works moment. need authentication methods, please fork GitHub repository send pull request feature suggestion. Example overriding authentication profile. Look databricks auth profiles know ones working.","code":"client <- DatabricksClient(profile=\"your-cli-profile\")"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/index.html","id":"complete-with-databricks-workspace-level-apis","dir":"","previous_headings":"","what":"Complete with Databricks workspace-level APIs","title":"Databricks SDK for R (Experimental)","text":"Databricks SDK R comes public workspace-level API consistent Databricks SDK Python, Databricks SDK Go, Databricks SDK Java. Databricks SDK R expose account-level API ’re recommended use Go, Python, Java SDK build account-level automation.","code":"library(dplyr) library(databricks) client <- DatabricksClient() running <- list_clusters(client) %>% filter(state == 'RUNNING') context <- create_command_execution_and_wait(client, cluster_id=running$cluster_id, language='python') res <- execute_command_and_wait(client, cluster_id=running$cluster_id, context_id=context$id, language='sql', command='show tables') res"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/index.html","id":"pagination","dir":"","previous_headings":"","what":"Pagination","title":"Databricks SDK for R (Experimental)","text":"list methods (, return list results), consistently return data.frame entries pages, regardless underlying implementation.","code":"list_clusters(client)[1:10,c(\"cluster_id\", \"cluster_name\", \"state\")] #              cluster_id                                      cluster_name      state # 1  1109-110110-kjfoeopq                              DEFAULT Test Cluster TERMINATED # 2  0110-221212-oqqpodoa                               GO_SDK Test Cluster TERMINATED # 3  1109-012301-qlwlwqpq                               BRICKS Test Cluster TERMINATED # 4  1109-110012-qpwoepqq                               VSCODE Test Cluster TERMINATED # 5  0110-201022-oqooqpqp                               JS_SDK Test Cluster TERMINATED"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/index.html","id":"long-running-operations","dir":"","previous_headings":"","what":"Long-running operations","title":"Databricks SDK for R (Experimental)","text":"long-running operations poll Databricks backend entity reaches desired state:","code":"create_cluster_and_wait(client, spark_version = \"12.x-snapshot-scala2.12\", cluster_name = \"r-sdk-cluster\", num_workers = 1, autotermination_minutes=20, node_type_id=\"i3.xlarge\") # PENDING: Finding instances for new nodes, acquiring more instances if necessary"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/index.html","id":"interface-stability","dir":"","previous_headings":"","what":"Interface stability","title":"Databricks SDK for R (Experimental)","text":"API clients services generated specification files synchronized main platform. Databricks may minor documented backward-incompatible changes, renaming methods type names bring consistency.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/index.html","id":"project-support","dir":"","previous_headings":"","what":"Project Support","title":"Databricks SDK for R (Experimental)","text":"Please note projects databrickslabs github space provided exploration , formally supported Databricks Service Level Agreements (SLAs). provided -make guarantees kind. Please submit support ticket relating issues arising use projects. issues discovered use project filed GitHub Issues Repo. reviewed time permits, formal SLAs support.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/DatabricksClient.html","id":null,"dir":"Reference","previous_headings":"","what":"DatabricksClient is a constructor for class that performs any operations with\nDatabricks REST API and handle a subset of Unified Client Authentication. — DatabricksClient","title":"DatabricksClient is a constructor for class that performs any operations with\nDatabricks REST API and handle a subset of Unified Client Authentication. — DatabricksClient","text":"DatabricksClient constructor class performs operations Databricks REST API handle subset Unified Client Authentication.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/DatabricksClient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DatabricksClient is a constructor for class that performs any operations with\nDatabricks REST API and handle a subset of Unified Client Authentication. — DatabricksClient","text":"","code":"DatabricksClient(profile = NULL, host = NULL, token = NULL, config_file = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/DatabricksClient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DatabricksClient is a constructor for class that performs any operations with\nDatabricks REST API and handle a subset of Unified Client Authentication. — DatabricksClient","text":"profile configuration profile ~/.databrickscfg. Defaults DEFAULT host URL Databricks Workspace token Personal Access Token config_file path Databricks CLI configuration file. Defaults ~/.databrickscfg","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/add_dbfs_block.html","id":null,"dir":"Reference","previous_headings":"","what":"Append data block. — add_dbfs_block","title":"Append data block. — add_dbfs_block","text":"Appends block data stream specified input handle. handle exist, call throw exception RESOURCE_DOES_NOT_EXIST.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/add_dbfs_block.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Append data block. — add_dbfs_block","text":"","code":"add_dbfs_block(client, handle, data)  dbfsAddBlock(client, handle, data)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/add_dbfs_block.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Append data block. — add_dbfs_block","text":"client Required. Instance DatabricksClient() handle Required. handle open stream. data Required. base64-encoded data append stream.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/add_dbfs_block.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Append data block. — add_dbfs_block","text":"block data exceeds 1 MB, call throw exception MAX_BLOCK_SIZE_EXCEEDED.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/add_instance_profile.html","id":null,"dir":"Reference","previous_headings":"","what":"Register an instance profile. — add_instance_profile","title":"Register an instance profile. — add_instance_profile","text":"UI, can select instance profile launching clusters. API available admin users.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/add_instance_profile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register an instance profile. — add_instance_profile","text":"","code":"add_instance_profile(   client,   instance_profile_arn,   iam_role_arn = NULL,   is_meta_instance_profile = NULL,   skip_validation = NULL )  instanceProfilesAdd(   client,   instance_profile_arn,   iam_role_arn = NULL,   is_meta_instance_profile = NULL,   skip_validation = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/add_instance_profile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register an instance profile. — add_instance_profile","text":"client Required. Instance DatabricksClient() instance_profile_arn Required. AWS ARN instance profile register Databricks. iam_role_arn AWS IAM role ARN role associated instance profile. is_meta_instance_profile Boolean flag indicating whether instance profile used credential passthrough scenarios. skip_validation default, Databricks validates sufficient permissions launch instances instance profile.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/all_cluster_library_statuses.html","id":null,"dir":"Reference","previous_headings":"","what":"Get all statuses. — all_cluster_library_statuses","title":"Get all statuses. — all_cluster_library_statuses","text":"Get status libraries clusters. status available libraries installed cluster via API libraries UI well libraries set installed clusters via libraries UI.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/all_cluster_library_statuses.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get all statuses. — all_cluster_library_statuses","text":"","code":"all_cluster_library_statuses(client)  librariesAllClusterStatuses(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/all_cluster_library_statuses.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get all statuses. — all_cluster_library_statuses","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/approve_model_transition_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Approve transition request. — approve_model_transition_request","title":"Approve transition request. — approve_model_transition_request","text":"Approves model version stage transition request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/approve_model_transition_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Approve transition request. — approve_model_transition_request","text":"","code":"approve_model_transition_request(   client,   name,   version,   stage,   archive_existing_versions,   comment = NULL )  modelRegistryApproveTransitionRequest(   client,   name,   version,   stage,   archive_existing_versions,   comment = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/approve_model_transition_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Approve transition request. — approve_model_transition_request","text":"client Required. Instance DatabricksClient() name Required. Name model. version Required. Version model. stage Required. Target stage transition. archive_existing_versions Required. Specifies whether archive current model versions target stage. comment User-provided comment action.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/assign_metastore.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an assignment. — assign_metastore","title":"Create an assignment. — assign_metastore","text":"Creates new metastore assignment. assignment workspace_id exists, overwritten new metastore_id default_catalog_name. caller must account admin.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/assign_metastore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an assignment. — assign_metastore","text":"","code":"assign_metastore(client, workspace_id, metastore_id, default_catalog_name)  metastoresAssign(client, workspace_id, metastore_id, default_catalog_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/assign_metastore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an assignment. — assign_metastore","text":"client Required. Instance DatabricksClient() workspace_id Required. workspace ID. metastore_id Required. unique ID metastore. default_catalog_name Required. name default catalog metastore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/build_serving_endpoint_logs.html","id":null,"dir":"Reference","previous_headings":"","what":"Get build logs for a served model. — build_serving_endpoint_logs","title":"Get build logs for a served model. — build_serving_endpoint_logs","text":"Retrieves build logs associated provided served model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/build_serving_endpoint_logs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get build logs for a served model. — build_serving_endpoint_logs","text":"","code":"build_serving_endpoint_logs(client, name, served_model_name)  servingEndpointsBuildLogs(client, name, served_model_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/build_serving_endpoint_logs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get build logs for a served model. — build_serving_endpoint_logs","text":"client Required. Instance DatabricksClient() name Required. name serving endpoint served model belongs . served_model_name Required. name served model build logs retrieved .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_command_execution.html","id":null,"dir":"Reference","previous_headings":"","what":"Cancel a command. — cancel_command_execution","title":"Cancel a command. — cancel_command_execution","text":"Cancels currently running command within execution context.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_command_execution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cancel a command. — cancel_command_execution","text":"","code":"cancel_command_execution(   client,   cluster_id = NULL,   command_id = NULL,   context_id = NULL )  commandExecutionCancel(   client,   cluster_id = NULL,   command_id = NULL,   context_id = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_command_execution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cancel a command. — cancel_command_execution","text":"client Required. Instance DatabricksClient() cluster_id field description yet. command_id field description yet. context_id field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_command_execution.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cancel a command. — cancel_command_execution","text":"command ID obtained prior successful call execute.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_command_execution_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Cancel a command. — cancel_command_execution_and_wait","title":"Cancel a command. — cancel_command_execution_and_wait","text":"long-running operation, blocks Command Execution Databricks reach Cancelled state timeout 20 minutes, can change via timeout parameter. default, state Databricks Command Execution reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_command_execution_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cancel a command. — cancel_command_execution_and_wait","text":"","code":"cancel_command_execution_and_wait(   client,   cluster_id = NULL,   command_id = NULL,   context_id = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_command_execution_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cancel a command. — cancel_command_execution_and_wait","text":"client Required. Instance DatabricksClient() cluster_id field description yet. command_id field description yet. context_id field description yet. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_command_execution_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cancel a command. — cancel_command_execution_and_wait","text":"Cancels currently running command within execution context. command ID obtained prior successful call execute.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_job_all_runs.html","id":null,"dir":"Reference","previous_headings":"","what":"Cancel all runs of a job. — cancel_job_all_runs","title":"Cancel all runs of a job. — cancel_job_all_runs","text":"Cancels active runs job. runs canceled asynchronously, prevent new runs started.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_job_all_runs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cancel all runs of a job. — cancel_job_all_runs","text":"","code":"cancel_job_all_runs(client, all_queued_runs = NULL, job_id = NULL)  jobsCancelAllRuns(client, all_queued_runs = NULL, job_id = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_job_all_runs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cancel all runs of a job. — cancel_job_all_runs","text":"client Required. Instance DatabricksClient() all_queued_runs Optional boolean parameter cancel queued runs. job_id canonical identifier job cancel runs .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_job_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Cancel a run. — cancel_job_run","title":"Cancel a run. — cancel_job_run","text":"Cancels job run task run. run canceled asynchronously, may still running request completes.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_job_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cancel a run. — cancel_job_run","text":"","code":"cancel_job_run(client, run_id)  jobsCancelRun(client, run_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_job_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cancel a run. — cancel_job_run","text":"client Required. Instance DatabricksClient() run_id Required. field required.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_job_run_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Cancel a run. — cancel_job_run_and_wait","title":"Cancel a run. — cancel_job_run_and_wait","text":"long-running operation, blocks Jobs Databricks reach TERMINATED SKIPPED state timeout 20 minutes, can change via timeout parameter. default, state Databricks Jobs reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_job_run_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cancel a run. — cancel_job_run_and_wait","text":"","code":"cancel_job_run_and_wait(client, run_id, timeout = 20, callback = cli_reporter)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_job_run_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cancel a run. — cancel_job_run_and_wait","text":"client Required. Instance DatabricksClient() run_id Required. field required. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_job_run_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cancel a run. — cancel_job_run_and_wait","text":"Cancels job run task run. run canceled asynchronously, may still running request completes.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_lakehouse_monitor_refresh.html","id":null,"dir":"Reference","previous_headings":"","what":"Cancel refresh. — cancel_lakehouse_monitor_refresh","title":"Cancel refresh. — cancel_lakehouse_monitor_refresh","text":"Cancel active monitor refresh given refresh ID.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_lakehouse_monitor_refresh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cancel refresh. — cancel_lakehouse_monitor_refresh","text":"","code":"cancel_lakehouse_monitor_refresh(client, full_name, refresh_id)  lakehouseMonitorsCancelRefresh(client, full_name, refresh_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_lakehouse_monitor_refresh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cancel refresh. — cancel_lakehouse_monitor_refresh","text":"client Required. Instance DatabricksClient() full_name Required. Full name table. refresh_id Required. ID refresh.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_lakehouse_monitor_refresh.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cancel refresh. — cancel_lakehouse_monitor_refresh","text":"caller must either: 1. owner table's parent catalog 2. USE_CATALOG table's parent catalog owner table's parent schema 3. following permissions: - USE_CATALOG table's parent catalog - USE_SCHEMA table's parent schema - owner table Additionally, call must made workspace monitor created.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_statement_execution.html","id":null,"dir":"Reference","previous_headings":"","what":"Cancel statement execution. — cancel_statement_execution","title":"Cancel statement execution. — cancel_statement_execution","text":"Requests executing statement canceled. Callers must poll status see terminal state.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_statement_execution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cancel statement execution. — cancel_statement_execution","text":"","code":"cancel_statement_execution(client, statement_id)  statementExecutionCancelExecution(client, statement_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cancel_statement_execution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cancel statement execution. — cancel_statement_execution","text":"client Required. Instance DatabricksClient() statement_id Required. statement ID returned upon successfully submitting SQL statement, required reference subsequent calls.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/change_cluster_owner.html","id":null,"dir":"Reference","previous_headings":"","what":"Change cluster owner. — change_cluster_owner","title":"Change cluster owner. — change_cluster_owner","text":"Change owner cluster. must admin cluster must terminated perform operation. service principal application ID can supplied argument owner_username.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/change_cluster_owner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Change cluster owner. — change_cluster_owner","text":"","code":"change_cluster_owner(client, cluster_id, owner_username)  clustersChangeOwner(client, cluster_id, owner_username)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/change_cluster_owner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Change cluster owner. — change_cluster_owner","text":"client Required. Instance DatabricksClient() cluster_id Required. needs content added. owner_username Required. New owner cluster_id RPC.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/close_dbfs.html","id":null,"dir":"Reference","previous_headings":"","what":"Close the stream. — close_dbfs","title":"Close the stream. — close_dbfs","text":"Closes stream specified input handle. handle exist, call throws exception RESOURCE_DOES_NOT_EXIST.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/close_dbfs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Close the stream. — close_dbfs","text":"","code":"close_dbfs(client, handle)  dbfsClose(client, handle)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/close_dbfs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Close the stream. — close_dbfs","text":"client Required. Instance DatabricksClient() handle Required. handle open stream.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cluster_library_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Get status. — cluster_library_status","title":"Get status. — cluster_library_status","text":"Get status libraries cluster. status available libraries installed cluster via API libraries UI well libraries set installed clusters via libraries UI. order returned libraries follows.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cluster_library_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get status. — cluster_library_status","text":"","code":"cluster_library_status(client, cluster_id)  librariesClusterStatus(client, cluster_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cluster_library_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get status. — cluster_library_status","text":"client Required. Instance DatabricksClient() cluster_id Required. Unique identifier cluster whose status retrieved.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cluster_library_status.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get status. — cluster_library_status","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/cluster_library_status.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get status. — cluster_library_status","text":"Libraries set installed cluster returned first. Within group, final order order libraries added cluster. Libraries set installed clusters returned next. Within group order guarantee. Libraries previously requested cluster clusters, now marked removal. Within group order guarantee.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/command_execution_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Get command info. — command_execution_status","title":"Get command info. — command_execution_status","text":"Gets status , available, results currently executing command.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/command_execution_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get command info. — command_execution_status","text":"","code":"command_execution_status(client, cluster_id, context_id, command_id)  commandExecutionCommandStatus(client, cluster_id, context_id, command_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/command_execution_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get command info. — command_execution_status","text":"client Required. Instance DatabricksClient() cluster_id Required. field description yet. context_id Required. field description yet. command_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/command_execution_status.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get command info. — command_execution_status","text":"command ID obtained prior successful call execute.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/context_command_execution_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Get status. — context_command_execution_status","title":"Get status. — context_command_execution_status","text":"Gets status execution context.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/context_command_execution_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get status. — context_command_execution_status","text":"","code":"context_command_execution_status(client, cluster_id, context_id)  commandExecutionContextStatus(client, cluster_id, context_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/context_command_execution_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get status. — context_command_execution_status","text":"client Required. Instance DatabricksClient() cluster_id Required. field description yet. context_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_alert.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an alert. — create_alert","title":"Create an alert. — create_alert","text":"Creates alert. alert Databricks SQL object periodically runs query, evaluates condition result, notifies users notification destinations condition met.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_alert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an alert. — create_alert","text":"","code":"create_alert(client, name, options, query_id, parent = NULL, rearm = NULL)  alertsCreate(client, name, options, query_id, parent = NULL, rearm = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_alert.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an alert. — create_alert","text":"client Required. Instance DatabricksClient() name Required. Name alert. options Required. Alert configuration options. query_id Required. Query ID. parent identifier workspace folder containing object. rearm Number seconds triggered alert rearms can triggered .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_app.html","id":null,"dir":"Reference","previous_headings":"","what":"Create and deploy an application. — create_app","title":"Create and deploy an application. — create_app","text":"Creates deploys application.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_app.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create and deploy an application. — create_app","text":"","code":"create_app(client, manifest, resources = NULL)  appsCreate(client, manifest, resources = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_app.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create and deploy an application. — create_app","text":"client Required. Instance DatabricksClient() manifest Required. Manifest specifies application requirements. resources Information passed app deployment time fulfill app dependencies.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_catalog.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a catalog. — create_catalog","title":"Create a catalog. — create_catalog","text":"Creates new catalog instance parent metastore caller metastore admin CREATE_CATALOG privilege.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_catalog.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a catalog. — create_catalog","text":"","code":"create_catalog(   client,   name,   comment = NULL,   connection_name = NULL,   options = NULL,   properties = NULL,   provider_name = NULL,   share_name = NULL,   storage_root = NULL )  catalogsCreate(   client,   name,   comment = NULL,   connection_name = NULL,   options = NULL,   properties = NULL,   provider_name = NULL,   share_name = NULL,   storage_root = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_catalog.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a catalog. — create_catalog","text":"client Required. Instance DatabricksClient() name Required. Name catalog. comment User-provided free-form text description. connection_name name connection external data source. options map key-value properties attached securable. properties map key-value properties attached securable. provider_name name delta sharing provider. share_name name share share provider. storage_root Storage root URL managed tables within catalog.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_clean_room.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a clean room. — create_clean_room","title":"Create a clean room. — create_clean_room","text":"Creates new clean room specified colaborators. caller must metastore admin CREATE_CLEAN_ROOM privilege metastore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_clean_room.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a clean room. — create_clean_room","text":"","code":"create_clean_room(client, name, remote_detailed_info, comment = NULL)  cleanRoomsCreate(client, name, remote_detailed_info, comment = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_clean_room.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a clean room. — create_clean_room","text":"client Required. Instance DatabricksClient() name Required. Name clean room. remote_detailed_info Required. Central clean room details. comment User-provided free-form text description.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Create new cluster. — create_cluster","title":"Create new cluster. — create_cluster","text":"Creates new Spark cluster. method acquire new instances cloud provider necessary. Note: Databricks may able acquire requested nodes, due cloud provider limitations (account limits, spot price, etc.) transient network issues.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create new cluster. — create_cluster","text":"","code":"create_cluster(   client,   spark_version,   apply_policy_default_values = NULL,   autoscale = NULL,   autotermination_minutes = NULL,   aws_attributes = NULL,   azure_attributes = NULL,   cluster_log_conf = NULL,   cluster_name = NULL,   cluster_source = NULL,   custom_tags = NULL,   data_security_mode = NULL,   docker_image = NULL,   driver_instance_pool_id = NULL,   driver_node_type_id = NULL,   enable_elastic_disk = NULL,   enable_local_disk_encryption = NULL,   gcp_attributes = NULL,   init_scripts = NULL,   instance_pool_id = NULL,   node_type_id = NULL,   num_workers = NULL,   policy_id = NULL,   runtime_engine = NULL,   single_user_name = NULL,   spark_conf = NULL,   spark_env_vars = NULL,   ssh_public_keys = NULL,   workload_type = NULL )  clustersCreate(   client,   spark_version,   apply_policy_default_values = NULL,   autoscale = NULL,   autotermination_minutes = NULL,   aws_attributes = NULL,   azure_attributes = NULL,   cluster_log_conf = NULL,   cluster_name = NULL,   cluster_source = NULL,   custom_tags = NULL,   data_security_mode = NULL,   docker_image = NULL,   driver_instance_pool_id = NULL,   driver_node_type_id = NULL,   enable_elastic_disk = NULL,   enable_local_disk_encryption = NULL,   gcp_attributes = NULL,   init_scripts = NULL,   instance_pool_id = NULL,   node_type_id = NULL,   num_workers = NULL,   policy_id = NULL,   runtime_engine = NULL,   single_user_name = NULL,   spark_conf = NULL,   spark_env_vars = NULL,   ssh_public_keys = NULL,   workload_type = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create new cluster. — create_cluster","text":"client Required. Instance DatabricksClient() spark_version Required. Spark version cluster, e.g. apply_policy_default_values field description yet. autoscale Parameters needed order automatically scale clusters based load. autotermination_minutes Automatically terminates cluster inactive time minutes. aws_attributes Attributes related clusters running Amazon Web Services. azure_attributes Attributes related clusters running Microsoft Azure. cluster_log_conf configuration delivering spark logs long-term storage destination. cluster_name Cluster name requested user. cluster_source Determines whether cluster created user UI, created Databricks Jobs Scheduler, API request. custom_tags Additional tags cluster resources. data_security_mode Data security mode decides data governance model use accessing data cluster. docker_image field description yet. driver_instance_pool_id optional ID instance pool driver cluster belongs. driver_node_type_id node type Spark driver. enable_elastic_disk Autoscaling Local Storage: enabled, cluster dynamically acquire additional disk space Spark workers running low disk space. enable_local_disk_encryption Whether enable LUKS cluster VMs' local disks. gcp_attributes Attributes related clusters running Google Cloud Platform. init_scripts configuration storing init scripts. instance_pool_id optional ID instance pool cluster belongs. node_type_id field encodes, single value, resources available Spark nodes cluster. num_workers Number worker nodes cluster . policy_id ID cluster policy used create cluster applicable. runtime_engine Decides runtime engine use, e.g. single_user_name Single user name data_security_mode SINGLE_USER. spark_conf object containing set optional, user-specified Spark configuration key-value pairs. spark_env_vars object containing set optional, user-specified environment variable key-value pairs. ssh_public_keys SSH public key contents added Spark node cluster. workload_type field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_cluster.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create new cluster. — create_cluster","text":"Databricks acquires least 85% requested -demand nodes, cluster creation succeed. Otherwise cluster terminate informative error message.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_cluster_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Create new cluster. — create_cluster_and_wait","title":"Create new cluster. — create_cluster_and_wait","text":"long-running operation, blocks Clusters Databricks reach RUNNING state timeout 20 minutes, can change via timeout parameter. default, state Databricks Clusters reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_cluster_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create new cluster. — create_cluster_and_wait","text":"","code":"create_cluster_and_wait(   client,   spark_version,   apply_policy_default_values = NULL,   autoscale = NULL,   autotermination_minutes = NULL,   aws_attributes = NULL,   azure_attributes = NULL,   cluster_log_conf = NULL,   cluster_name = NULL,   cluster_source = NULL,   custom_tags = NULL,   data_security_mode = NULL,   docker_image = NULL,   driver_instance_pool_id = NULL,   driver_node_type_id = NULL,   enable_elastic_disk = NULL,   enable_local_disk_encryption = NULL,   gcp_attributes = NULL,   init_scripts = NULL,   instance_pool_id = NULL,   node_type_id = NULL,   num_workers = NULL,   policy_id = NULL,   runtime_engine = NULL,   single_user_name = NULL,   spark_conf = NULL,   spark_env_vars = NULL,   ssh_public_keys = NULL,   workload_type = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_cluster_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create new cluster. — create_cluster_and_wait","text":"client Required. Instance DatabricksClient() spark_version Required. Spark version cluster, e.g. apply_policy_default_values field description yet. autoscale Parameters needed order automatically scale clusters based load. autotermination_minutes Automatically terminates cluster inactive time minutes. aws_attributes Attributes related clusters running Amazon Web Services. azure_attributes Attributes related clusters running Microsoft Azure. cluster_log_conf configuration delivering spark logs long-term storage destination. cluster_name Cluster name requested user. cluster_source Determines whether cluster created user UI, created Databricks Jobs Scheduler, API request. custom_tags Additional tags cluster resources. data_security_mode Data security mode decides data governance model use accessing data cluster. docker_image field description yet. driver_instance_pool_id optional ID instance pool driver cluster belongs. driver_node_type_id node type Spark driver. enable_elastic_disk Autoscaling Local Storage: enabled, cluster dynamically acquire additional disk space Spark workers running low disk space. enable_local_disk_encryption Whether enable LUKS cluster VMs' local disks. gcp_attributes Attributes related clusters running Google Cloud Platform. init_scripts configuration storing init scripts. instance_pool_id optional ID instance pool cluster belongs. node_type_id field encodes, single value, resources available Spark nodes cluster. num_workers Number worker nodes cluster . policy_id ID cluster policy used create cluster applicable. runtime_engine Decides runtime engine use, e.g. single_user_name Single user name data_security_mode SINGLE_USER. spark_conf object containing set optional, user-specified Spark configuration key-value pairs. spark_env_vars object containing set optional, user-specified environment variable key-value pairs. ssh_public_keys SSH public key contents added Spark node cluster. workload_type field description yet. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_cluster_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create new cluster. — create_cluster_and_wait","text":"Creates new Spark cluster. method acquire new instances cloud provider necessary. Note: Databricks may able acquire requested nodes, due cloud provider limitations (account limits, spot price, etc.) transient network issues. Databricks acquires least 85% requested -demand nodes, cluster creation succeed. Otherwise cluster terminate informative error message.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_cluster_policy.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new policy. — create_cluster_policy","title":"Create a new policy. — create_cluster_policy","text":"Creates new policy prescribed settings.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_cluster_policy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new policy. — create_cluster_policy","text":"","code":"create_cluster_policy(   client,   name,   definition = NULL,   description = NULL,   libraries = NULL,   max_clusters_per_user = NULL,   policy_family_definition_overrides = NULL,   policy_family_id = NULL )  clusterPoliciesCreate(   client,   name,   definition = NULL,   description = NULL,   libraries = NULL,   max_clusters_per_user = NULL,   policy_family_definition_overrides = NULL,   policy_family_id = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_cluster_policy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new policy. — create_cluster_policy","text":"client Required. Instance DatabricksClient() name Required. Cluster Policy name requested user. definition Policy definition document expressed Databricks Cluster Policy Definition Language. description Additional human-readable description cluster policy. libraries list libraries installed next cluster restart uses policy. max_clusters_per_user Max number clusters per user can active using policy. policy_family_definition_overrides Policy definition JSON document expressed Databricks Policy Definition Language. policy_family_id ID policy family.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_command_execution.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an execution context. — create_command_execution","title":"Create an execution context. — create_command_execution","text":"Creates execution context running cluster commands.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_command_execution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an execution context. — create_command_execution","text":"","code":"create_command_execution(client, cluster_id = NULL, language = NULL)  commandExecutionCreate(client, cluster_id = NULL, language = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_command_execution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an execution context. — create_command_execution","text":"client Required. Instance DatabricksClient() cluster_id Running cluster id. language field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_command_execution.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create an execution context. — create_command_execution","text":"successful, method returns ID new execution context.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_command_execution_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an execution context. — create_command_execution_and_wait","title":"Create an execution context. — create_command_execution_and_wait","text":"long-running operation, blocks Command Execution Databricks reach Running state timeout 20 minutes, can change via timeout parameter. default, state Databricks Command Execution reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_command_execution_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an execution context. — create_command_execution_and_wait","text":"","code":"create_command_execution_and_wait(   client,   cluster_id = NULL,   language = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_command_execution_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an execution context. — create_command_execution_and_wait","text":"client Required. Instance DatabricksClient() cluster_id Running cluster id. language field description yet. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_command_execution_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create an execution context. — create_command_execution_and_wait","text":"Creates execution context running cluster commands. successful, method returns ID new execution context.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a connection. — create_connection","title":"Create a connection. — create_connection","text":"Creates new connection","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a connection. — create_connection","text":"","code":"create_connection(   client,   name,   connection_type,   options,   comment = NULL,   properties = NULL,   read_only = NULL )  connectionsCreate(   client,   name,   connection_type,   options,   comment = NULL,   properties = NULL,   read_only = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a connection. — create_connection","text":"client Required. Instance DatabricksClient() name Required. Name connection. connection_type Required. type connection. options Required. map key-value properties attached securable. comment User-provided free-form text description. properties object containing map key-value properties attached connection. read_only connection read .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_connection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a connection. — create_connection","text":"Creates new connection external data source. allows users specify connection details configurations interaction external server.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_dashboard.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a dashboard object. — create_dashboard","title":"Create a dashboard object. — create_dashboard","text":"Create dashboard object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_dashboard.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a dashboard object. — create_dashboard","text":"","code":"create_dashboard(   client,   name,   dashboard_filters_enabled = NULL,   is_favorite = NULL,   parent = NULL,   run_as_role = NULL,   tags = NULL )  dashboardsCreate(   client,   name,   dashboard_filters_enabled = NULL,   is_favorite = NULL,   parent = NULL,   run_as_role = NULL,   tags = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_dashboard.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a dashboard object. — create_dashboard","text":"client Required. Instance DatabricksClient() name Required. title dashboard appears list views top dashboard page. dashboard_filters_enabled Indicates whether dashboard filters enabled. is_favorite Indicates whether dashboard object appear current user's favorites list. parent identifier workspace folder containing object. run_as_role Sets Run role object. tags field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_dashboard_widget.html","id":null,"dir":"Reference","previous_headings":"","what":"Add widget to a dashboard. — create_dashboard_widget","title":"Add widget to a dashboard. — create_dashboard_widget","text":"Add widget dashboard.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_dashboard_widget.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add widget to a dashboard. — create_dashboard_widget","text":"","code":"create_dashboard_widget(   client,   dashboard_id,   options,   width,   text = NULL,   visualization_id = NULL )  dashboardWidgetsCreate(   client,   dashboard_id,   options,   width,   text = NULL,   visualization_id = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_dashboard_widget.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add widget to a dashboard. — create_dashboard_widget","text":"client Required. Instance DatabricksClient() dashboard_id Required. Dashboard ID returned :method:dashboards/create. options Required. field description yet. width Required. Width widget. text textbox widget, application displays text. visualization_id Query Vizualization ID returned :method:queryvisualizations/create.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_dbfs.html","id":null,"dir":"Reference","previous_headings":"","what":"Open a stream. — create_dbfs","title":"Open a stream. — create_dbfs","text":"Opens stream write file returns handle stream. 10 minute idle timeout handle. file directory already exists given path overwrite set false, call throw exception RESOURCE_ALREADY_EXISTS.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_dbfs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open a stream. — create_dbfs","text":"","code":"create_dbfs(client, path, overwrite = NULL)  dbfsCreate(client, path, overwrite = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_dbfs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open a stream. — create_dbfs","text":"client Required. Instance DatabricksClient() path Required. path new file. overwrite flag specifies whether overwrite existing file/files.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_dbfs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Open a stream. — create_dbfs","text":"typical workflow file upload : Issue create call get handle. 2. Issue one add-block calls handle . 3. Issue close call handle .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_experiment.html","id":null,"dir":"Reference","previous_headings":"","what":"Create experiment. — create_experiment","title":"Create experiment. — create_experiment","text":"Creates experiment name. Returns ID newly created experiment. Validates another experiment name already exist fails another experiment name already exists.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_experiment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create experiment. — create_experiment","text":"","code":"create_experiment(client, name, artifact_location = NULL, tags = NULL)  experimentsCreateExperiment(   client,   name,   artifact_location = NULL,   tags = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_experiment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create experiment. — create_experiment","text":"client Required. Instance DatabricksClient() name Required. Experiment name. artifact_location Location artifacts experiment stored. tags collection tags set experiment.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_experiment.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create experiment. — create_experiment","text":"Throws RESOURCE_ALREADY_EXISTS experiment given name exists.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_experiment_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a run. — create_experiment_run","title":"Create a run. — create_experiment_run","text":"Creates new run within experiment. run usually single execution machine learning data ETL pipeline. MLflow uses runs track mlflowParam, mlflowMetric mlflowRunTag associated single execution.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_experiment_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a run. — create_experiment_run","text":"","code":"create_experiment_run(   client,   experiment_id = NULL,   start_time = NULL,   tags = NULL,   user_id = NULL )  experimentsCreateRun(   client,   experiment_id = NULL,   start_time = NULL,   tags = NULL,   user_id = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_experiment_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a run. — create_experiment_run","text":"client Required. Instance DatabricksClient() experiment_id ID associated experiment. start_time Unix timestamp milliseconds run started. tags Additional metadata run. user_id ID user executing run.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_external_location.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an external location. — create_external_location","title":"Create an external location. — create_external_location","text":"Creates new external location entry metastore. caller must metastore admin CREATE_EXTERNAL_LOCATION privilege metastore associated storage credential.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_external_location.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an external location. — create_external_location","text":"","code":"create_external_location(   client,   name,   url,   credential_name,   access_point = NULL,   comment = NULL,   encryption_details = NULL,   read_only = NULL,   skip_validation = NULL )  externalLocationsCreate(   client,   name,   url,   credential_name,   access_point = NULL,   comment = NULL,   encryption_details = NULL,   read_only = NULL,   skip_validation = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_external_location.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an external location. — create_external_location","text":"client Required. Instance DatabricksClient() name Required. Name external location. url Required. Path URL external location. credential_name Required. Name storage credential used location. access_point AWS access point use accesing s3 external location. comment User-provided free-form text description. encryption_details Encryption options apply clients connecting cloud storage. read_only Indicates whether external location read-. skip_validation Skips validation storage credential associated external location.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_file_directory.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a directory. — create_file_directory","title":"Create a directory. — create_file_directory","text":"Creates empty directory. necessary, also creates parent directories new, empty directory (like shell command mkdir -p). called existing directory, returns success response; method idempotent (succeed directory already exists).","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_file_directory.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a directory. — create_file_directory","text":"","code":"create_file_directory(client, directory_path)  filesCreateDirectory(client, directory_path)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_file_directory.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a directory. — create_file_directory","text":"client Required. Instance DatabricksClient() directory_path Required. absolute path directory.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a function. — create_function","title":"Create a function. — create_function","text":"Creates new function","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a function. — create_function","text":"","code":"create_function(client, function_info)  functionsCreate(client, function_info)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a function. — create_function","text":"client Required. Instance DatabricksClient() function_info Required. Partial FunctionInfo specifying function created.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_function.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a function. — create_function","text":"user must following permissions order function created: - USE_CATALOG function's parent catalog - USE_SCHEMA CREATE_FUNCTION function's parent schema","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_git_credential.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a credential entry. — create_git_credential","title":"Create a credential entry. — create_git_credential","text":"Creates Git credential entry user. one Git credential per user supported, attempts create credentials entry already exists fail. Use PATCH endpoint update existing credentials, DELETE endpoint delete existing credentials.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_git_credential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a credential entry. — create_git_credential","text":"","code":"create_git_credential(   client,   git_provider,   git_username = NULL,   personal_access_token = NULL )  gitCredentialsCreate(   client,   git_provider,   git_username = NULL,   personal_access_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_git_credential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a credential entry. — create_git_credential","text":"client Required. Instance DatabricksClient() git_provider Required. Git provider. git_username Git username. personal_access_token personal access token used authenticate corresponding Git provider.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_global_init_script.html","id":null,"dir":"Reference","previous_headings":"","what":"Create init script. — create_global_init_script","title":"Create init script. — create_global_init_script","text":"Creates new global init script workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_global_init_script.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create init script. — create_global_init_script","text":"","code":"create_global_init_script(   client,   name,   script,   enabled = NULL,   position = NULL )  globalInitScriptsCreate(client, name, script, enabled = NULL, position = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_global_init_script.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create init script. — create_global_init_script","text":"client Required. Instance DatabricksClient() name Required. name script. script Required. Base64-encoded content script. enabled Specifies whether script enabled. position position global init script, 0 represents first script run, 1 second script run, ascending order.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new group. — create_group","title":"Create a new group. — create_group","text":"Creates group Databricks workspace unique name, using supplied group details.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new group. — create_group","text":"","code":"create_group(   client,   display_name = NULL,   entitlements = NULL,   external_id = NULL,   groups = NULL,   id = NULL,   members = NULL,   meta = NULL,   roles = NULL,   schemas = NULL )  groupsCreate(   client,   display_name = NULL,   entitlements = NULL,   external_id = NULL,   groups = NULL,   id = NULL,   members = NULL,   meta = NULL,   roles = NULL,   schemas = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new group. — create_group","text":"client Required. Instance DatabricksClient() display_name String represents human-readable group name. entitlements Entitlements assigned group. external_id field description yet. groups field description yet. id Databricks group ID. members field description yet. meta Container group identifier. roles Corresponds AWS instance profile/arn role. schemas schema group.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_instance_pool.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new instance pool. — create_instance_pool","title":"Create a new instance pool. — create_instance_pool","text":"Creates new instance pool using idle ready--use cloud instances.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_instance_pool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new instance pool. — create_instance_pool","text":"","code":"create_instance_pool(   client,   instance_pool_name,   node_type_id,   aws_attributes = NULL,   azure_attributes = NULL,   custom_tags = NULL,   disk_spec = NULL,   enable_elastic_disk = NULL,   gcp_attributes = NULL,   idle_instance_autotermination_minutes = NULL,   max_capacity = NULL,   min_idle_instances = NULL,   preloaded_docker_images = NULL,   preloaded_spark_versions = NULL )  instancePoolsCreate(   client,   instance_pool_name,   node_type_id,   aws_attributes = NULL,   azure_attributes = NULL,   custom_tags = NULL,   disk_spec = NULL,   enable_elastic_disk = NULL,   gcp_attributes = NULL,   idle_instance_autotermination_minutes = NULL,   max_capacity = NULL,   min_idle_instances = NULL,   preloaded_docker_images = NULL,   preloaded_spark_versions = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_instance_pool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new instance pool. — create_instance_pool","text":"client Required. Instance DatabricksClient() instance_pool_name Required. Pool name requested user. node_type_id Required. field encodes, single value, resources available Spark nodes cluster. aws_attributes Attributes related instance pools running Amazon Web Services. azure_attributes Attributes related instance pools running Azure. custom_tags Additional tags pool resources. disk_spec Defines specification disks attached spark containers. enable_elastic_disk Autoscaling Local Storage: enabled, instances pool dynamically acquire additional disk space Spark workers running low disk space. gcp_attributes Attributes related instance pools running Google Cloud Platform. idle_instance_autotermination_minutes Automatically terminates extra instances pool cache inactive time minutes min_idle_instances requirement already met. max_capacity Maximum number outstanding instances keep pool, including instances used clusters idle instances. min_idle_instances Minimum number idle instances keep instance pool. preloaded_docker_images Custom Docker Image BYOC. preloaded_spark_versions list containing one preloaded Spark image version pool.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_ip_access_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Create access list. — create_ip_access_list","title":"Create access list. — create_ip_access_list","text":"Creates IP access list workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_ip_access_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create access list. — create_ip_access_list","text":"","code":"create_ip_access_list(client, label, list_type, ip_addresses = NULL)  ipAccessListsCreate(client, label, list_type, ip_addresses = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_ip_access_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create access list. — create_ip_access_list","text":"client Required. Instance DatabricksClient() label Required. Label IP access list. list_type Required. Type IP access list. ip_addresses field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_ip_access_list.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create access list. — create_ip_access_list","text":"list can allow list block list. See top file description server treats allow lists block lists runtime. creating updating IP access list: allow lists block lists combined, API supports maximum 1000 IP/CIDR values, one CIDR counts single value. Attempts exceed number return error 400 error_code value QUOTA_EXCEEDED. new list block calling user's current IP, error 400 returned error_code value INVALID_STATE. can take minutes changes take effect. Note: new IP access list effect enable feature. See :method:workspaceconf/setStatus","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_job.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new job. — create_job","title":"Create a new job. — create_job","text":"Create new job.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_job.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new job. — create_job","text":"","code":"create_job(   client,   access_control_list = NULL,   compute = NULL,   continuous = NULL,   deployment = NULL,   description = NULL,   edit_mode = NULL,   email_notifications = NULL,   format = NULL,   git_source = NULL,   health = NULL,   job_clusters = NULL,   max_concurrent_runs = NULL,   name = NULL,   notification_settings = NULL,   parameters = NULL,   queue = NULL,   run_as = NULL,   schedule = NULL,   tags = NULL,   tasks = NULL,   timeout_seconds = NULL,   trigger = NULL,   webhook_notifications = NULL )  jobsCreate(   client,   access_control_list = NULL,   compute = NULL,   continuous = NULL,   deployment = NULL,   description = NULL,   edit_mode = NULL,   email_notifications = NULL,   format = NULL,   git_source = NULL,   health = NULL,   job_clusters = NULL,   max_concurrent_runs = NULL,   name = NULL,   notification_settings = NULL,   parameters = NULL,   queue = NULL,   run_as = NULL,   schedule = NULL,   tags = NULL,   tasks = NULL,   timeout_seconds = NULL,   trigger = NULL,   webhook_notifications = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_job.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new job. — create_job","text":"client Required. Instance DatabricksClient() access_control_list List permissions set job. compute list compute requirements can referenced tasks job. continuous optional continuous property job. deployment Deployment information jobs managed external sources. description optional description job. edit_mode Edit mode job. email_notifications optional set email addresses notified runs job begin complete well job deleted. format Used tell format job. git_source optional specification remote Git repository containing source code used tasks. health optional set health rules can defined job. job_clusters list job cluster specifications can shared reused tasks job. max_concurrent_runs optional maximum allowed number concurrent runs job. name optional name job. notification_settings Optional notification settings used sending notifications email_notifications webhook_notifications job. parameters Job-level parameter definitions. queue queue settings job. run_as Write-setting, available Create/Update/Reset Submit calls. schedule optional periodic schedule job. tags map tags associated job. tasks list task specifications executed job. timeout_seconds optional timeout applied run job. trigger configuration trigger run certain conditions met. webhook_notifications collection system notification IDs notify runs job begin complete.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_lakehouse_monitor.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a table monitor. — create_lakehouse_monitor","title":"Create a table monitor. — create_lakehouse_monitor","text":"Creates new monitor specified table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_lakehouse_monitor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a table monitor. — create_lakehouse_monitor","text":"","code":"create_lakehouse_monitor(   client,   full_name,   assets_dir,   output_schema_name,   baseline_table_name = NULL,   custom_metrics = NULL,   data_classification_config = NULL,   inference_log = NULL,   notifications = NULL,   schedule = NULL,   skip_builtin_dashboard = NULL,   slicing_exprs = NULL,   snapshot = NULL,   time_series = NULL,   warehouse_id = NULL )  lakehouseMonitorsCreate(   client,   full_name,   assets_dir,   output_schema_name,   baseline_table_name = NULL,   custom_metrics = NULL,   data_classification_config = NULL,   inference_log = NULL,   notifications = NULL,   schedule = NULL,   skip_builtin_dashboard = NULL,   slicing_exprs = NULL,   snapshot = NULL,   time_series = NULL,   warehouse_id = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_lakehouse_monitor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a table monitor. — create_lakehouse_monitor","text":"client Required. Instance DatabricksClient() full_name Required. Full name table. assets_dir Required. directory store monitoring assets (e.g. output_schema_name Required. Schema output metric tables created. baseline_table_name Name baseline table drift metrics computed . custom_metrics Custom metrics compute monitored table. data_classification_config data classification config monitor. inference_log Configuration monitoring inference logs. notifications notification settings monitor. schedule schedule automatically updating refreshing metric tables. skip_builtin_dashboard Whether skip creating default dashboard summarizing data quality metrics. slicing_exprs List column expressions slice data targeted analysis. snapshot Configuration monitoring snapshot tables. time_series Configuration monitoring time series tables. warehouse_id Optional argument specify warehouse dashboard creation.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_lakehouse_monitor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a table monitor. — create_lakehouse_monitor","text":"caller must either: 1. owner table's parent catalog, USE_SCHEMA table's parent schema, SELECT access table 2. USE_CATALOG table's parent catalog, owner table's parent schema, SELECT access table. 3. following permissions: - USE_CATALOG table's parent catalog - USE_SCHEMA table's parent schema - owner table. Workspace assets, dashboard, created workspace call made.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_lakeview.html","id":null,"dir":"Reference","previous_headings":"","what":"Create dashboard. — create_lakeview","title":"Create dashboard. — create_lakeview","text":"Create draft dashboard.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_lakeview.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create dashboard. — create_lakeview","text":"","code":"create_lakeview(   client,   display_name,   parent_path = NULL,   serialized_dashboard = NULL,   warehouse_id = NULL )  lakeviewCreate(   client,   display_name,   parent_path = NULL,   serialized_dashboard = NULL,   warehouse_id = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_lakeview.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create dashboard. — create_lakeview","text":"client Required. Instance DatabricksClient() display_name Required. display name dashboard. parent_path workspace path folder containing dashboard. serialized_dashboard contents dashboard serialized string form. warehouse_id warehouse ID used run dashboard.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_metastore.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a metastore. — create_metastore","title":"Create a metastore. — create_metastore","text":"Creates new metastore based provided name optional storage root path. default (owner field set), owner new metastore user calling createMetastore API. owner field set empty string (''), ownership assigned System User instead.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_metastore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a metastore. — create_metastore","text":"","code":"create_metastore(client, name, region = NULL, storage_root = NULL)  metastoresCreate(client, name, region = NULL, storage_root = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_metastore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a metastore. — create_metastore","text":"client Required. Instance DatabricksClient() name Required. user-specified name metastore. region Cloud region metastore serves (e.g., us-west-2, westus). storage_root storage root URL metastore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a model. — create_model","title":"Create a model. — create_model","text":"Creates new registered model name specified request body.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a model. — create_model","text":"","code":"create_model(client, name, description = NULL, tags = NULL)  modelRegistryCreateModel(client, name, description = NULL, tags = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a model. — create_model","text":"client Required. Instance DatabricksClient() name Required. Register models name. description Optional description registered model. tags Additional metadata registered model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a model. — create_model","text":"Throws RESOURCE_ALREADY_EXISTS registered model given name exists.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model_comment.html","id":null,"dir":"Reference","previous_headings":"","what":"Post a comment. — create_model_comment","title":"Post a comment. — create_model_comment","text":"Posts comment model version. comment can submitted either user programmatically display relevant information model. example, test results deployment errors.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model_comment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Post a comment. — create_model_comment","text":"","code":"create_model_comment(client, name, version, comment)  modelRegistryCreateComment(client, name, version, comment)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model_comment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Post a comment. — create_model_comment","text":"client Required. Instance DatabricksClient() name Required. Name model. version Required. Version model. comment Required. User-provided comment action.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model_transition_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a transition request. — create_model_transition_request","title":"Make a transition request. — create_model_transition_request","text":"Creates model version stage transition request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model_transition_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a transition request. — create_model_transition_request","text":"","code":"create_model_transition_request(client, name, version, stage, comment = NULL)  modelRegistryCreateTransitionRequest(   client,   name,   version,   stage,   comment = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model_transition_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a transition request. — create_model_transition_request","text":"client Required. Instance DatabricksClient() name Required. Name model. version Required. Version model. stage Required. Target stage transition. comment User-provided comment action.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model_version.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a model version. — create_model_version","title":"Create a model version. — create_model_version","text":"Creates model version.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model_version.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a model version. — create_model_version","text":"","code":"create_model_version(   client,   name,   source,   description = NULL,   run_id = NULL,   run_link = NULL,   tags = NULL )  modelRegistryCreateModelVersion(   client,   name,   source,   description = NULL,   run_id = NULL,   run_link = NULL,   tags = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model_version.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a model version. — create_model_version","text":"client Required. Instance DatabricksClient() name Required. Register model name. source Required. URI indicating location model artifacts. description Optional description model version. run_id MLflow run ID correlation, source generated experiment run MLflow tracking server. run_link MLflow run link - exact link run generated model version, potentially hosted another instance MLflow. tags Additional metadata model version.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model_webhook.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a webhook. — create_model_webhook","title":"Create a webhook. — create_model_webhook","text":"NOTE: endpoint Public Preview.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model_webhook.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a webhook. — create_model_webhook","text":"","code":"create_model_webhook(   client,   events,   description = NULL,   http_url_spec = NULL,   job_spec = NULL,   model_name = NULL,   status = NULL )  modelRegistryCreateWebhook(   client,   events,   description = NULL,   http_url_spec = NULL,   job_spec = NULL,   model_name = NULL,   status = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model_webhook.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a webhook. — create_model_webhook","text":"client Required. Instance DatabricksClient() events Required. Events can trigger registry webhook: * MODEL_VERSION_CREATED: new model version created associated model. description User-specified description webhook. http_url_spec field description yet. job_spec field description yet. model_name Name model whose events trigger webhook. status Enable disable triggering webhook, put webhook test mode.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_model_webhook.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a webhook. — create_model_webhook","text":"Creates registry webhook.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_obo_token.html","id":null,"dir":"Reference","previous_headings":"","what":"Create on-behalf token. — create_obo_token","title":"Create on-behalf token. — create_obo_token","text":"Creates token behalf service principal.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_obo_token.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create on-behalf token. — create_obo_token","text":"","code":"create_obo_token(   client,   application_id,   comment = NULL,   lifetime_seconds = NULL )  tokenManagementCreateOboToken(   client,   application_id,   comment = NULL,   lifetime_seconds = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_obo_token.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create on-behalf token. — create_obo_token","text":"client Required. Instance DatabricksClient() application_id Required. Application ID service principal. comment Comment describes purpose token. lifetime_seconds number seconds token expires.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_online_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an Online Table. — create_online_table","title":"Create an Online Table. — create_online_table","text":"Create new Online Table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_online_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an Online Table. — create_online_table","text":"","code":"create_online_table(client, name = NULL, spec = NULL)  onlineTablesCreate(client, name = NULL, spec = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_online_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an Online Table. — create_online_table","text":"client Required. Instance DatabricksClient() name Full three-part (catalog, schema, table) name table. spec Specification online table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a pipeline. — create_pipeline","title":"Create a pipeline. — create_pipeline","text":"Creates new data processing pipeline based requested configuration. successful, method returns ID new pipeline.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a pipeline. — create_pipeline","text":"","code":"create_pipeline(   client,   allow_duplicate_names = NULL,   catalog = NULL,   channel = NULL,   clusters = NULL,   configuration = NULL,   continuous = NULL,   development = NULL,   dry_run = NULL,   edition = NULL,   filters = NULL,   id = NULL,   libraries = NULL,   name = NULL,   notifications = NULL,   photon = NULL,   serverless = NULL,   storage = NULL,   target = NULL,   trigger = NULL )  pipelinesCreate(   client,   allow_duplicate_names = NULL,   catalog = NULL,   channel = NULL,   clusters = NULL,   configuration = NULL,   continuous = NULL,   development = NULL,   dry_run = NULL,   edition = NULL,   filters = NULL,   id = NULL,   libraries = NULL,   name = NULL,   notifications = NULL,   photon = NULL,   serverless = NULL,   storage = NULL,   target = NULL,   trigger = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a pipeline. — create_pipeline","text":"client Required. Instance DatabricksClient() allow_duplicate_names false, deployment fail name conflicts another pipeline. catalog catalog Unity Catalog publish data pipeline . channel DLT Release Channel specifies version use. clusters Cluster settings pipeline deployment. configuration String-String configuration pipeline execution. continuous Whether pipeline continuous triggered. development Whether pipeline Development mode. dry_run field description yet. edition Pipeline product edition. filters Filters Pipeline packages include deployed graph. id Unique identifier pipeline. libraries Libraries code needed deployment. name Friendly identifier pipeline. notifications List notification settings pipeline. photon Whether Photon enabled pipeline. serverless Whether serverless compute enabled pipeline. storage DBFS root directory storing checkpoints tables. target Target schema (database) add tables pipeline . trigger pipeline trigger use.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_provider.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an auth provider. — create_provider","title":"Create an auth provider. — create_provider","text":"Creates new authentication provider minimally based name authentication type. caller must admin metastore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_provider.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an auth provider. — create_provider","text":"","code":"create_provider(   client,   name,   authentication_type,   comment = NULL,   recipient_profile_str = NULL )  providersCreate(   client,   name,   authentication_type,   comment = NULL,   recipient_profile_str = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_provider.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an auth provider. — create_provider","text":"client Required. Instance DatabricksClient() name Required. name Provider. authentication_type Required. delta sharing authentication type. comment Description provider. recipient_profile_str field required authentication_type TOKEN provided.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new query definition. — create_query","title":"Create a new query definition. — create_query","text":"Creates new query definition. Queries created endpoint belong authenticated user making request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new query definition. — create_query","text":"","code":"create_query(   client,   data_source_id = NULL,   description = NULL,   name = NULL,   options = NULL,   parent = NULL,   query = NULL,   run_as_role = NULL )  queriesCreate(   client,   data_source_id = NULL,   description = NULL,   name = NULL,   options = NULL,   parent = NULL,   query = NULL,   run_as_role = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new query definition. — create_query","text":"client Required. Instance DatabricksClient() data_source_id Data source ID maps ID data source used resource distinct warehouse ID. description General description conveys additional information query usage notes. name title query appears list views, widget headings, query page. options Exclusively used storing list parameter definitions. parent identifier workspace folder containing object. query text query run. run_as_role Sets Run role object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_query.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a new query definition. — create_query","text":"data_source_id field specifies ID SQL warehouse run query . can use Data Sources API see complete list available SQL warehouses. can copy data_source_id existing query. Note: add visualization create query.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_query_visualization.html","id":null,"dir":"Reference","previous_headings":"","what":"Add visualization to a query. — create_query_visualization","title":"Add visualization to a query. — create_query_visualization","text":"Add visualization query.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_query_visualization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add visualization to a query. — create_query_visualization","text":"","code":"create_query_visualization(   client,   query_id,   type,   options,   description = NULL,   name = NULL )  queryVisualizationsCreate(   client,   query_id,   type,   options,   description = NULL,   name = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_query_visualization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add visualization to a query. — create_query_visualization","text":"client Required. Instance DatabricksClient() query_id Required. identifier returned :method:queries/create. type Required. type visualization: chart, table, pivot table, . options Required. options object varies widely one visualization type next unsupported. description short description visualization. name name visualization appears dashboards query screen.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_recipient.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a share recipient. — create_recipient","title":"Create a share recipient. — create_recipient","text":"Creates new recipient delta sharing authentication type metastore. caller must metastore admin CREATE_RECIPIENT privilege metastore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_recipient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a share recipient. — create_recipient","text":"","code":"create_recipient(   client,   name,   authentication_type,   comment = NULL,   data_recipient_global_metastore_id = NULL,   ip_access_list = NULL,   owner = NULL,   properties_kvpairs = NULL,   sharing_code = NULL )  recipientsCreate(   client,   name,   authentication_type,   comment = NULL,   data_recipient_global_metastore_id = NULL,   ip_access_list = NULL,   owner = NULL,   properties_kvpairs = NULL,   sharing_code = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_recipient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a share recipient. — create_recipient","text":"client Required. Instance DatabricksClient() name Required. Name Recipient. authentication_type Required. delta sharing authentication type. comment Description recipient. data_recipient_global_metastore_id global Unity Catalog metastore id provided data recipient. ip_access_list IP Access List. owner Username recipient owner. properties_kvpairs Recipient properties map string key-value pairs. sharing_code one-time sharing code provided data recipient.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_registered_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Registered Model. — create_registered_model","title":"Create a Registered Model. — create_registered_model","text":"Creates new registered model Unity Catalog.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_registered_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Registered Model. — create_registered_model","text":"","code":"create_registered_model(   client,   catalog_name,   schema_name,   name,   comment = NULL,   storage_location = NULL )  registeredModelsCreate(   client,   catalog_name,   schema_name,   name,   comment = NULL,   storage_location = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_registered_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Registered Model. — create_registered_model","text":"client Required. Instance DatabricksClient() catalog_name Required. name catalog schema registered model reside. schema_name Required. name schema registered model resides. name Required. name registered model. comment comment attached registered model. storage_location storage location cloud model version data files stored.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_registered_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Registered Model. — create_registered_model","text":"File storage model versions registered model located default location specified parent schema, parent catalog, Metastore. registered model creation succeed, user must satisfy following conditions: - caller must metastore admin, owner parent catalog schema, USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema. - caller must CREATE MODEL CREATE FUNCTION privilege parent schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_repo.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a repo. — create_repo","title":"Create a repo. — create_repo","text":"Creates repo workspace links remote Git repo specified. Note repos created programmatically must linked remote Git repo, unlike repos created browser.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_repo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a repo. — create_repo","text":"","code":"create_repo(client, url, provider, path = NULL, sparse_checkout = NULL)  reposCreate(client, url, provider, path = NULL, sparse_checkout = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_repo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a repo. — create_repo","text":"client Required. Instance DatabricksClient() url Required. URL Git repository linked. provider Required. Git provider. path Desired path repo workspace. sparse_checkout specified, repo created sparse checkout enabled.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a schema. — create_schema","title":"Create a schema. — create_schema","text":"Creates new schema catalog Metatastore. caller must metastore admin, CREATE_SCHEMA privilege parent catalog.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a schema. — create_schema","text":"","code":"create_schema(   client,   name,   catalog_name,   comment = NULL,   properties = NULL,   storage_root = NULL )  schemasCreate(   client,   name,   catalog_name,   comment = NULL,   properties = NULL,   storage_root = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a schema. — create_schema","text":"client Required. Instance DatabricksClient() name Required. Name schema, relative parent catalog. catalog_name Required. Name parent catalog. comment User-provided free-form text description. properties map key-value properties attached securable. storage_root Storage root URL managed tables within schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_secret_scope.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new secret scope. — create_secret_scope","title":"Create a new secret scope. — create_secret_scope","text":"scope name must consist alphanumeric characters, dashes, underscores, periods, may exceed 128 characters.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_secret_scope.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new secret scope. — create_secret_scope","text":"","code":"create_secret_scope(   client,   scope,   backend_azure_keyvault = NULL,   initial_manage_principal = NULL,   scope_backend_type = NULL )  secretsCreateScope(   client,   scope,   backend_azure_keyvault = NULL,   initial_manage_principal = NULL,   scope_backend_type = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_secret_scope.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new secret scope. — create_secret_scope","text":"client Required. Instance DatabricksClient() scope Required. Scope name requested user. backend_azure_keyvault metadata secret scope type AZURE_KEYVAULT. initial_manage_principal principal initially granted MANAGE permission created scope. scope_backend_type backend type scope created .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_service_principal.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a service principal. — create_service_principal","title":"Create a service principal. — create_service_principal","text":"Creates new service principal Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_service_principal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a service principal. — create_service_principal","text":"","code":"create_service_principal(   client,   active = NULL,   application_id = NULL,   display_name = NULL,   entitlements = NULL,   external_id = NULL,   groups = NULL,   id = NULL,   roles = NULL,   schemas = NULL )  servicePrincipalsCreate(   client,   active = NULL,   application_id = NULL,   display_name = NULL,   entitlements = NULL,   external_id = NULL,   groups = NULL,   id = NULL,   roles = NULL,   schemas = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_service_principal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a service principal. — create_service_principal","text":"client Required. Instance DatabricksClient() active user active. application_id UUID relating service principal. display_name String represents concatenation given family names. entitlements Entitlements assigned service principal. external_id field description yet. groups field description yet. id Databricks service principal ID. roles Corresponds AWS instance profile/arn role. schemas schema List response.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_serving_endpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new serving endpoint. — create_serving_endpoint","title":"Create a new serving endpoint. — create_serving_endpoint","text":"Create new serving endpoint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_serving_endpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new serving endpoint. — create_serving_endpoint","text":"","code":"create_serving_endpoint(client, name, config, rate_limits = NULL, tags = NULL)  servingEndpointsCreate(client, name, config, rate_limits = NULL, tags = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_serving_endpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new serving endpoint. — create_serving_endpoint","text":"client Required. Instance DatabricksClient() name Required. name serving endpoint. config Required. core config serving endpoint. rate_limits Rate limits applied serving endpoint. tags Tags attached serving endpoint automatically propagated billing logs.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_serving_endpoint_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new serving endpoint. — create_serving_endpoint_and_wait","title":"Create a new serving endpoint. — create_serving_endpoint_and_wait","text":"long-running operation, blocks Serving Endpoints Databricks reach NOT_UPDATING state timeout 20 minutes, can change via timeout parameter. default, state Databricks Serving Endpoints reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_serving_endpoint_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new serving endpoint. — create_serving_endpoint_and_wait","text":"","code":"create_serving_endpoint_and_wait(   client,   name,   config,   rate_limits = NULL,   tags = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_serving_endpoint_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new serving endpoint. — create_serving_endpoint_and_wait","text":"client Required. Instance DatabricksClient() name Required. name serving endpoint. config Required. core config serving endpoint. rate_limits Rate limits applied serving endpoint. tags Tags attached serving endpoint automatically propagated billing logs. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_share.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a share. — create_share","title":"Create a share. — create_share","text":"Creates new share data objects. Data objects can added creation update. caller must metastore admin CREATE_SHARE privilege metastore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_share.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a share. — create_share","text":"","code":"create_share(client, name, comment = NULL)  sharesCreate(client, name, comment = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_share.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a share. — create_share","text":"client Required. Instance DatabricksClient() name Required. Name share. comment User-provided free-form text description.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_storage_credential.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a storage credential. — create_storage_credential","title":"Create a storage credential. — create_storage_credential","text":"Creates new storage credential.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_storage_credential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a storage credential. — create_storage_credential","text":"","code":"create_storage_credential(   client,   name,   aws_iam_role = NULL,   azure_managed_identity = NULL,   azure_service_principal = NULL,   cloudflare_api_token = NULL,   comment = NULL,   databricks_gcp_service_account = NULL,   read_only = NULL,   skip_validation = NULL )  storageCredentialsCreate(   client,   name,   aws_iam_role = NULL,   azure_managed_identity = NULL,   azure_service_principal = NULL,   cloudflare_api_token = NULL,   comment = NULL,   databricks_gcp_service_account = NULL,   read_only = NULL,   skip_validation = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_storage_credential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a storage credential. — create_storage_credential","text":"client Required. Instance DatabricksClient() name Required. credential name. aws_iam_role AWS IAM role configuration. azure_managed_identity Azure managed identity configuration. azure_service_principal Azure service principal configuration. cloudflare_api_token Cloudflare API token configuration. comment Comment associated credential. databricks_gcp_service_account Databricks managed GCP service account configuration. read_only Whether storage credential usable read operations. skip_validation Supplying true argument skips validation created credential.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_table_constraint.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a table constraint. — create_table_constraint","title":"Create a table constraint. — create_table_constraint","text":"Creates new table constraint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_table_constraint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a table constraint. — create_table_constraint","text":"","code":"create_table_constraint(client, full_name_arg, constraint)  tableConstraintsCreate(client, full_name_arg, constraint)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_table_constraint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a table constraint. — create_table_constraint","text":"client Required. Instance DatabricksClient() full_name_arg Required. full name table referenced constraint. constraint Required. table constraint, defined one following fields set: primary_key_constraint, foreign_key_constraint, named_table_constraint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_table_constraint.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a table constraint. — create_table_constraint","text":"table constraint creation succeed, user must satisfy conditions: - user must USE_CATALOG privilege table's parent catalog, USE_SCHEMA privilege table's parent schema, owner table. - new constraint ForeignKeyConstraint, user must USE_CATALOG privilege referenced parent table's catalog, USE_SCHEMA privilege referenced parent table's schema, owner referenced parent table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_token.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a user token. — create_token","title":"Create a user token. — create_token","text":"Creates returns token user. call made token authentication, creates token client ID authenticated token. user's token quota exceeded, call returns error QUOTA_EXCEEDED.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_token.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a user token. — create_token","text":"","code":"create_token(client, comment = NULL, lifetime_seconds = NULL)  tokensCreate(client, comment = NULL, lifetime_seconds = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_token.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a user token. — create_token","text":"client Required. Instance DatabricksClient() comment Optional description attach token. lifetime_seconds lifetime token, seconds.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_user.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new user. — create_user","title":"Create a new user. — create_user","text":"Creates new user Databricks workspace. new user also added Databricks account.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_user.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new user. — create_user","text":"","code":"create_user(   client,   active = NULL,   display_name = NULL,   emails = NULL,   entitlements = NULL,   external_id = NULL,   groups = NULL,   id = NULL,   name = NULL,   roles = NULL,   schemas = NULL,   user_name = NULL )  usersCreate(   client,   active = NULL,   display_name = NULL,   emails = NULL,   entitlements = NULL,   external_id = NULL,   groups = NULL,   id = NULL,   name = NULL,   roles = NULL,   schemas = NULL,   user_name = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_user.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new user. — create_user","text":"client Required. Instance DatabricksClient() active user active. display_name String represents concatenation given family names. emails emails associated Databricks user. entitlements Entitlements assigned user. external_id External ID currently supported. groups field description yet. id Databricks user ID. name field description yet. roles Corresponds AWS instance profile/arn role. schemas schema user. user_name Email address Databricks user.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_vector_search_endpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an endpoint. — create_vector_search_endpoint","title":"Create an endpoint. — create_vector_search_endpoint","text":"Create new endpoint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_vector_search_endpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an endpoint. — create_vector_search_endpoint","text":"","code":"create_vector_search_endpoint(client, name, endpoint_type)  vectorSearchEndpointsCreateEndpoint(client, name, endpoint_type)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_vector_search_endpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an endpoint. — create_vector_search_endpoint","text":"client Required. Instance DatabricksClient() name Required. Name endpoint. endpoint_type Required. Type endpoint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_vector_search_endpoint_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an endpoint. — create_vector_search_endpoint_and_wait","title":"Create an endpoint. — create_vector_search_endpoint_and_wait","text":"long-running operation, blocks Vector Search Endpoints Databricks reach ONLINE state timeout 20 minutes, can change via timeout parameter. default, state Databricks Vector Search Endpoints reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_vector_search_endpoint_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an endpoint. — create_vector_search_endpoint_and_wait","text":"","code":"create_vector_search_endpoint_and_wait(   client,   name,   endpoint_type,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_vector_search_endpoint_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an endpoint. — create_vector_search_endpoint_and_wait","text":"client Required. Instance DatabricksClient() name Required. Name endpoint. endpoint_type Required. Type endpoint. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_vector_search_endpoint_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create an endpoint. — create_vector_search_endpoint_and_wait","text":"Create new endpoint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_vector_search_index.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an index. — create_vector_search_index","title":"Create an index. — create_vector_search_index","text":"Create new index.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_vector_search_index.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an index. — create_vector_search_index","text":"","code":"create_vector_search_index(   client,   name,   endpoint_name,   primary_key,   index_type,   delta_sync_index_spec = NULL,   direct_access_index_spec = NULL )  vectorSearchIndexesCreateIndex(   client,   name,   endpoint_name,   primary_key,   index_type,   delta_sync_index_spec = NULL,   direct_access_index_spec = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_vector_search_index.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an index. — create_vector_search_index","text":"client Required. Instance DatabricksClient() name Required. Name index. endpoint_name Required. Name endpoint used serving index. primary_key Required. Primary key index. index_type Required. 2 types Vector Search indexes: - DELTA_SYNC: index automatically syncs source Delta Table, automatically incrementally updating index underlying data Delta Table changes. delta_sync_index_spec Specification Delta Sync Index. direct_access_index_spec Specification Direct Vector Access Index.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_volume.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Volume. — create_volume","title":"Create a Volume. — create_volume","text":"Creates new volume.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_volume.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Volume. — create_volume","text":"","code":"create_volume(   client,   catalog_name,   schema_name,   name,   volume_type,   comment = NULL,   storage_location = NULL )  volumesCreate(   client,   catalog_name,   schema_name,   name,   volume_type,   comment = NULL,   storage_location = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_volume.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Volume. — create_volume","text":"client Required. Instance DatabricksClient() catalog_name Required. name catalog schema volume . schema_name Required. name schema volume . name Required. name volume. volume_type Required. field description yet. comment comment attached volume. storage_location storage location cloud.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_volume.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Volume. — create_volume","text":"user create either external volume managed volume. external volume created specified external location, managed volume located default location specified parent schema, parent catalog, Metastore. volume creation succeed, user must satisfy following conditions: - caller must metastore admin, owner parent catalog schema, USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema. - caller must CREATE VOLUME privilege parent schema. external volume, following conditions also need satisfy - caller must CREATE EXTERNAL VOLUME privilege external location. - tables, volumes existing specified storage location. - specified storage location location tables, volumes, catalogs schemas.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_warehouse.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a warehouse. — create_warehouse","title":"Create a warehouse. — create_warehouse","text":"Creates new SQL warehouse.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_warehouse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a warehouse. — create_warehouse","text":"","code":"create_warehouse(   client,   auto_stop_mins = NULL,   channel = NULL,   cluster_size = NULL,   creator_name = NULL,   enable_photon = NULL,   enable_serverless_compute = NULL,   instance_profile_arn = NULL,   max_num_clusters = NULL,   min_num_clusters = NULL,   name = NULL,   spot_instance_policy = NULL,   tags = NULL,   warehouse_type = NULL )  warehousesCreate(   client,   auto_stop_mins = NULL,   channel = NULL,   cluster_size = NULL,   creator_name = NULL,   enable_photon = NULL,   enable_serverless_compute = NULL,   instance_profile_arn = NULL,   max_num_clusters = NULL,   min_num_clusters = NULL,   name = NULL,   spot_instance_policy = NULL,   tags = NULL,   warehouse_type = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_warehouse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a warehouse. — create_warehouse","text":"client Required. Instance DatabricksClient() auto_stop_mins amount time minutes SQL warehouse must idle (.e., RUNNING queries) automatically stopped. channel Channel Details. cluster_size Size clusters allocated warehouse. creator_name warehouse creator name. enable_photon Configures whether warehouse use Photon optimized clusters. enable_serverless_compute Configures whether warehouse use serverless compute. instance_profile_arn Deprecated. max_num_clusters Maximum number clusters autoscaler create handle concurrent queries. min_num_clusters Minimum number available clusters maintained SQL warehouse. name Logical name cluster. spot_instance_policy Configurations whether warehouse use spot instances. tags set key-value pairs tagged resources (e.g., AWS instances EBS volumes) associated SQL warehouse. warehouse_type Warehouse type: PRO CLASSIC.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_warehouse_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a warehouse. — create_warehouse_and_wait","title":"Create a warehouse. — create_warehouse_and_wait","text":"long-running operation, blocks Warehouses Databricks reach RUNNING state timeout 20 minutes, can change via timeout parameter. default, state Databricks Warehouses reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_warehouse_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a warehouse. — create_warehouse_and_wait","text":"","code":"create_warehouse_and_wait(   client,   auto_stop_mins = NULL,   channel = NULL,   cluster_size = NULL,   creator_name = NULL,   enable_photon = NULL,   enable_serverless_compute = NULL,   instance_profile_arn = NULL,   max_num_clusters = NULL,   min_num_clusters = NULL,   name = NULL,   spot_instance_policy = NULL,   tags = NULL,   warehouse_type = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_warehouse_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a warehouse. — create_warehouse_and_wait","text":"client Required. Instance DatabricksClient() auto_stop_mins amount time minutes SQL warehouse must idle (.e., RUNNING queries) automatically stopped. channel Channel Details. cluster_size Size clusters allocated warehouse. creator_name warehouse creator name. enable_photon Configures whether warehouse use Photon optimized clusters. enable_serverless_compute Configures whether warehouse use serverless compute. instance_profile_arn Deprecated. max_num_clusters Maximum number clusters autoscaler create handle concurrent queries. min_num_clusters Minimum number available clusters maintained SQL warehouse. name Logical name cluster. spot_instance_policy Configurations whether warehouse use spot instances. tags set key-value pairs tagged resources (e.g., AWS instances EBS volumes) associated SQL warehouse. warehouse_type Warehouse type: PRO CLASSIC. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/create_warehouse_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a warehouse. — create_warehouse_and_wait","text":"Creates new SQL warehouse.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/current_metastore.html","id":null,"dir":"Reference","previous_headings":"","what":"Get metastore assignment for workspace. — current_metastore","title":"Get metastore assignment for workspace. — current_metastore","text":"Gets metastore assignment workspace accessed.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/current_metastore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get metastore assignment for workspace. — current_metastore","text":"","code":"current_metastore(client)  metastoresCurrent(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/current_metastore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get metastore assignment for workspace. — current_metastore","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/databricks-package.html","id":null,"dir":"Reference","previous_headings":"","what":"databricks: Databricks SDK for R (Experimental) — databricks-package","title":"databricks: Databricks SDK for R (Experimental) — databricks-package","text":"Call Databricks REST APIs R.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/databricks-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"databricks: Databricks SDK for R (Experimental) — databricks-package","text":"Maintainer: Serge Smertin serge.smertin@databricks.com contributors: Databricks [copyright holder, funder]","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_alert.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete an alert. — delete_alert","title":"Delete an alert. — delete_alert","text":"Deletes alert. Deleted alerts longer accessible restored. Note: Unlike queries dashboards, alerts moved trash.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_alert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete an alert. — delete_alert","text":"","code":"delete_alert(client, alert_id)  alertsDelete(client, alert_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_alert.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete an alert. — delete_alert","text":"client Required. Instance DatabricksClient() alert_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_app.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete an application. — delete_app","title":"Delete an application. — delete_app","text":"Delete application definition","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_app.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete an application. — delete_app","text":"","code":"delete_app(client, name)  appsDeleteApp(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_app.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete an application. — delete_app","text":"client Required. Instance DatabricksClient() name Required. name application.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_catalog.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a catalog. — delete_catalog","title":"Delete a catalog. — delete_catalog","text":"Deletes catalog matches supplied name. caller must metastore admin owner catalog.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_catalog.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a catalog. — delete_catalog","text":"","code":"delete_catalog(client, name, force = NULL)  catalogsDelete(client, name, force = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_catalog.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a catalog. — delete_catalog","text":"client Required. Instance DatabricksClient() name Required. name catalog. force Force deletion even catalog empty.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_clean_room.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a clean room. — delete_clean_room","title":"Delete a clean room. — delete_clean_room","text":"Deletes data object clean room metastore. caller must owner clean room.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_clean_room.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a clean room. — delete_clean_room","text":"","code":"delete_clean_room(client, name)  cleanRoomsDelete(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_clean_room.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a clean room. — delete_clean_room","text":"client Required. Instance DatabricksClient() name Required. name clean room.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Terminate cluster. — delete_cluster","title":"Terminate cluster. — delete_cluster","text":"Terminates Spark cluster specified ID. cluster removed asynchronously. termination completed, cluster TERMINATED state. cluster already TERMINATING TERMINATED state, nothing happen.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Terminate cluster. — delete_cluster","text":"","code":"delete_cluster(client, cluster_id)  clustersDelete(client, cluster_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Terminate cluster. — delete_cluster","text":"client Required. Instance DatabricksClient() cluster_id Required. cluster terminated.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_cluster_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Terminate cluster. — delete_cluster_and_wait","title":"Terminate cluster. — delete_cluster_and_wait","text":"long-running operation, blocks Clusters Databricks reach TERMINATED state timeout 20 minutes, can change via timeout parameter. default, state Databricks Clusters reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_cluster_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Terminate cluster. — delete_cluster_and_wait","text":"","code":"delete_cluster_and_wait(   client,   cluster_id,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_cluster_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Terminate cluster. — delete_cluster_and_wait","text":"client Required. Instance DatabricksClient() cluster_id Required. cluster terminated. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_cluster_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Terminate cluster. — delete_cluster_and_wait","text":"Terminates Spark cluster specified ID. cluster removed asynchronously. termination completed, cluster TERMINATED state. cluster already TERMINATING TERMINATED state, nothing happen.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_cluster_policy.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a cluster policy. — delete_cluster_policy","title":"Delete a cluster policy. — delete_cluster_policy","text":"Delete policy cluster. Clusters governed policy can still run, edited.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_cluster_policy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a cluster policy. — delete_cluster_policy","text":"","code":"delete_cluster_policy(client, policy_id)  clusterPoliciesDelete(client, policy_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_cluster_policy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a cluster policy. — delete_cluster_policy","text":"client Required. Instance DatabricksClient() policy_id Required. ID policy delete.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a connection. — delete_connection","title":"Delete a connection. — delete_connection","text":"Deletes connection matches supplied name.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a connection. — delete_connection","text":"","code":"delete_connection(client, name)  connectionsDelete(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a connection. — delete_connection","text":"client Required. Instance DatabricksClient() name Required. name connection deleted.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_dashboard.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove a dashboard. — delete_dashboard","title":"Remove a dashboard. — delete_dashboard","text":"Moves dashboard trash. Trashed dashboards appear list views searches, shared.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_dashboard.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove a dashboard. — delete_dashboard","text":"","code":"delete_dashboard(client, dashboard_id)  dashboardsDelete(client, dashboard_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_dashboard.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove a dashboard. — delete_dashboard","text":"client Required. Instance DatabricksClient() dashboard_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_dashboard_widget.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove widget. — delete_dashboard_widget","title":"Remove widget. — delete_dashboard_widget","text":"Remove widget.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_dashboard_widget.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove widget. — delete_dashboard_widget","text":"","code":"delete_dashboard_widget(client, id)  dashboardWidgetsDelete(client, id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_dashboard_widget.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove widget. — delete_dashboard_widget","text":"client Required. Instance DatabricksClient() id Required. Widget ID returned :method:dashboardwidgets/create.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_dbfs.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a file/directory. — delete_dbfs","title":"Delete a file/directory. — delete_dbfs","text":"Delete file directory (optionally recursively delete files directory). call throws exception IO_ERROR path non-empty directory recursive set false similar errors.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_dbfs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a file/directory. — delete_dbfs","text":"","code":"delete_dbfs(client, path, recursive = NULL)  dbfsDelete(client, path, recursive = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_dbfs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a file/directory. — delete_dbfs","text":"client Required. Instance DatabricksClient() path Required. path file directory delete. recursive Whether recursively delete directory's contents.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_dbfs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete a file/directory. — delete_dbfs","text":"delete large number files, delete operation done increments. call returns response approximately 45 seconds error message (503 Service Unavailable) asking re-invoke delete operation directory structure fully deleted. operations delete 10K files, discourage using DBFS REST API, advise perform operations context cluster, using File system utility (dbutils.fs). dbutils.fs covers functional scope DBFS REST API, notebooks. Running operations using notebooks provides better control manageability, selective deletes, possibility automate periodic delete jobs.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_default_namespace.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete the default namespace setting. — delete_default_namespace","title":"Delete the default namespace setting. — delete_default_namespace","text":"Deletes default namespace setting workspace. fresh etag needs provided DELETE requests (query parameter). etag can retrieved making GET request DELETE request. setting updated/deleted concurrently, DELETE fails 409 request must retried using fresh etag 409 response.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_default_namespace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete the default namespace setting. — delete_default_namespace","text":"","code":"delete_default_namespace(client, etag = NULL)  defaultNamespaceDelete(client, etag = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_default_namespace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete the default namespace setting. — delete_default_namespace","text":"client Required. Instance DatabricksClient() etag etag used versioning.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_experiment.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete an experiment. — delete_experiment","title":"Delete an experiment. — delete_experiment","text":"Marks experiment associated metadata, runs, metrics, params, tags deletion. experiment uses FileStore, artifacts associated experiment also deleted.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_experiment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete an experiment. — delete_experiment","text":"","code":"delete_experiment(client, experiment_id)  experimentsDeleteExperiment(client, experiment_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_experiment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete an experiment. — delete_experiment","text":"client Required. Instance DatabricksClient() experiment_id Required. ID associated experiment.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_experiment_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a run. — delete_experiment_run","title":"Delete a run. — delete_experiment_run","text":"Marks run deletion.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_experiment_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a run. — delete_experiment_run","text":"","code":"delete_experiment_run(client, run_id)  experimentsDeleteRun(client, run_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_experiment_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a run. — delete_experiment_run","text":"client Required. Instance DatabricksClient() run_id Required. ID run delete.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_experiment_runs.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete runs by creation time. — delete_experiment_runs","title":"Delete runs by creation time. — delete_experiment_runs","text":"Bulk delete runs experiment created prior specified timestamp. Deletes max_runs per request. call API Databricks Notebook Python, can use client code snippet https://learn.microsoft.com/en-us/azure/databricks/mlflow/runs#bulk-delete.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_experiment_runs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete runs by creation time. — delete_experiment_runs","text":"","code":"delete_experiment_runs(   client,   experiment_id,   max_timestamp_millis,   max_runs = NULL )  experimentsDeleteRuns(   client,   experiment_id,   max_timestamp_millis,   max_runs = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_experiment_runs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete runs by creation time. — delete_experiment_runs","text":"client Required. Instance DatabricksClient() experiment_id Required. ID experiment containing runs delete. max_timestamp_millis Required. maximum creation timestamp milliseconds since UNIX epoch deleting runs. max_runs optional positive integer indicating maximum number runs delete.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_experiment_tag.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a tag. — delete_experiment_tag","title":"Delete a tag. — delete_experiment_tag","text":"Deletes tag run. Tags run metadata can updated run run completes.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_experiment_tag.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a tag. — delete_experiment_tag","text":"","code":"delete_experiment_tag(client, run_id, key)  experimentsDeleteTag(client, run_id, key)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_experiment_tag.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a tag. — delete_experiment_tag","text":"client Required. Instance DatabricksClient() run_id Required. ID run tag logged . key Required. Name tag.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_external_location.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete an external location. — delete_external_location","title":"Delete an external location. — delete_external_location","text":"Deletes specified external location metastore. caller must owner external location.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_external_location.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete an external location. — delete_external_location","text":"","code":"delete_external_location(client, name, force = NULL)  externalLocationsDelete(client, name, force = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_external_location.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete an external location. — delete_external_location","text":"client Required. Instance DatabricksClient() name Required. Name external location. force Force deletion even dependent external tables mounts.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a file. — delete_file","title":"Delete a file. — delete_file","text":"Deletes file. request successful, response body.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a file. — delete_file","text":"","code":"delete_file(client, file_path)  filesDelete(client, file_path)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a file. — delete_file","text":"client Required. Instance DatabricksClient() file_path Required. absolute path file.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_file_directory.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a directory. — delete_file_directory","title":"Delete a directory. — delete_file_directory","text":"Deletes empty directory.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_file_directory.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a directory. — delete_file_directory","text":"","code":"delete_file_directory(client, directory_path)  filesDeleteDirectory(client, directory_path)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_file_directory.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a directory. — delete_file_directory","text":"client Required. Instance DatabricksClient() directory_path Required. absolute path directory.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_file_directory.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete a directory. — delete_file_directory","text":"delete non-empty directory, first delete contents. can done listing directory contents deleting file subdirectory recursively.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a function. — delete_function","title":"Delete a function. — delete_function","text":"Deletes function matches supplied name. deletion succeed, user must satisfy one following conditions: - owner function's parent catalog - owner function's parent schema USE_CATALOG privilege parent catalog - owner function USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a function. — delete_function","text":"","code":"delete_function(client, name, force = NULL)  functionsDelete(client, name, force = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a function. — delete_function","text":"client Required. Instance DatabricksClient() name Required. fully-qualified name function (form catalog_name.schema_name.function__name). force Force deletion even function notempty.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_git_credential.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a credential. — delete_git_credential","title":"Delete a credential. — delete_git_credential","text":"Deletes specified Git credential.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_git_credential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a credential. — delete_git_credential","text":"","code":"delete_git_credential(client, credential_id)  gitCredentialsDelete(client, credential_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_git_credential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a credential. — delete_git_credential","text":"client Required. Instance DatabricksClient() credential_id Required. ID corresponding credential access.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_global_init_script.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete init script. — delete_global_init_script","title":"Delete init script. — delete_global_init_script","text":"Deletes global init script.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_global_init_script.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete init script. — delete_global_init_script","text":"","code":"delete_global_init_script(client, script_id)  globalInitScriptsDelete(client, script_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_global_init_script.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete init script. — delete_global_init_script","text":"client Required. Instance DatabricksClient() script_id Required. ID global init script.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a group. — delete_group","title":"Delete a group. — delete_group","text":"Deletes group Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a group. — delete_group","text":"","code":"delete_group(client, id)  groupsDelete(client, id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a group. — delete_group","text":"client Required. Instance DatabricksClient() id Required. Unique ID group Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_instance_pool.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete an instance pool. — delete_instance_pool","title":"Delete an instance pool. — delete_instance_pool","text":"Deletes instance pool permanently. idle instances pool terminated asynchronously.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_instance_pool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete an instance pool. — delete_instance_pool","text":"","code":"delete_instance_pool(client, instance_pool_id)  instancePoolsDelete(client, instance_pool_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_instance_pool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete an instance pool. — delete_instance_pool","text":"client Required. Instance DatabricksClient() instance_pool_id Required. instance pool terminated.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_ip_access_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete access list. — delete_ip_access_list","title":"Delete access list. — delete_ip_access_list","text":"Deletes IP access list, specified list ID.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_ip_access_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete access list. — delete_ip_access_list","text":"","code":"delete_ip_access_list(client, ip_access_list_id)  ipAccessListsDelete(client, ip_access_list_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_ip_access_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete access list. — delete_ip_access_list","text":"client Required. Instance DatabricksClient() ip_access_list_id Required. ID corresponding IP access list.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_job.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a job. — delete_job","title":"Delete a job. — delete_job","text":"Deletes job.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_job.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a job. — delete_job","text":"","code":"delete_job(client, job_id)  jobsDelete(client, job_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_job.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a job. — delete_job","text":"client Required. Instance DatabricksClient() job_id Required. canonical identifier job delete.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_job_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a job run. — delete_job_run","title":"Delete a job run. — delete_job_run","text":"Deletes non-active run. Returns error run active.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_job_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a job run. — delete_job_run","text":"","code":"delete_job_run(client, run_id)  jobsDeleteRun(client, run_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_job_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a job run. — delete_job_run","text":"client Required. Instance DatabricksClient() run_id Required. canonical identifier run retrieve metadata.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_lakehouse_monitor.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a table monitor. — delete_lakehouse_monitor","title":"Delete a table monitor. — delete_lakehouse_monitor","text":"Deletes monitor specified table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_lakehouse_monitor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a table monitor. — delete_lakehouse_monitor","text":"","code":"delete_lakehouse_monitor(client, full_name)  lakehouseMonitorsDelete(client, full_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_lakehouse_monitor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a table monitor. — delete_lakehouse_monitor","text":"client Required. Instance DatabricksClient() full_name Required. Full name table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_lakehouse_monitor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete a table monitor. — delete_lakehouse_monitor","text":"caller must either: 1. owner table's parent catalog 2. USE_CATALOG table's parent catalog owner table's parent schema 3. following permissions: - USE_CATALOG table's parent catalog - USE_SCHEMA table's parent schema - owner table. Additionally, call must made workspace monitor created. Note metric tables dashboard deleted part call; assets must manually cleaned (desired).","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_metastore.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a metastore. — delete_metastore","title":"Delete a metastore. — delete_metastore","text":"Deletes metastore. caller must metastore admin.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_metastore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a metastore. — delete_metastore","text":"","code":"delete_metastore(client, id, force = NULL)  metastoresDelete(client, id, force = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_metastore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a metastore. — delete_metastore","text":"client Required. Instance DatabricksClient() id Required. Unique ID metastore. force Force deletion even metastore empty.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a model. — delete_model","title":"Delete a model. — delete_model","text":"Deletes registered model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a model. — delete_model","text":"","code":"delete_model(client, name)  modelRegistryDeleteModel(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a model. — delete_model","text":"client Required. Instance DatabricksClient() name Required. Registered model unique name identifier.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_comment.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a comment. — delete_model_comment","title":"Delete a comment. — delete_model_comment","text":"Deletes comment model version.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_comment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a comment. — delete_model_comment","text":"","code":"delete_model_comment(client, id)  modelRegistryDeleteComment(client, id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_comment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a comment. — delete_model_comment","text":"client Required. Instance DatabricksClient() id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_tag.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a model tag. — delete_model_tag","title":"Delete a model tag. — delete_model_tag","text":"Deletes tag registered model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_tag.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a model tag. — delete_model_tag","text":"","code":"delete_model_tag(client, name, key)  modelRegistryDeleteModelTag(client, name, key)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_tag.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a model tag. — delete_model_tag","text":"client Required. Instance DatabricksClient() name Required. Name registered model tag logged . key Required. Name tag.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_transition_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a transition request. — delete_model_transition_request","title":"Delete a transition request. — delete_model_transition_request","text":"Cancels model version stage transition request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_transition_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a transition request. — delete_model_transition_request","text":"","code":"delete_model_transition_request(   client,   name,   version,   stage,   creator,   comment = NULL )  modelRegistryDeleteTransitionRequest(   client,   name,   version,   stage,   creator,   comment = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_transition_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a transition request. — delete_model_transition_request","text":"client Required. Instance DatabricksClient() name Required. Name model. version Required. Version model. stage Required. Target stage transition request. creator Required. Username user created request. comment User-provided comment action.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_version.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a model version. — delete_model_version","title":"Delete a model version. — delete_model_version","text":"Deletes model version. Deletes model version specified registered model. aliases assigned model version also deleted.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_version.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a model version. — delete_model_version","text":"","code":"delete_model_version(client, full_name, version)  modelRegistryDeleteModelVersion(client, name, version)  delete_model_version(client, full_name, version)  modelVersionsDelete(client, full_name, version)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_version.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a model version. — delete_model_version","text":"client Required. Instance DatabricksClient() full_name Required. three-level (fully qualified) name model version. version Required. integer version number model version. name Required. Name registered model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_version.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete a model version. — delete_model_version","text":"caller must metastore admin owner parent registered model. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_version_tag.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a model version tag. — delete_model_version_tag","title":"Delete a model version tag. — delete_model_version_tag","text":"Deletes model version tag.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_version_tag.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a model version tag. — delete_model_version_tag","text":"","code":"delete_model_version_tag(client, name, version, key)  modelRegistryDeleteModelVersionTag(client, name, version, key)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_version_tag.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a model version tag. — delete_model_version_tag","text":"client Required. Instance DatabricksClient() name Required. Name registered model tag logged . version Required. Model version number tag logged . key Required. Name tag.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_webhook.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a webhook. — delete_model_webhook","title":"Delete a webhook. — delete_model_webhook","text":"NOTE: endpoint Public Preview.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_webhook.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a webhook. — delete_model_webhook","text":"","code":"delete_model_webhook(client, id = NULL)  modelRegistryDeleteWebhook(client, id = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_webhook.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a webhook. — delete_model_webhook","text":"client Required. Instance DatabricksClient() id Webhook ID required delete registry webhook.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_model_webhook.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete a webhook. — delete_model_webhook","text":"Deletes registry webhook.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_notebook.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a workspace object. — delete_notebook","title":"Delete a workspace object. — delete_notebook","text":"Deletes object directory (optionally recursively deletes objects directory). * path exist, call returns error RESOURCE_DOES_NOT_EXIST. * path non-empty directory recursive set false, call returns error DIRECTORY_NOT_EMPTY.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_notebook.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a workspace object. — delete_notebook","text":"","code":"delete_notebook(client, path, recursive = NULL)  workspaceDelete(client, path, recursive = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_notebook.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a workspace object. — delete_notebook","text":"client Required. Instance DatabricksClient() path Required. absolute path notebook directory. recursive flag specifies whether delete object recursively.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_notebook.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete a workspace object. — delete_notebook","text":"Object deletion undone deleting directory recursively atomic.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_online_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete an Online Table. — delete_online_table","title":"Delete an Online Table. — delete_online_table","text":"Delete online table. Warning: delete data online table. source Delta table deleted modified since Online Table created, lose data forever!","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_online_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete an Online Table. — delete_online_table","text":"","code":"delete_online_table(client, name)  onlineTablesDelete(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_online_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete an Online Table. — delete_online_table","text":"client Required. Instance DatabricksClient() name Required. Full three-part (catalog, schema, table) name table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a pipeline. — delete_pipeline","title":"Delete a pipeline. — delete_pipeline","text":"Deletes pipeline.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a pipeline. — delete_pipeline","text":"","code":"delete_pipeline(client, pipeline_id)  pipelinesDelete(client, pipeline_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a pipeline. — delete_pipeline","text":"client Required. Instance DatabricksClient() pipeline_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_provider.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a provider. — delete_provider","title":"Delete a provider. — delete_provider","text":"Deletes authentication provider, caller metastore admin owner provider.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_provider.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a provider. — delete_provider","text":"","code":"delete_provider(client, name)  providersDelete(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_provider.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a provider. — delete_provider","text":"client Required. Instance DatabricksClient() name Required. Name provider.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a query. — delete_query","title":"Delete a query. — delete_query","text":"Moves query trash. Trashed queries immediately disappear searches list views, used alerts. trash deleted 30 days.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a query. — delete_query","text":"","code":"delete_query(client, query_id)  queriesDelete(client, query_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a query. — delete_query","text":"client Required. Instance DatabricksClient() query_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_query_visualization.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove visualization. — delete_query_visualization","title":"Remove visualization. — delete_query_visualization","text":"Remove visualization.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_query_visualization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove visualization. — delete_query_visualization","text":"","code":"delete_query_visualization(client, id)  queryVisualizationsDelete(client, id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_query_visualization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove visualization. — delete_query_visualization","text":"client Required. Instance DatabricksClient() id Required. Widget ID returned :method:queryvizualisations/create.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_recipient.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a share recipient. — delete_recipient","title":"Delete a share recipient. — delete_recipient","text":"Deletes specified recipient metastore. caller must owner recipient.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_recipient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a share recipient. — delete_recipient","text":"","code":"delete_recipient(client, name)  recipientsDelete(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_recipient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a share recipient. — delete_recipient","text":"client Required. Instance DatabricksClient() name Required. Name recipient.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_registered_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a Registered Model. — delete_registered_model","title":"Delete a Registered Model. — delete_registered_model","text":"Deletes registered model model versions specified parent catalog schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_registered_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a Registered Model. — delete_registered_model","text":"","code":"delete_registered_model(client, full_name)  registeredModelsDelete(client, full_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_registered_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a Registered Model. — delete_registered_model","text":"client Required. Instance DatabricksClient() full_name Required. three-level (fully qualified) name registered model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_registered_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete a Registered Model. — delete_registered_model","text":"caller must metastore admin owner registered model. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_registered_model_alias.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a Registered Model Alias. — delete_registered_model_alias","title":"Delete a Registered Model Alias. — delete_registered_model_alias","text":"Deletes registered model alias.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_registered_model_alias.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a Registered Model Alias. — delete_registered_model_alias","text":"","code":"delete_registered_model_alias(client, full_name, alias)  registeredModelsDeleteAlias(client, full_name, alias)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_registered_model_alias.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a Registered Model Alias. — delete_registered_model_alias","text":"client Required. Instance DatabricksClient() full_name Required. three-level (fully qualified) name registered model. alias Required. name alias.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_registered_model_alias.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete a Registered Model Alias. — delete_registered_model_alias","text":"caller must metastore admin owner registered model. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_repo.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a repo. — delete_repo","title":"Delete a repo. — delete_repo","text":"Deletes specified repo.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_repo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a repo. — delete_repo","text":"","code":"delete_repo(client, repo_id)  reposDelete(client, repo_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_repo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a repo. — delete_repo","text":"client Required. Instance DatabricksClient() repo_id Required. ID corresponding repo access.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_restrict_workspace_admin.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete the restrict workspace admins setting. — delete_restrict_workspace_admin","title":"Delete the restrict workspace admins setting. — delete_restrict_workspace_admin","text":"Reverts restrict workspace admins setting status workspace. fresh etag needs provided DELETE requests (query parameter). etag can retrieved making GET request DELETE request. setting updated/deleted concurrently, DELETE fails 409 request must retried using fresh etag 409 response.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_restrict_workspace_admin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete the restrict workspace admins setting. — delete_restrict_workspace_admin","text":"","code":"delete_restrict_workspace_admin(client, etag = NULL)  restrictWorkspaceAdminsDelete(client, etag = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_restrict_workspace_admin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete the restrict workspace admins setting. — delete_restrict_workspace_admin","text":"client Required. Instance DatabricksClient() etag etag used versioning.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a schema. — delete_schema","title":"Delete a schema. — delete_schema","text":"Deletes specified schema parent catalog. caller must owner schema owner parent catalog.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a schema. — delete_schema","text":"","code":"delete_schema(client, full_name)  schemasDelete(client, full_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a schema. — delete_schema","text":"client Required. Instance DatabricksClient() full_name Required. Full name schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_secret.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a secret. — delete_secret","title":"Delete a secret. — delete_secret","text":"Deletes secret stored secret scope. must WRITE MANAGE permission secret scope.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_secret.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a secret. — delete_secret","text":"","code":"delete_secret(client, scope, key)  secretsDeleteSecret(client, scope, key)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_secret.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a secret. — delete_secret","text":"client Required. Instance DatabricksClient() scope Required. name scope contains secret delete. key Required. Name secret delete.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_secret.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete a secret. — delete_secret","text":"Throws RESOURCE_DOES_NOT_EXIST secret scope secret exists. Throws PERMISSION_DENIED user permission make API call.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_secret_acl.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete an ACL. — delete_secret_acl","title":"Delete an ACL. — delete_secret_acl","text":"Deletes given ACL given scope.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_secret_acl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete an ACL. — delete_secret_acl","text":"","code":"delete_secret_acl(client, scope, principal)  secretsDeleteAcl(client, scope, principal)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_secret_acl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete an ACL. — delete_secret_acl","text":"client Required. Instance DatabricksClient() scope Required. name scope remove permissions . principal Required. principal remove existing ACL .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_secret_acl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete an ACL. — delete_secret_acl","text":"Users must MANAGE permission invoke API. Throws RESOURCE_DOES_NOT_EXIST secret scope, principal, ACL exists. Throws PERMISSION_DENIED user permission make API call.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_secret_scope.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a secret scope. — delete_secret_scope","title":"Delete a secret scope. — delete_secret_scope","text":"Deletes secret scope.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_secret_scope.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a secret scope. — delete_secret_scope","text":"","code":"delete_secret_scope(client, scope)  secretsDeleteScope(client, scope)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_secret_scope.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a secret scope. — delete_secret_scope","text":"client Required. Instance DatabricksClient() scope Required. Name scope delete.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_secret_scope.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete a secret scope. — delete_secret_scope","text":"Throws RESOURCE_DOES_NOT_EXIST scope exist. Throws PERMISSION_DENIED user permission make API call.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_service_principal.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a service principal. — delete_service_principal","title":"Delete a service principal. — delete_service_principal","text":"Delete single service principal Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_service_principal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a service principal. — delete_service_principal","text":"","code":"delete_service_principal(client, id)  servicePrincipalsDelete(client, id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_service_principal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a service principal. — delete_service_principal","text":"client Required. Instance DatabricksClient() id Required. Unique ID service principal Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_serving_endpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a serving endpoint. — delete_serving_endpoint","title":"Delete a serving endpoint. — delete_serving_endpoint","text":"Delete serving endpoint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_serving_endpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a serving endpoint. — delete_serving_endpoint","text":"","code":"delete_serving_endpoint(client, name)  servingEndpointsDelete(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_serving_endpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a serving endpoint. — delete_serving_endpoint","text":"client Required. Instance DatabricksClient() name Required. name serving endpoint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_share.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a share. — delete_share","title":"Delete a share. — delete_share","text":"Deletes data object share metastore. caller must owner share.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_share.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a share. — delete_share","text":"","code":"delete_share(client, name)  sharesDelete(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_share.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a share. — delete_share","text":"client Required. Instance DatabricksClient() name Required. name share.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_storage_credential.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a credential. — delete_storage_credential","title":"Delete a credential. — delete_storage_credential","text":"Deletes storage credential metastore. caller must owner storage credential.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_storage_credential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a credential. — delete_storage_credential","text":"","code":"delete_storage_credential(client, name, force = NULL)  storageCredentialsDelete(client, name, force = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_storage_credential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a credential. — delete_storage_credential","text":"client Required. Instance DatabricksClient() name Required. Name storage credential. force Force deletion even dependent external locations external tables.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a table. — delete_table","title":"Delete a table. — delete_table","text":"Deletes table specified parent catalog schema. caller must owner parent catalog, USE_CATALOG privilege parent catalog owner parent schema, owner table USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a table. — delete_table","text":"","code":"delete_table(client, full_name)  tablesDelete(client, full_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a table. — delete_table","text":"client Required. Instance DatabricksClient() full_name Required. Full name table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_table_constraint.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a table constraint. — delete_table_constraint","title":"Delete a table constraint. — delete_table_constraint","text":"Deletes table constraint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_table_constraint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a table constraint. — delete_table_constraint","text":"","code":"delete_table_constraint(client, full_name, constraint_name, cascade)  tableConstraintsDelete(client, full_name, constraint_name, cascade)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_table_constraint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a table constraint. — delete_table_constraint","text":"client Required. Instance DatabricksClient() full_name Required. Full name table referenced constraint. constraint_name Required. name constraint delete. cascade Required. true, try deleting child constraints current constraint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_table_constraint.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete a table constraint. — delete_table_constraint","text":"table constraint deletion succeed, user must satisfy conditions: - user must USE_CATALOG privilege table's parent catalog, USE_SCHEMA privilege table's parent schema, owner table. - cascade argument true, user must following permissions child tables: USE_CATALOG privilege table's catalog, USE_SCHEMA privilege table's schema, owner table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_token.html","id":null,"dir":"Reference","previous_headings":"","what":"Revoke token. — delete_token","title":"Revoke token. — delete_token","text":"Revokes access token.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_token.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Revoke token. — delete_token","text":"","code":"delete_token(client, token_id)  tokensDelete(client, token_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_token.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Revoke token. — delete_token","text":"client Required. Instance DatabricksClient() token_id Required. ID token revoked.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_token.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Revoke token. — delete_token","text":"token specified ID valid, call returns error RESOURCE_DOES_NOT_EXIST.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_token_management.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a token. — delete_token_management","title":"Delete a token. — delete_token_management","text":"Deletes token, specified ID.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_token_management.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a token. — delete_token_management","text":"","code":"delete_token_management(client, token_id)  tokenManagementDelete(client, token_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_token_management.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a token. — delete_token_management","text":"client Required. Instance DatabricksClient() token_id Required. ID token get.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_user.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a user. — delete_user","title":"Delete a user. — delete_user","text":"Deletes user. Deleting user Databricks workspace also removes objects associated user.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_user.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a user. — delete_user","text":"","code":"delete_user(client, id)  usersDelete(client, id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_user.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a user. — delete_user","text":"client Required. Instance DatabricksClient() id Required. Unique ID user Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_vector_search_endpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete an endpoint. — delete_vector_search_endpoint","title":"Delete an endpoint. — delete_vector_search_endpoint","text":"Delete endpoint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_vector_search_endpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete an endpoint. — delete_vector_search_endpoint","text":"","code":"delete_vector_search_endpoint(client, endpoint_name)  vectorSearchEndpointsDeleteEndpoint(client, endpoint_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_vector_search_endpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete an endpoint. — delete_vector_search_endpoint","text":"client Required. Instance DatabricksClient() endpoint_name Required. Name endpoint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_vector_search_index.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete an index. — delete_vector_search_index","title":"Delete an index. — delete_vector_search_index","text":"Delete index.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_vector_search_index.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete an index. — delete_vector_search_index","text":"","code":"delete_vector_search_index(client, index_name)  vectorSearchIndexesDeleteIndex(client, index_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_vector_search_index.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete an index. — delete_vector_search_index","text":"client Required. Instance DatabricksClient() index_name Required. Name index.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_vector_search_index_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete data from index. — delete_vector_search_index_data","title":"Delete data from index. — delete_vector_search_index_data","text":"Handles deletion data specified vector index.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_vector_search_index_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete data from index. — delete_vector_search_index_data","text":"","code":"delete_vector_search_index_data(client, index_name, primary_keys)  vectorSearchIndexesDeleteDataVectorIndex(client, index_name, primary_keys)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_vector_search_index_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete data from index. — delete_vector_search_index_data","text":"client Required. Instance DatabricksClient() index_name Required. Name vector index data deleted. primary_keys Required. List primary keys data deleted.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_volume.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a Volume. — delete_volume","title":"Delete a Volume. — delete_volume","text":"Deletes volume specified parent catalog schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_volume.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a Volume. — delete_volume","text":"","code":"delete_volume(client, name)  volumesDelete(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_volume.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a Volume. — delete_volume","text":"client Required. Instance DatabricksClient() name Required. three-level (fully qualified) name volume.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_volume.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete a Volume. — delete_volume","text":"caller must metastore admin owner volume. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_warehouse.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a warehouse. — delete_warehouse","title":"Delete a warehouse. — delete_warehouse","text":"Deletes SQL warehouse.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_warehouse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a warehouse. — delete_warehouse","text":"","code":"delete_warehouse(client, id)  warehousesDelete(client, id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/delete_warehouse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a warehouse. — delete_warehouse","text":"client Required. Instance DatabricksClient() id Required. Required.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/destroy_command_execution.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete an execution context. — destroy_command_execution","title":"Delete an execution context. — destroy_command_execution","text":"Deletes execution context.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/destroy_command_execution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete an execution context. — destroy_command_execution","text":"","code":"destroy_command_execution(client, cluster_id, context_id)  commandExecutionDestroy(client, cluster_id, context_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/destroy_command_execution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete an execution context. — destroy_command_execution","text":"client Required. Instance DatabricksClient() cluster_id Required. field description yet. context_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/disable_system_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Disable a system schema. — disable_system_schema","title":"Disable a system schema. — disable_system_schema","text":"Disables system schema removes system catalog. caller must account admin metastore admin.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/disable_system_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Disable a system schema. — disable_system_schema","text":"","code":"disable_system_schema(client, metastore_id, schema_name)  systemSchemasDisable(client, metastore_id, schema_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/disable_system_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Disable a system schema. — disable_system_schema","text":"client Required. Instance DatabricksClient() metastore_id Required. metastore ID system schema lives. schema_name Required. Full name system schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/download_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Download a file. — download_file","title":"Download a file. — download_file","text":"Downloads file 5 GiB. file contents response body. standard HTTP file download, JSON RPC.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/download_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download a file. — download_file","text":"","code":"download_file(client, file_path)  filesDownload(client, file_path)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/download_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download a file. — download_file","text":"client Required. Instance DatabricksClient() file_path Required. absolute path file.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Update cluster configuration. — edit_cluster","title":"Update cluster configuration. — edit_cluster","text":"Updates configuration cluster match provided attributes size. cluster can updated RUNNING TERMINATED state.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update cluster configuration. — edit_cluster","text":"","code":"edit_cluster(   client,   cluster_id,   spark_version,   apply_policy_default_values = NULL,   autoscale = NULL,   autotermination_minutes = NULL,   aws_attributes = NULL,   azure_attributes = NULL,   cluster_log_conf = NULL,   cluster_name = NULL,   cluster_source = NULL,   custom_tags = NULL,   data_security_mode = NULL,   docker_image = NULL,   driver_instance_pool_id = NULL,   driver_node_type_id = NULL,   enable_elastic_disk = NULL,   enable_local_disk_encryption = NULL,   gcp_attributes = NULL,   init_scripts = NULL,   instance_pool_id = NULL,   node_type_id = NULL,   num_workers = NULL,   policy_id = NULL,   runtime_engine = NULL,   single_user_name = NULL,   spark_conf = NULL,   spark_env_vars = NULL,   ssh_public_keys = NULL,   workload_type = NULL )  clustersEdit(   client,   cluster_id,   spark_version,   apply_policy_default_values = NULL,   autoscale = NULL,   autotermination_minutes = NULL,   aws_attributes = NULL,   azure_attributes = NULL,   cluster_log_conf = NULL,   cluster_name = NULL,   cluster_source = NULL,   custom_tags = NULL,   data_security_mode = NULL,   docker_image = NULL,   driver_instance_pool_id = NULL,   driver_node_type_id = NULL,   enable_elastic_disk = NULL,   enable_local_disk_encryption = NULL,   gcp_attributes = NULL,   init_scripts = NULL,   instance_pool_id = NULL,   node_type_id = NULL,   num_workers = NULL,   policy_id = NULL,   runtime_engine = NULL,   single_user_name = NULL,   spark_conf = NULL,   spark_env_vars = NULL,   ssh_public_keys = NULL,   workload_type = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update cluster configuration. — edit_cluster","text":"client Required. Instance DatabricksClient() cluster_id Required. ID cluser. spark_version Required. Spark version cluster, e.g. apply_policy_default_values field description yet. autoscale Parameters needed order automatically scale clusters based load. autotermination_minutes Automatically terminates cluster inactive time minutes. aws_attributes Attributes related clusters running Amazon Web Services. azure_attributes Attributes related clusters running Microsoft Azure. cluster_log_conf configuration delivering spark logs long-term storage destination. cluster_name Cluster name requested user. cluster_source Determines whether cluster created user UI, created Databricks Jobs Scheduler, API request. custom_tags Additional tags cluster resources. data_security_mode Data security mode decides data governance model use accessing data cluster. docker_image field description yet. driver_instance_pool_id optional ID instance pool driver cluster belongs. driver_node_type_id node type Spark driver. enable_elastic_disk Autoscaling Local Storage: enabled, cluster dynamically acquire additional disk space Spark workers running low disk space. enable_local_disk_encryption Whether enable LUKS cluster VMs' local disks. gcp_attributes Attributes related clusters running Google Cloud Platform. init_scripts configuration storing init scripts. instance_pool_id optional ID instance pool cluster belongs. node_type_id field encodes, single value, resources available Spark nodes cluster. num_workers Number worker nodes cluster . policy_id ID cluster policy used create cluster applicable. runtime_engine Decides runtime engine use, e.g. single_user_name Single user name data_security_mode SINGLE_USER. spark_conf object containing set optional, user-specified Spark configuration key-value pairs. spark_env_vars object containing set optional, user-specified environment variable key-value pairs. ssh_public_keys SSH public key contents added Spark node cluster. workload_type field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_cluster.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update cluster configuration. — edit_cluster","text":"cluster updated RUNNING state, restarted new attributes can take effect. cluster updated TERMINATED state, remain TERMINATED. next time started using clusters/start API, new attributes take effect. attempt update cluster state rejected INVALID_STATE error code. Clusters created Databricks Jobs service edited.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_cluster_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Update cluster configuration. — edit_cluster_and_wait","title":"Update cluster configuration. — edit_cluster_and_wait","text":"long-running operation, blocks Clusters Databricks reach RUNNING state timeout 20 minutes, can change via timeout parameter. default, state Databricks Clusters reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_cluster_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update cluster configuration. — edit_cluster_and_wait","text":"","code":"edit_cluster_and_wait(   client,   cluster_id,   spark_version,   apply_policy_default_values = NULL,   autoscale = NULL,   autotermination_minutes = NULL,   aws_attributes = NULL,   azure_attributes = NULL,   cluster_log_conf = NULL,   cluster_name = NULL,   cluster_source = NULL,   custom_tags = NULL,   data_security_mode = NULL,   docker_image = NULL,   driver_instance_pool_id = NULL,   driver_node_type_id = NULL,   enable_elastic_disk = NULL,   enable_local_disk_encryption = NULL,   gcp_attributes = NULL,   init_scripts = NULL,   instance_pool_id = NULL,   node_type_id = NULL,   num_workers = NULL,   policy_id = NULL,   runtime_engine = NULL,   single_user_name = NULL,   spark_conf = NULL,   spark_env_vars = NULL,   ssh_public_keys = NULL,   workload_type = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_cluster_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update cluster configuration. — edit_cluster_and_wait","text":"client Required. Instance DatabricksClient() cluster_id Required. ID cluser. spark_version Required. Spark version cluster, e.g. apply_policy_default_values field description yet. autoscale Parameters needed order automatically scale clusters based load. autotermination_minutes Automatically terminates cluster inactive time minutes. aws_attributes Attributes related clusters running Amazon Web Services. azure_attributes Attributes related clusters running Microsoft Azure. cluster_log_conf configuration delivering spark logs long-term storage destination. cluster_name Cluster name requested user. cluster_source Determines whether cluster created user UI, created Databricks Jobs Scheduler, API request. custom_tags Additional tags cluster resources. data_security_mode Data security mode decides data governance model use accessing data cluster. docker_image field description yet. driver_instance_pool_id optional ID instance pool driver cluster belongs. driver_node_type_id node type Spark driver. enable_elastic_disk Autoscaling Local Storage: enabled, cluster dynamically acquire additional disk space Spark workers running low disk space. enable_local_disk_encryption Whether enable LUKS cluster VMs' local disks. gcp_attributes Attributes related clusters running Google Cloud Platform. init_scripts configuration storing init scripts. instance_pool_id optional ID instance pool cluster belongs. node_type_id field encodes, single value, resources available Spark nodes cluster. num_workers Number worker nodes cluster . policy_id ID cluster policy used create cluster applicable. runtime_engine Decides runtime engine use, e.g. single_user_name Single user name data_security_mode SINGLE_USER. spark_conf object containing set optional, user-specified Spark configuration key-value pairs. spark_env_vars object containing set optional, user-specified environment variable key-value pairs. ssh_public_keys SSH public key contents added Spark node cluster. workload_type field description yet. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_cluster_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update cluster configuration. — edit_cluster_and_wait","text":"Updates configuration cluster match provided attributes size. cluster can updated RUNNING TERMINATED state. cluster updated RUNNING state, restarted new attributes can take effect. cluster updated TERMINATED state, remain TERMINATED. next time started using clusters/start API, new attributes take effect. attempt update cluster state rejected INVALID_STATE error code. Clusters created Databricks Jobs service edited.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_cluster_policy.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a cluster policy. — edit_cluster_policy","title":"Update a cluster policy. — edit_cluster_policy","text":"Update existing policy cluster. operation may make clusters governed previous policy invalid.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_cluster_policy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a cluster policy. — edit_cluster_policy","text":"","code":"edit_cluster_policy(   client,   policy_id,   name,   definition = NULL,   description = NULL,   libraries = NULL,   max_clusters_per_user = NULL,   policy_family_definition_overrides = NULL,   policy_family_id = NULL )  clusterPoliciesEdit(   client,   policy_id,   name,   definition = NULL,   description = NULL,   libraries = NULL,   max_clusters_per_user = NULL,   policy_family_definition_overrides = NULL,   policy_family_id = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_cluster_policy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a cluster policy. — edit_cluster_policy","text":"client Required. Instance DatabricksClient() policy_id Required. ID policy update. name Required. Cluster Policy name requested user. definition Policy definition document expressed Databricks Cluster Policy Definition Language. description Additional human-readable description cluster policy. libraries list libraries installed next cluster restart uses policy. max_clusters_per_user Max number clusters per user can active using policy. policy_family_definition_overrides Policy definition JSON document expressed Databricks Policy Definition Language. policy_family_id ID policy family.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_instance_pool.html","id":null,"dir":"Reference","previous_headings":"","what":"Edit an existing instance pool. — edit_instance_pool","title":"Edit an existing instance pool. — edit_instance_pool","text":"Modifies configuration existing instance pool.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_instance_pool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Edit an existing instance pool. — edit_instance_pool","text":"","code":"edit_instance_pool(   client,   instance_pool_id,   instance_pool_name,   node_type_id,   custom_tags = NULL,   idle_instance_autotermination_minutes = NULL,   max_capacity = NULL,   min_idle_instances = NULL )  instancePoolsEdit(   client,   instance_pool_id,   instance_pool_name,   node_type_id,   custom_tags = NULL,   idle_instance_autotermination_minutes = NULL,   max_capacity = NULL,   min_idle_instances = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_instance_pool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Edit an existing instance pool. — edit_instance_pool","text":"client Required. Instance DatabricksClient() instance_pool_id Required. Instance pool ID. instance_pool_name Required. Pool name requested user. node_type_id Required. field encodes, single value, resources available Spark nodes cluster. custom_tags Additional tags pool resources. idle_instance_autotermination_minutes Automatically terminates extra instances pool cache inactive time minutes min_idle_instances requirement already met. max_capacity Maximum number outstanding instances keep pool, including instances used clusters idle instances. min_idle_instances Minimum number idle instances keep instance pool.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_instance_profile.html","id":null,"dir":"Reference","previous_headings":"","what":"Edit an instance profile. — edit_instance_profile","title":"Edit an instance profile. — edit_instance_profile","text":"supported field change optional IAM role ARN associated instance profile. required specify IAM role ARN following true:","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_instance_profile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Edit an instance profile. — edit_instance_profile","text":"","code":"edit_instance_profile(   client,   instance_profile_arn,   iam_role_arn = NULL,   is_meta_instance_profile = NULL )  instanceProfilesEdit(   client,   instance_profile_arn,   iam_role_arn = NULL,   is_meta_instance_profile = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_instance_profile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Edit an instance profile. — edit_instance_profile","text":"client Required. Instance DatabricksClient() instance_profile_arn Required. AWS ARN instance profile register Databricks. iam_role_arn AWS IAM role ARN role associated instance profile. is_meta_instance_profile Boolean flag indicating whether instance profile used credential passthrough scenarios.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_instance_profile.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Edit an instance profile. — edit_instance_profile","text":"role name instance profile name match. name part last slash ARN. * want use instance profile Databricks SQL Serverless. understand fields AWS console, see Enable serverless SQL warehouses. API available admin users.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_warehouse.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a warehouse. — edit_warehouse","title":"Update a warehouse. — edit_warehouse","text":"Updates configuration SQL warehouse.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_warehouse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a warehouse. — edit_warehouse","text":"","code":"edit_warehouse(   client,   id,   auto_stop_mins = NULL,   channel = NULL,   cluster_size = NULL,   creator_name = NULL,   enable_photon = NULL,   enable_serverless_compute = NULL,   instance_profile_arn = NULL,   max_num_clusters = NULL,   min_num_clusters = NULL,   name = NULL,   spot_instance_policy = NULL,   tags = NULL,   warehouse_type = NULL )  warehousesEdit(   client,   id,   auto_stop_mins = NULL,   channel = NULL,   cluster_size = NULL,   creator_name = NULL,   enable_photon = NULL,   enable_serverless_compute = NULL,   instance_profile_arn = NULL,   max_num_clusters = NULL,   min_num_clusters = NULL,   name = NULL,   spot_instance_policy = NULL,   tags = NULL,   warehouse_type = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_warehouse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a warehouse. — edit_warehouse","text":"client Required. Instance DatabricksClient() id Required. Required. auto_stop_mins amount time minutes SQL warehouse must idle (.e., RUNNING queries) automatically stopped. channel Channel Details. cluster_size Size clusters allocated warehouse. creator_name warehouse creator name. enable_photon Configures whether warehouse use Photon optimized clusters. enable_serverless_compute Configures whether warehouse use serverless compute. instance_profile_arn Deprecated. max_num_clusters Maximum number clusters autoscaler create handle concurrent queries. min_num_clusters Minimum number available clusters maintained SQL warehouse. name Logical name cluster. spot_instance_policy Configurations whether warehouse use spot instances. tags set key-value pairs tagged resources (e.g., AWS instances EBS volumes) associated SQL warehouse. warehouse_type Warehouse type: PRO CLASSIC.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_warehouse_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a warehouse. — edit_warehouse_and_wait","title":"Update a warehouse. — edit_warehouse_and_wait","text":"long-running operation, blocks Warehouses Databricks reach RUNNING state timeout 20 minutes, can change via timeout parameter. default, state Databricks Warehouses reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_warehouse_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a warehouse. — edit_warehouse_and_wait","text":"","code":"edit_warehouse_and_wait(   client,   id,   auto_stop_mins = NULL,   channel = NULL,   cluster_size = NULL,   creator_name = NULL,   enable_photon = NULL,   enable_serverless_compute = NULL,   instance_profile_arn = NULL,   max_num_clusters = NULL,   min_num_clusters = NULL,   name = NULL,   spot_instance_policy = NULL,   tags = NULL,   warehouse_type = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_warehouse_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a warehouse. — edit_warehouse_and_wait","text":"client Required. Instance DatabricksClient() id Required. Required. auto_stop_mins amount time minutes SQL warehouse must idle (.e., RUNNING queries) automatically stopped. channel Channel Details. cluster_size Size clusters allocated warehouse. creator_name warehouse creator name. enable_photon Configures whether warehouse use Photon optimized clusters. enable_serverless_compute Configures whether warehouse use serverless compute. instance_profile_arn Deprecated. max_num_clusters Maximum number clusters autoscaler create handle concurrent queries. min_num_clusters Minimum number available clusters maintained SQL warehouse. name Logical name cluster. spot_instance_policy Configurations whether warehouse use spot instances. tags set key-value pairs tagged resources (e.g., AWS instances EBS volumes) associated SQL warehouse. warehouse_type Warehouse type: PRO CLASSIC. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/edit_warehouse_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update a warehouse. — edit_warehouse_and_wait","text":"Updates configuration SQL warehouse.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/enable_system_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Enable a system schema. — enable_system_schema","title":"Enable a system schema. — enable_system_schema","text":"Enables system schema adds system catalog. caller must account admin metastore admin.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/enable_system_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enable a system schema. — enable_system_schema","text":"","code":"enable_system_schema(client, metastore_id, schema_name)  systemSchemasEnable(client, metastore_id, schema_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/enable_system_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Enable a system schema. — enable_system_schema","text":"client Required. Instance DatabricksClient() metastore_id Required. metastore ID system schema lives. schema_name Required. Full name system schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/events_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"List cluster activity events. — events_cluster","title":"List cluster activity events. — events_cluster","text":"Retrieves list events activity cluster. API paginated. events read, response includes nparameters necessary request next page events.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/events_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List cluster activity events. — events_cluster","text":"","code":"events_cluster(   client,   cluster_id,   end_time = NULL,   event_types = NULL,   limit = NULL,   offset = NULL,   order = NULL,   start_time = NULL )  clustersEvents(   client,   cluster_id,   end_time = NULL,   event_types = NULL,   limit = NULL,   offset = NULL,   order = NULL,   start_time = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/events_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List cluster activity events. — events_cluster","text":"client Required. Instance DatabricksClient() cluster_id Required. ID cluster retrieve events . end_time end time epoch milliseconds. event_types optional set event types filter . limit maximum number events include page events. offset offset result set. order order list events ; either 'ASC' 'DESC'. start_time start time epoch milliseconds.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/events_cluster.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List cluster activity events. — events_cluster","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/exchange_credentials_manager_token.html","id":null,"dir":"Reference","previous_headings":"","what":"Exchange token. — exchange_credentials_manager_token","title":"Exchange token. — exchange_credentials_manager_token","text":"Exchange tokens Identity Provider get new access token. allows specifying scopes determine token permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/exchange_credentials_manager_token.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exchange token. — exchange_credentials_manager_token","text":"","code":"exchange_credentials_manager_token(client, partition_id, token_type, scopes)  credentialsManagerExchangeToken(client, partition_id, token_type, scopes)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/exchange_credentials_manager_token.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exchange token. — exchange_credentials_manager_token","text":"client Required. Instance DatabricksClient() partition_id Required. partition Credentials store. token_type Required. list token types requested. scopes Required. Array scopes token request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/execute_command.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a command. — execute_command","title":"Run a command. — execute_command","text":"Runs cluster command given execution context, using provided language.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/execute_command.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a command. — execute_command","text":"","code":"execute_command(   client,   cluster_id = NULL,   command = NULL,   context_id = NULL,   language = NULL )  commandExecutionExecute(   client,   cluster_id = NULL,   command = NULL,   context_id = NULL,   language = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/execute_command.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a command. — execute_command","text":"client Required. Instance DatabricksClient() cluster_id Running cluster id. command Executable code. context_id Running context id. language field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/execute_command.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a command. — execute_command","text":"successful, returns ID tracking status command's execution.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/execute_command_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a command. — execute_command_and_wait","title":"Run a command. — execute_command_and_wait","text":"long-running operation, blocks Command Execution Databricks reach Finished Error state timeout 20 minutes, can change via timeout parameter. default, state Databricks Command Execution reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/execute_command_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a command. — execute_command_and_wait","text":"","code":"execute_command_and_wait(   client,   cluster_id = NULL,   command = NULL,   context_id = NULL,   language = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/execute_command_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a command. — execute_command_and_wait","text":"client Required. Instance DatabricksClient() cluster_id Running cluster id. command Executable code. context_id Running context id. language field description yet. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/execute_command_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a command. — execute_command_and_wait","text":"Runs cluster command given execution context, using provided language. successful, returns ID tracking status command's execution.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/execute_statement.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute a SQL statement. — execute_statement","title":"Execute a SQL statement. — execute_statement","text":"Execute SQL statement.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/execute_statement.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute a SQL statement. — execute_statement","text":"","code":"execute_statement(   client,   statement,   warehouse_id,   byte_limit = NULL,   catalog = NULL,   disposition = NULL,   format = NULL,   on_wait_timeout = NULL,   parameters = NULL,   row_limit = NULL,   schema = NULL,   wait_timeout = NULL )  statementExecutionExecuteStatement(   client,   statement,   warehouse_id,   byte_limit = NULL,   catalog = NULL,   disposition = NULL,   format = NULL,   on_wait_timeout = NULL,   parameters = NULL,   row_limit = NULL,   schema = NULL,   wait_timeout = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/execute_statement.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute a SQL statement. — execute_statement","text":"client Required. Instance DatabricksClient() statement Required. SQL statement execute. warehouse_id Required. Warehouse upon execute statement. byte_limit Applies given byte limit statement's result size. catalog Sets default catalog statement execution, similar USE CATALOG SQL. disposition fetch disposition provides two modes fetching results: INLINE EXTERNAL_LINKS. format Statement execution supports three result formats: JSON_ARRAY (default), ARROW_STREAM, CSV. on_wait_timeout wait_timeout > 0s, call block specified time. parameters list parameters pass SQL statement containing parameter markers. row_limit Applies given row limit statement's result set, unlike LIMIT clause SQL, also sets truncated field response indicate whether result trimmed due limit . schema Sets default schema statement execution, similar USE SCHEMA SQL. wait_timeout time seconds call wait statement's result set Ns, N can set 0 value 5 50.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/exists_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Get boolean reflecting if table exists. — exists_table","title":"Get boolean reflecting if table exists. — exists_table","text":"Gets table exists metastore specific catalog schema. caller must satisfy one following requirements: * metastore admin * owner parent catalog * owner parent schema USE_CATALOG privilege parent catalog * USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema, either table owner SELECT privilege table. * BROWSE privilege parent catalog BROWSE privilege parent schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/exists_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get boolean reflecting if table exists. — exists_table","text":"","code":"exists_table(client, full_name)  tablesExists(client, full_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/exists_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get boolean reflecting if table exists. — exists_table","text":"client Required. Instance DatabricksClient() full_name Required. Full name table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/export_job_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Export and retrieve a job run. — export_job_run","title":"Export and retrieve a job run. — export_job_run","text":"Export retrieve job run task.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/export_job_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Export and retrieve a job run. — export_job_run","text":"","code":"export_job_run(client, run_id, views_to_export = NULL)  jobsExportRun(client, run_id, views_to_export = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/export_job_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Export and retrieve a job run. — export_job_run","text":"client Required. Instance DatabricksClient() run_id Required. canonical identifier run. views_to_export views export (CODE, DASHBOARDS, ).","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/export_notebook.html","id":null,"dir":"Reference","previous_headings":"","what":"Export a workspace object. — export_notebook","title":"Export a workspace object. — export_notebook","text":"Exports object contents entire directory.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/export_notebook.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Export a workspace object. — export_notebook","text":"","code":"export_notebook(client, path, format = NULL)  workspaceExport(client, path, format = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/export_notebook.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Export a workspace object. — export_notebook","text":"client Required. Instance DatabricksClient() path Required. absolute path object directory. format specifies format exported file.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/export_notebook.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Export a workspace object. — export_notebook","text":"path exist, call returns error RESOURCE_DOES_NOT_EXIST. exported data exceed size limit, call returns MAX_NOTEBOOK_SIZE_EXCEEDED. Currently, API support exporting library.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/export_serving_endpoint_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Get metrics of a serving endpoint. — export_serving_endpoint_metrics","title":"Get metrics of a serving endpoint. — export_serving_endpoint_metrics","text":"Retrieves metrics associated provided serving endpoint either Prometheus OpenMetrics exposition format.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/export_serving_endpoint_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get metrics of a serving endpoint. — export_serving_endpoint_metrics","text":"","code":"export_serving_endpoint_metrics(client, name)  servingEndpointsExportMetrics(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/export_serving_endpoint_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get metrics of a serving endpoint. — export_serving_endpoint_metrics","text":"client Required. Instance DatabricksClient() name Required. name serving endpoint retrieve metrics .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_account_access_control_proxy_assignable_roles_for_resource.html","id":null,"dir":"Reference","previous_headings":"","what":"Get assignable roles for a resource. — get_account_access_control_proxy_assignable_roles_for_resource","title":"Get assignable roles for a resource. — get_account_access_control_proxy_assignable_roles_for_resource","text":"Gets roles can granted account-level resource. role grantable rule set resource can contain access rule role.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_account_access_control_proxy_assignable_roles_for_resource.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get assignable roles for a resource. — get_account_access_control_proxy_assignable_roles_for_resource","text":"","code":"get_account_access_control_proxy_assignable_roles_for_resource(   client,   resource )  accountAccessControlProxyGetAssignableRolesForResource(client, resource)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_account_access_control_proxy_assignable_roles_for_resource.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get assignable roles for a resource. — get_account_access_control_proxy_assignable_roles_for_resource","text":"client Required. Instance DatabricksClient() resource Required. resource name assignable roles listed.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_account_access_control_proxy_rule_set.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a rule set. — get_account_access_control_proxy_rule_set","title":"Get a rule set. — get_account_access_control_proxy_rule_set","text":"Get rule set name. rule set always attached resource contains list access rules said resource. Currently default rule set resource supported.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_account_access_control_proxy_rule_set.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a rule set. — get_account_access_control_proxy_rule_set","text":"","code":"get_account_access_control_proxy_rule_set(client, name, etag)  accountAccessControlProxyGetRuleSet(client, name, etag)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_account_access_control_proxy_rule_set.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a rule set. — get_account_access_control_proxy_rule_set","text":"client Required. Instance DatabricksClient() name Required. ruleset name associated request. etag Required. Etag used versioning.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_alert.html","id":null,"dir":"Reference","previous_headings":"","what":"Get an alert. — get_alert","title":"Get an alert. — get_alert","text":"Gets alert.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_alert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get an alert. — get_alert","text":"","code":"get_alert(client, alert_id)  alertsGet(client, alert_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_alert.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get an alert. — get_alert","text":"client Required. Instance DatabricksClient() alert_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_app.html","id":null,"dir":"Reference","previous_headings":"","what":"Get definition for an application. — get_app","title":"Get definition for an application. — get_app","text":"Get application definition","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_app.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get definition for an application. — get_app","text":"","code":"get_app(client, name)  appsGetApp(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_app.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get definition for an application. — get_app","text":"client Required. Instance DatabricksClient() name Required. name application.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_app_apps.html","id":null,"dir":"Reference","previous_headings":"","what":"List all applications. — get_app_apps","title":"List all applications. — get_app_apps","text":"List available applications","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_app_apps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List all applications. — get_app_apps","text":"","code":"get_app_apps(client)  appsGetApps(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_app_apps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List all applications. — get_app_apps","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_app_deployment_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Get deployment status for an application. — get_app_deployment_status","title":"Get deployment status for an application. — get_app_deployment_status","text":"Get deployment status application","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_app_deployment_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get deployment status for an application. — get_app_deployment_status","text":"","code":"get_app_deployment_status(client, deployment_id, include_app_log = NULL)  appsGetAppDeploymentStatus(client, deployment_id, include_app_log = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_app_deployment_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get deployment status for an application. — get_app_deployment_status","text":"client Required. Instance DatabricksClient() deployment_id Required. deployment id application. include_app_log Boolean flag include application logs.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_app_events.html","id":null,"dir":"Reference","previous_headings":"","what":"Get deployment events for an application. — get_app_events","title":"Get deployment events for an application. — get_app_events","text":"Get deployment events application","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_app_events.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get deployment events for an application. — get_app_events","text":"","code":"get_app_events(client, name)  appsGetEvents(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_app_events.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get deployment events for an application. — get_app_events","text":"client Required. Instance DatabricksClient() name Required. name application.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_artifact_allowlist.html","id":null,"dir":"Reference","previous_headings":"","what":"Get an artifact allowlist. — get_artifact_allowlist","title":"Get an artifact allowlist. — get_artifact_allowlist","text":"Get artifact allowlist certain artifact type. caller must metastore admin MANAGE ALLOWLIST privilege metastore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_artifact_allowlist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get an artifact allowlist. — get_artifact_allowlist","text":"","code":"get_artifact_allowlist(client, artifact_type)  artifactAllowlistsGet(client, artifact_type)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_artifact_allowlist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get an artifact allowlist. — get_artifact_allowlist","text":"client Required. Instance DatabricksClient() artifact_type Required. artifact type allowlist.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_automatic_cluster_update.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the automatic cluster update setting. — get_automatic_cluster_update","title":"Get the automatic cluster update setting. — get_automatic_cluster_update","text":"Gets automatic cluster update setting.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_automatic_cluster_update.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the automatic cluster update setting. — get_automatic_cluster_update","text":"","code":"get_automatic_cluster_update(client, etag = NULL)  automaticClusterUpdateGet(client, etag = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_automatic_cluster_update.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the automatic cluster update setting. — get_automatic_cluster_update","text":"client Required. Instance DatabricksClient() etag etag used versioning.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_catalog.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a catalog. — get_catalog","title":"Get a catalog. — get_catalog","text":"Gets specified catalog metastore. caller must metastore admin, owner catalog, user USE_CATALOG privilege set account.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_catalog.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a catalog. — get_catalog","text":"","code":"get_catalog(client, name, include_browse = NULL)  catalogsGet(client, name, include_browse = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_catalog.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a catalog. — get_catalog","text":"client Required. Instance DatabricksClient() name Required. name catalog. include_browse Whether include catalogs response principal can access selective metadata .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_clean_room.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a clean room. — get_clean_room","title":"Get a clean room. — get_clean_room","text":"Gets data object clean room metastore. caller must metastore admin owner clean room.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_clean_room.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a clean room. — get_clean_room","text":"","code":"get_clean_room(client, name, include_remote_details = NULL)  cleanRoomsGet(client, name, include_remote_details = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_clean_room.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a clean room. — get_clean_room","text":"client Required. Instance DatabricksClient() name Required. name clean room. include_remote_details Whether include remote details (central) clean room.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Get cluster info. — get_cluster","title":"Get cluster info. — get_cluster","text":"Retrieves information cluster given identifier. Clusters can described running, 60 days terminated.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get cluster info. — get_cluster","text":"","code":"get_cluster(client, cluster_id)  clustersGet(client, cluster_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get cluster info. — get_cluster","text":"client Required. Instance DatabricksClient() cluster_id Required. cluster retrieve information.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_permission_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get cluster permission levels. — get_cluster_permission_levels","title":"Get cluster permission levels. — get_cluster_permission_levels","text":"Gets permission levels user can object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_permission_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get cluster permission levels. — get_cluster_permission_levels","text":"","code":"get_cluster_permission_levels(client, cluster_id)  clustersGetPermissionLevels(client, cluster_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_permission_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get cluster permission levels. — get_cluster_permission_levels","text":"client Required. Instance DatabricksClient() cluster_id Required. cluster get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get cluster permissions. — get_cluster_permissions","title":"Get cluster permissions. — get_cluster_permissions","text":"Gets permissions cluster. Clusters can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get cluster permissions. — get_cluster_permissions","text":"","code":"get_cluster_permissions(client, cluster_id)  clustersGetPermissions(client, cluster_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get cluster permissions. — get_cluster_permissions","text":"client Required. Instance DatabricksClient() cluster_id Required. cluster get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_policy.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a cluster policy. — get_cluster_policy","title":"Get a cluster policy. — get_cluster_policy","text":"Get cluster policy entity. Creation editing available admins .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_policy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a cluster policy. — get_cluster_policy","text":"","code":"get_cluster_policy(client, policy_id)  clusterPoliciesGet(client, policy_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_policy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a cluster policy. — get_cluster_policy","text":"client Required. Instance DatabricksClient() policy_id Required. Canonical unique identifier cluster policy.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_policy_family.html","id":null,"dir":"Reference","previous_headings":"","what":"Get policy family information. — get_cluster_policy_family","title":"Get policy family information. — get_cluster_policy_family","text":"Retrieve information policy family based identifier.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_policy_family.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get policy family information. — get_cluster_policy_family","text":"","code":"get_cluster_policy_family(client, policy_family_id)  policyFamiliesGet(client, policy_family_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_policy_family.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get policy family information. — get_cluster_policy_family","text":"client Required. Instance DatabricksClient() policy_family_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_policy_permission_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get cluster policy permission levels. — get_cluster_policy_permission_levels","title":"Get cluster policy permission levels. — get_cluster_policy_permission_levels","text":"Gets permission levels user can object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_policy_permission_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get cluster policy permission levels. — get_cluster_policy_permission_levels","text":"","code":"get_cluster_policy_permission_levels(client, cluster_policy_id)  clusterPoliciesGetPermissionLevels(client, cluster_policy_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_policy_permission_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get cluster policy permission levels. — get_cluster_policy_permission_levels","text":"client Required. Instance DatabricksClient() cluster_policy_id Required. cluster policy get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_policy_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get cluster policy permissions. — get_cluster_policy_permissions","title":"Get cluster policy permissions. — get_cluster_policy_permissions","text":"Gets permissions cluster policy. Cluster policies can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_policy_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get cluster policy permissions. — get_cluster_policy_permissions","text":"","code":"get_cluster_policy_permissions(client, cluster_policy_id)  clusterPoliciesGetPermissions(client, cluster_policy_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_cluster_policy_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get cluster policy permissions. — get_cluster_policy_permissions","text":"client Required. Instance DatabricksClient() cluster_policy_id Required. cluster policy get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a connection. — get_connection","title":"Get a connection. — get_connection","text":"Gets connection name.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a connection. — get_connection","text":"","code":"get_connection(client, name)  connectionsGet(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a connection. — get_connection","text":"client Required. Instance DatabricksClient() name Required. Name connection.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_csp_enablement.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the compliance security profile setting. — get_csp_enablement","title":"Get the compliance security profile setting. — get_csp_enablement","text":"Gets compliance security profile setting.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_csp_enablement.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the compliance security profile setting. — get_csp_enablement","text":"","code":"get_csp_enablement(client, etag = NULL)  cspEnablementGet(client, etag = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_csp_enablement.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the compliance security profile setting. — get_csp_enablement","text":"client Required. Instance DatabricksClient() etag etag used versioning.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_dashboard.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve a definition. — get_dashboard","title":"Retrieve a definition. — get_dashboard","text":"Returns JSON representation dashboard object, including visualization query objects.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_dashboard.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve a definition. — get_dashboard","text":"","code":"get_dashboard(client, dashboard_id)  dashboardsGet(client, dashboard_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_dashboard.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve a definition. — get_dashboard","text":"client Required. Instance DatabricksClient() dashboard_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_dbfs_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the information of a file or directory. — get_dbfs_status","title":"Get the information of a file or directory. — get_dbfs_status","text":"Gets file information file directory. file directory exist, call throws exception RESOURCE_DOES_NOT_EXIST.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_dbfs_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the information of a file or directory. — get_dbfs_status","text":"","code":"get_dbfs_status(client, path)  dbfsGetStatus(client, path)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_dbfs_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the information of a file or directory. — get_dbfs_status","text":"client Required. Instance DatabricksClient() path Required. path file directory.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_dbsql_permission.html","id":null,"dir":"Reference","previous_headings":"","what":"Get object ACL. — get_dbsql_permission","title":"Get object ACL. — get_dbsql_permission","text":"Gets JSON representation access control list (ACL) specified object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_dbsql_permission.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get object ACL. — get_dbsql_permission","text":"","code":"get_dbsql_permission(client, object_type, object_id)  dbsqlPermissionsGet(client, object_type, object_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_dbsql_permission.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get object ACL. — get_dbsql_permission","text":"client Required. Instance DatabricksClient() object_type Required. type object permissions check. object_id Required. Object ID.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_default_namespace.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the default namespace setting. — get_default_namespace","title":"Get the default namespace setting. — get_default_namespace","text":"Gets default namespace setting.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_default_namespace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the default namespace setting. — get_default_namespace","text":"","code":"get_default_namespace(client, etag = NULL)  defaultNamespaceGet(client, etag = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_default_namespace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the default namespace setting. — get_default_namespace","text":"client Required. Instance DatabricksClient() etag etag used versioning.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_esm_enablement.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the enhanced security monitoring setting. — get_esm_enablement","title":"Get the enhanced security monitoring setting. — get_esm_enablement","text":"Gets enhanced security monitoring setting.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_esm_enablement.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the enhanced security monitoring setting. — get_esm_enablement","text":"","code":"get_esm_enablement(client, etag = NULL)  esmEnablementGet(client, etag = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_esm_enablement.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the enhanced security monitoring setting. — get_esm_enablement","text":"client Required. Instance DatabricksClient() etag etag used versioning.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment.html","id":null,"dir":"Reference","previous_headings":"","what":"Get an experiment. — get_experiment","title":"Get an experiment. — get_experiment","text":"Gets metadata experiment. method works deleted experiments.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get an experiment. — get_experiment","text":"","code":"get_experiment(client, experiment_id)  experimentsGetExperiment(client, experiment_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get an experiment. — get_experiment","text":"client Required. Instance DatabricksClient() experiment_id Required. ID associated experiment.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_by_name.html","id":null,"dir":"Reference","previous_headings":"","what":"Get metadata. — get_experiment_by_name","title":"Get metadata. — get_experiment_by_name","text":"Gets metadata experiment.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_by_name.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get metadata. — get_experiment_by_name","text":"","code":"get_experiment_by_name(client, experiment_name)  experimentsGetByName(client, experiment_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_by_name.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get metadata. — get_experiment_by_name","text":"client Required. Instance DatabricksClient() experiment_name Required. Name associated experiment.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_by_name.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get metadata. — get_experiment_by_name","text":"endpoint return deleted experiments, prefers active experiment active deleted experiment share name. multiple deleted experiments share name, API return one . Throws RESOURCE_DOES_NOT_EXIST experiment specified name exists.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_history.html","id":null,"dir":"Reference","previous_headings":"","what":"Get history of a given metric within a run. — get_experiment_history","title":"Get history of a given metric within a run. — get_experiment_history","text":"Gets list values specified metric given run.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_history.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get history of a given metric within a run. — get_experiment_history","text":"","code":"get_experiment_history(   client,   metric_key,   max_results = NULL,   page_token = NULL,   run_id = NULL,   run_uuid = NULL )  experimentsGetHistory(   client,   metric_key,   max_results = NULL,   page_token = NULL,   run_id = NULL,   run_uuid = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_history.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get history of a given metric within a run. — get_experiment_history","text":"client Required. Instance DatabricksClient() metric_key Required. Name metric. max_results Maximum number Metric records return per paginated request. page_token Token indicating page metric histories fetch. run_id ID run fetch metric values. run_uuid Deprecated, use run_id instead. ID run fetch metric values.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_history.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get history of a given metric within a run. — get_experiment_history","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_permission_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get experiment permission levels. — get_experiment_permission_levels","title":"Get experiment permission levels. — get_experiment_permission_levels","text":"Gets permission levels user can object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_permission_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get experiment permission levels. — get_experiment_permission_levels","text":"","code":"get_experiment_permission_levels(client, experiment_id)  experimentsGetPermissionLevels(client, experiment_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_permission_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get experiment permission levels. — get_experiment_permission_levels","text":"client Required. Instance DatabricksClient() experiment_id Required. experiment get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get experiment permissions. — get_experiment_permissions","title":"Get experiment permissions. — get_experiment_permissions","text":"Gets permissions experiment. Experiments can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get experiment permissions. — get_experiment_permissions","text":"","code":"get_experiment_permissions(client, experiment_id)  experimentsGetPermissions(client, experiment_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get experiment permissions. — get_experiment_permissions","text":"client Required. Instance DatabricksClient() experiment_id Required. experiment get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a run. — get_experiment_run","title":"Get a run. — get_experiment_run","text":"Gets metadata, metrics, params, tags run. case multiple metrics key logged run, return value latest timestamp.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a run. — get_experiment_run","text":"","code":"get_experiment_run(client, run_id, run_uuid = NULL)  experimentsGetRun(client, run_id, run_uuid = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a run. — get_experiment_run","text":"client Required. Instance DatabricksClient() run_id Required. ID run fetch. run_uuid Deprecated, use run_id instead. ID run fetch.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_experiment_run.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a run. — get_experiment_run","text":"multiple values latest timestamp, return maximum values.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_external_location.html","id":null,"dir":"Reference","previous_headings":"","what":"Get an external location. — get_external_location","title":"Get an external location. — get_external_location","text":"Gets external location metastore. caller must either metastore admin, owner external location, user privilege external location.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_external_location.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get an external location. — get_external_location","text":"","code":"get_external_location(client, name, include_browse = NULL)  externalLocationsGet(client, name, include_browse = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_external_location.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get an external location. — get_external_location","text":"client Required. Instance DatabricksClient() name Required. Name external location. include_browse Whether include external locations response principal can access selective metadata .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_file_directory_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Get directory metadata. — get_file_directory_metadata","title":"Get directory metadata. — get_file_directory_metadata","text":"Get metadata directory. response HTTP headers contain metadata. response body.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_file_directory_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get directory metadata. — get_file_directory_metadata","text":"","code":"get_file_directory_metadata(client, directory_path)  filesGetDirectoryMetadata(client, directory_path)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_file_directory_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get directory metadata. — get_file_directory_metadata","text":"client Required. Instance DatabricksClient() directory_path Required. absolute path directory.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_file_directory_metadata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get directory metadata. — get_file_directory_metadata","text":"method useful check directory exists caller access . wish ensure directory exists, can instead use PUT, create directory exist, idempotent (succeed directory already exists).","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_file_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Get file metadata. — get_file_metadata","title":"Get file metadata. — get_file_metadata","text":"Get metadata file. response HTTP headers contain metadata. response body.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_file_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get file metadata. — get_file_metadata","text":"","code":"get_file_metadata(client, file_path)  filesGetMetadata(client, file_path)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_file_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get file metadata. — get_file_metadata","text":"client Required. Instance DatabricksClient() file_path Required. absolute path file.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a function. — get_function","title":"Get a function. — get_function","text":"Gets function within parent catalog schema. fetch succeed, user must satisfy one following requirements: - metastore admin - owner function's parent catalog - USE_CATALOG privilege function's parent catalog owner function - USE_CATALOG privilege function's parent catalog, USE_SCHEMA privilege function's parent schema, EXECUTE privilege function ","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a function. — get_function","text":"","code":"get_function(client, name, include_browse = NULL)  functionsGet(client, name, include_browse = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a function. — get_function","text":"client Required. Instance DatabricksClient() name Required. fully-qualified name function (form catalog_name.schema_name.function__name). include_browse Whether include functions response principal can access selective metadata .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_git_credential.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a credential entry. — get_git_credential","title":"Get a credential entry. — get_git_credential","text":"Gets Git credential specified credential ID.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_git_credential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a credential entry. — get_git_credential","text":"","code":"get_git_credential(client, credential_id)  gitCredentialsGet(client, credential_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_git_credential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a credential entry. — get_git_credential","text":"client Required. Instance DatabricksClient() credential_id Required. ID corresponding credential access.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_global_init_script.html","id":null,"dir":"Reference","previous_headings":"","what":"Get an init script. — get_global_init_script","title":"Get an init script. — get_global_init_script","text":"Gets details script, including Base64-encoded contents.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_global_init_script.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get an init script. — get_global_init_script","text":"","code":"get_global_init_script(client, script_id)  globalInitScriptsGet(client, script_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_global_init_script.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get an init script. — get_global_init_script","text":"client Required. Instance DatabricksClient() script_id Required. ID global init script.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_grant.html","id":null,"dir":"Reference","previous_headings":"","what":"Get permissions. — get_grant","title":"Get permissions. — get_grant","text":"Gets permissions securable.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_grant.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get permissions. — get_grant","text":"","code":"get_grant(client, securable_type, full_name, principal = NULL)  grantsGet(client, securable_type, full_name, principal = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_grant.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get permissions. — get_grant","text":"client Required. Instance DatabricksClient() securable_type Required. Type securable. full_name Required. Full name securable. principal provided, permissions specified principal (user group) returned.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_grant_effective.html","id":null,"dir":"Reference","previous_headings":"","what":"Get effective permissions. — get_grant_effective","title":"Get effective permissions. — get_grant_effective","text":"Gets effective permissions securable.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_grant_effective.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get effective permissions. — get_grant_effective","text":"","code":"get_grant_effective(client, securable_type, full_name, principal = NULL)  grantsGetEffective(client, securable_type, full_name, principal = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_grant_effective.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get effective permissions. — get_grant_effective","text":"client Required. Instance DatabricksClient() securable_type Required. Type securable. full_name Required. Full name securable. principal provided, effective permissions specified principal (user group) returned.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Get group details. — get_group","title":"Get group details. — get_group","text":"Gets information specific group Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get group details. — get_group","text":"","code":"get_group(client, id)  groupsGet(client, id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get group details. — get_group","text":"client Required. Instance DatabricksClient() id Required. Unique ID group Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_instance_pool.html","id":null,"dir":"Reference","previous_headings":"","what":"Get instance pool information. — get_instance_pool","title":"Get instance pool information. — get_instance_pool","text":"Retrieve information instance pool based identifier.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_instance_pool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get instance pool information. — get_instance_pool","text":"","code":"get_instance_pool(client, instance_pool_id)  instancePoolsGet(client, instance_pool_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_instance_pool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get instance pool information. — get_instance_pool","text":"client Required. Instance DatabricksClient() instance_pool_id Required. canonical unique identifier instance pool.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_instance_pool_permission_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get instance pool permission levels. — get_instance_pool_permission_levels","title":"Get instance pool permission levels. — get_instance_pool_permission_levels","text":"Gets permission levels user can object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_instance_pool_permission_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get instance pool permission levels. — get_instance_pool_permission_levels","text":"","code":"get_instance_pool_permission_levels(client, instance_pool_id)  instancePoolsGetPermissionLevels(client, instance_pool_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_instance_pool_permission_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get instance pool permission levels. — get_instance_pool_permission_levels","text":"client Required. Instance DatabricksClient() instance_pool_id Required. instance pool get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_instance_pool_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get instance pool permissions. — get_instance_pool_permissions","title":"Get instance pool permissions. — get_instance_pool_permissions","text":"Gets permissions instance pool. Instance pools can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_instance_pool_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get instance pool permissions. — get_instance_pool_permissions","text":"","code":"get_instance_pool_permissions(client, instance_pool_id)  instancePoolsGetPermissions(client, instance_pool_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_instance_pool_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get instance pool permissions. — get_instance_pool_permissions","text":"client Required. Instance DatabricksClient() instance_pool_id Required. instance pool get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_ip_access_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Get access list. — get_ip_access_list","title":"Get access list. — get_ip_access_list","text":"Gets IP access list, specified list ID.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_ip_access_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get access list. — get_ip_access_list","text":"","code":"get_ip_access_list(client, ip_access_list_id)  ipAccessListsGet(client, ip_access_list_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_ip_access_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get access list. — get_ip_access_list","text":"client Required. Instance DatabricksClient() ip_access_list_id Required. ID corresponding IP access list.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a single job. — get_job","title":"Get a single job. — get_job","text":"Retrieves details single job.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a single job. — get_job","text":"","code":"get_job(client, job_id)  jobsGet(client, job_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a single job. — get_job","text":"client Required. Instance DatabricksClient() job_id Required. canonical identifier job retrieve information .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_permission_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get job permission levels. — get_job_permission_levels","title":"Get job permission levels. — get_job_permission_levels","text":"Gets permission levels user can object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_permission_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get job permission levels. — get_job_permission_levels","text":"","code":"get_job_permission_levels(client, job_id)  jobsGetPermissionLevels(client, job_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_permission_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get job permission levels. — get_job_permission_levels","text":"client Required. Instance DatabricksClient() job_id Required. job get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get job permissions. — get_job_permissions","title":"Get job permissions. — get_job_permissions","text":"Gets permissions job. Jobs can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get job permissions. — get_job_permissions","text":"","code":"get_job_permissions(client, job_id)  jobsGetPermissions(client, job_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get job permissions. — get_job_permissions","text":"client Required. Instance DatabricksClient() job_id Required. job get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a single job run. — get_job_run","title":"Get a single job run. — get_job_run","text":"Retrieve metadata run.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a single job run. — get_job_run","text":"","code":"get_job_run(   client,   run_id,   include_history = NULL,   include_resolved_values = NULL )  jobsGetRun(   client,   run_id,   include_history = NULL,   include_resolved_values = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a single job run. — get_job_run","text":"client Required. Instance DatabricksClient() run_id Required. canonical identifier run retrieve metadata. include_history Whether include repair history response. include_resolved_values Whether include resolved parameter values response.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_run_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a single job run. — get_job_run_and_wait","title":"Get a single job run. — get_job_run_and_wait","text":"long-running operation, blocks Jobs Databricks reach TERMINATED SKIPPED state timeout 20 minutes, can change via timeout parameter. default, state Databricks Jobs reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_run_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a single job run. — get_job_run_and_wait","text":"","code":"get_job_run_and_wait(   client,   run_id,   include_history = NULL,   include_resolved_values = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_run_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a single job run. — get_job_run_and_wait","text":"client Required. Instance DatabricksClient() run_id Required. canonical identifier run retrieve metadata. include_history Whether include repair history response. include_resolved_values Whether include resolved parameter values response. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_run_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a single job run. — get_job_run_and_wait","text":"Retrieve metadata run.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_run_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the output for a single run. — get_job_run_output","title":"Get the output for a single run. — get_job_run_output","text":"Retrieve output metadata single task run. notebook task returns value dbutils.notebook.exit() call, can use endpoint retrieve value. Databricks restricts API returning first 5 MB output. return larger result, can store job results cloud storage service.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_run_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the output for a single run. — get_job_run_output","text":"","code":"get_job_run_output(client, run_id)  jobsGetRunOutput(client, run_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_run_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the output for a single run. — get_job_run_output","text":"client Required. Instance DatabricksClient() run_id Required. canonical identifier run.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_job_run_output.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the output for a single run. — get_job_run_output","text":"endpoint validates run_id parameter valid returns HTTP status code 400 run_id parameter invalid. Runs automatically removed 60 days. want reference beyond 60 days, must save old run results expire.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_lakehouse_monitor.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a table monitor. — get_lakehouse_monitor","title":"Get a table monitor. — get_lakehouse_monitor","text":"Gets monitor specified table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_lakehouse_monitor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a table monitor. — get_lakehouse_monitor","text":"","code":"get_lakehouse_monitor(client, full_name)  lakehouseMonitorsGet(client, full_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_lakehouse_monitor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a table monitor. — get_lakehouse_monitor","text":"client Required. Instance DatabricksClient() full_name Required. Full name table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_lakehouse_monitor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a table monitor. — get_lakehouse_monitor","text":"caller must either: 1. owner table's parent catalog 2. USE_CATALOG table's parent catalog owner table's parent schema. 3. following permissions: - USE_CATALOG table's parent catalog - USE_SCHEMA table's parent schema - SELECT privilege table. returned information includes configuration values, well information assets created monitor. information (e.g., dashboard) may filtered caller different workspace monitor created.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_lakehouse_monitor_refresh.html","id":null,"dir":"Reference","previous_headings":"","what":"Get refresh. — get_lakehouse_monitor_refresh","title":"Get refresh. — get_lakehouse_monitor_refresh","text":"Gets info specific monitor refresh using given refresh ID.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_lakehouse_monitor_refresh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get refresh. — get_lakehouse_monitor_refresh","text":"","code":"get_lakehouse_monitor_refresh(client, full_name, refresh_id)  lakehouseMonitorsGetRefresh(client, full_name, refresh_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_lakehouse_monitor_refresh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get refresh. — get_lakehouse_monitor_refresh","text":"client Required. Instance DatabricksClient() full_name Required. Full name table. refresh_id Required. ID refresh.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_lakehouse_monitor_refresh.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get refresh. — get_lakehouse_monitor_refresh","text":"caller must either: 1. owner table's parent catalog 2. USE_CATALOG table's parent catalog owner table's parent schema 3. following permissions: - USE_CATALOG table's parent catalog - USE_SCHEMA table's parent schema - SELECT privilege table. Additionally, call must made workspace monitor created.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_lakeview.html","id":null,"dir":"Reference","previous_headings":"","what":"Get dashboard. — get_lakeview","title":"Get dashboard. — get_lakeview","text":"Get draft dashboard.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_lakeview.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get dashboard. — get_lakeview","text":"","code":"get_lakeview(client, dashboard_id)  lakeviewGet(client, dashboard_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_lakeview.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get dashboard. — get_lakeview","text":"client Required. Instance DatabricksClient() dashboard_id Required. UUID identifying dashboard.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_lakeview_published.html","id":null,"dir":"Reference","previous_headings":"","what":"Get published dashboard. — get_lakeview_published","title":"Get published dashboard. — get_lakeview_published","text":"Get current published dashboard.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_lakeview_published.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get published dashboard. — get_lakeview_published","text":"","code":"get_lakeview_published(client, dashboard_id)  lakeviewGetPublished(client, dashboard_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_lakeview_published.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get published dashboard. — get_lakeview_published","text":"client Required. Instance DatabricksClient() dashboard_id Required. UUID identifying dashboard published.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_metastore.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a metastore. — get_metastore","title":"Get a metastore. — get_metastore","text":"Gets metastore matches supplied ID. caller must metastore admin retrieve info.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_metastore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a metastore. — get_metastore","text":"","code":"get_metastore(client, id)  metastoresGet(client, id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_metastore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a metastore. — get_metastore","text":"client Required. Instance DatabricksClient() id Required. Unique ID metastore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Get model. — get_model","title":"Get model. — get_model","text":"Get details model. Databricks workspace version MLflow endpoint also returns model's Databricks workspace ID permission level requesting user model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get model. — get_model","text":"","code":"get_model(client, name)  modelRegistryGetModel(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get model. — get_model","text":"client Required. Instance DatabricksClient() name Required. Registered model unique name identifier.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get model. — get_model","text":"MLflow endpoint: https://www.mlflow.org/docs/latest/rest-api.html#get-registeredmodel","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_latest_versions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the latest version. — get_model_latest_versions","title":"Get the latest version. — get_model_latest_versions","text":"Gets latest version registered model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_latest_versions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the latest version. — get_model_latest_versions","text":"","code":"get_model_latest_versions(client, name, stages = NULL)  modelRegistryGetLatestVersions(client, name, stages = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_latest_versions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the latest version. — get_model_latest_versions","text":"client Required. Instance DatabricksClient() name Required. Registered model unique name identifier. stages List stages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_latest_versions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the latest version. — get_model_latest_versions","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_permission_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get registered model permission levels. — get_model_permission_levels","title":"Get registered model permission levels. — get_model_permission_levels","text":"Gets permission levels user can object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_permission_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get registered model permission levels. — get_model_permission_levels","text":"","code":"get_model_permission_levels(client, registered_model_id)  modelRegistryGetPermissionLevels(client, registered_model_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_permission_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get registered model permission levels. — get_model_permission_levels","text":"client Required. Instance DatabricksClient() registered_model_id Required. registered model get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get registered model permissions. — get_model_permissions","title":"Get registered model permissions. — get_model_permissions","text":"Gets permissions registered model. Registered models can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get registered model permissions. — get_model_permissions","text":"","code":"get_model_permissions(client, registered_model_id)  modelRegistryGetPermissions(client, registered_model_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get registered model permissions. — get_model_permissions","text":"client Required. Instance DatabricksClient() registered_model_id Required. registered model get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_version.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a model version. — get_model_version","title":"Get a model version. — get_model_version","text":"Get model version. Get model version.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_version.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a model version. — get_model_version","text":"","code":"get_model_version(client, full_name, version, include_browse = NULL)  modelRegistryGetModelVersion(client, name, version)  get_model_version(client, full_name, version, include_browse = NULL)  modelVersionsGet(client, full_name, version, include_browse = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_version.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a model version. — get_model_version","text":"client Required. Instance DatabricksClient() full_name Required. three-level (fully qualified) name model version. version Required. integer version number model version. include_browse Whether include model versions response principal can access selective metadata . name Required. Name registered model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_version.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a model version. — get_model_version","text":"caller must metastore admin owner (EXECUTE privilege ) parent registered model. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_version_by_alias.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Model Version By Alias. — get_model_version_by_alias","title":"Get Model Version By Alias. — get_model_version_by_alias","text":"Get model version alias.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_version_by_alias.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Model Version By Alias. — get_model_version_by_alias","text":"","code":"get_model_version_by_alias(client, full_name, alias)  modelVersionsGetByAlias(client, full_name, alias)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_version_by_alias.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Model Version By Alias. — get_model_version_by_alias","text":"client Required. Instance DatabricksClient() full_name Required. three-level (fully qualified) name registered model. alias Required. name alias.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_version_by_alias.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Model Version By Alias. — get_model_version_by_alias","text":"caller must metastore admin owner (EXECUTE privilege ) registered model. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_version_download_uri.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a model version URI. — get_model_version_download_uri","title":"Get a model version URI. — get_model_version_download_uri","text":"Gets URI download model version.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_version_download_uri.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a model version URI. — get_model_version_download_uri","text":"","code":"get_model_version_download_uri(client, name, version)  modelRegistryGetModelVersionDownloadUri(client, name, version)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_model_version_download_uri.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a model version URI. — get_model_version_download_uri","text":"client Required. Instance DatabricksClient() name Required. Name registered model. version Required. Model version number.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_notebook_permission_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get workspace object permission levels. — get_notebook_permission_levels","title":"Get workspace object permission levels. — get_notebook_permission_levels","text":"Gets permission levels user can object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_notebook_permission_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get workspace object permission levels. — get_notebook_permission_levels","text":"","code":"get_notebook_permission_levels(   client,   workspace_object_type,   workspace_object_id )  workspaceGetPermissionLevels(   client,   workspace_object_type,   workspace_object_id )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_notebook_permission_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get workspace object permission levels. — get_notebook_permission_levels","text":"client Required. Instance DatabricksClient() workspace_object_type Required. workspace object type get manage permissions. workspace_object_id Required. workspace object get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_notebook_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get workspace object permissions. — get_notebook_permissions","title":"Get workspace object permissions. — get_notebook_permissions","text":"Gets permissions workspace object. Workspace objects can inherit permissions parent objects root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_notebook_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get workspace object permissions. — get_notebook_permissions","text":"","code":"get_notebook_permissions(client, workspace_object_type, workspace_object_id)  workspaceGetPermissions(client, workspace_object_type, workspace_object_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_notebook_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get workspace object permissions. — get_notebook_permissions","text":"client Required. Instance DatabricksClient() workspace_object_type Required. workspace object type get manage permissions. workspace_object_id Required. workspace object get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_notebook_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Get status. — get_notebook_status","title":"Get status. — get_notebook_status","text":"Gets status object directory. path exist, call returns error RESOURCE_DOES_NOT_EXIST.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_notebook_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get status. — get_notebook_status","text":"","code":"get_notebook_status(client, path)  workspaceGetStatus(client, path)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_notebook_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get status. — get_notebook_status","text":"client Required. Instance DatabricksClient() path Required. absolute path notebook directory.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_online_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Get an Online Table. — get_online_table","title":"Get an Online Table. — get_online_table","text":"Get information existing online table status.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_online_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get an Online Table. — get_online_table","text":"","code":"get_online_table(client, name)  onlineTablesGet(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_online_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get an Online Table. — get_online_table","text":"client Required. Instance DatabricksClient() name Required. Full three-part (catalog, schema, table) name table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_permission.html","id":null,"dir":"Reference","previous_headings":"","what":"Get object permissions. — get_permission","title":"Get object permissions. — get_permission","text":"Gets permissions object. Objects can inherit permissions parent objects root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_permission.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get object permissions. — get_permission","text":"","code":"get_permission(client, request_object_type, request_object_id)  permissionsGet(client, request_object_type, request_object_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_permission.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get object permissions. — get_permission","text":"client Required. Instance DatabricksClient() request_object_type Required. type request object. request_object_id Required. id request object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_permission_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get object permission levels. — get_permission_levels","title":"Get object permission levels. — get_permission_levels","text":"Gets permission levels user can object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_permission_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get object permission levels. — get_permission_levels","text":"","code":"get_permission_levels(client, request_object_type, request_object_id)  permissionsGetPermissionLevels(client, request_object_type, request_object_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_permission_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get object permission levels. — get_permission_levels","text":"client Required. Instance DatabricksClient() request_object_type Required. needs content. request_object_id Required. needs content.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a pipeline. — get_pipeline","title":"Get a pipeline. — get_pipeline","text":"Get pipeline.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a pipeline. — get_pipeline","text":"","code":"get_pipeline(client, pipeline_id)  pipelinesGet(client, pipeline_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a pipeline. — get_pipeline","text":"client Required. Instance DatabricksClient() pipeline_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_pipeline_permission_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get pipeline permission levels. — get_pipeline_permission_levels","title":"Get pipeline permission levels. — get_pipeline_permission_levels","text":"Gets permission levels user can object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_pipeline_permission_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get pipeline permission levels. — get_pipeline_permission_levels","text":"","code":"get_pipeline_permission_levels(client, pipeline_id)  pipelinesGetPermissionLevels(client, pipeline_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_pipeline_permission_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get pipeline permission levels. — get_pipeline_permission_levels","text":"client Required. Instance DatabricksClient() pipeline_id Required. pipeline get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_pipeline_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get pipeline permissions. — get_pipeline_permissions","title":"Get pipeline permissions. — get_pipeline_permissions","text":"Gets permissions pipeline. Pipelines can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_pipeline_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get pipeline permissions. — get_pipeline_permissions","text":"","code":"get_pipeline_permissions(client, pipeline_id)  pipelinesGetPermissions(client, pipeline_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_pipeline_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get pipeline permissions. — get_pipeline_permissions","text":"client Required. Instance DatabricksClient() pipeline_id Required. pipeline get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_pipeline_update.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a pipeline update. — get_pipeline_update","title":"Get a pipeline update. — get_pipeline_update","text":"Gets update active pipeline.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_pipeline_update.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a pipeline update. — get_pipeline_update","text":"","code":"get_pipeline_update(client, pipeline_id, update_id)  pipelinesGetUpdate(client, pipeline_id, update_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_pipeline_update.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a pipeline update. — get_pipeline_update","text":"client Required. Instance DatabricksClient() pipeline_id Required. ID pipeline. update_id Required. ID update.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_provider.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a provider. — get_provider","title":"Get a provider. — get_provider","text":"Gets specific authentication provider. caller must supply name provider, must either metastore admin owner provider.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_provider.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a provider. — get_provider","text":"","code":"get_provider(client, name)  providersGet(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_provider.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a provider. — get_provider","text":"client Required. Instance DatabricksClient() name Required. Name provider.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a query definition. — get_query","title":"Get a query definition. — get_query","text":"Retrieve query object definition along contextual permissions information currently authenticated user.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a query definition. — get_query","text":"","code":"get_query(client, query_id)  queriesGet(client, query_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a query definition. — get_query","text":"client Required. Instance DatabricksClient() query_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_recipient.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a share recipient. — get_recipient","title":"Get a share recipient. — get_recipient","text":"Gets share recipient metastore :","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_recipient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a share recipient. — get_recipient","text":"","code":"get_recipient(client, name)  recipientsGet(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_recipient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a share recipient. — get_recipient","text":"client Required. Instance DatabricksClient() name Required. Name recipient.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_recipient.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a share recipient. — get_recipient","text":"caller owner share recipient, : * metastore admin","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_recipient_activation_url_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a share activation URL. — get_recipient_activation_url_info","title":"Get a share activation URL. — get_recipient_activation_url_info","text":"Gets activation URL share.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_recipient_activation_url_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a share activation URL. — get_recipient_activation_url_info","text":"","code":"get_recipient_activation_url_info(client, activation_url)  recipientActivationGetActivationUrlInfo(client, activation_url)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_recipient_activation_url_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a share activation URL. — get_recipient_activation_url_info","text":"client Required. Instance DatabricksClient() activation_url Required. one time activation url.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_registered_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a Registered Model. — get_registered_model","title":"Get a Registered Model. — get_registered_model","text":"Get registered model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_registered_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a Registered Model. — get_registered_model","text":"","code":"get_registered_model(client, full_name, include_browse = NULL)  registeredModelsGet(client, full_name, include_browse = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_registered_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a Registered Model. — get_registered_model","text":"client Required. Instance DatabricksClient() full_name Required. three-level (fully qualified) name registered model. include_browse Whether include registered models response principal can access selective metadata .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_registered_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a Registered Model. — get_registered_model","text":"caller must metastore admin owner (EXECUTE privilege ) registered model. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_repo.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a repo. — get_repo","title":"Get a repo. — get_repo","text":"Returns repo given repo ID.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_repo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a repo. — get_repo","text":"","code":"get_repo(client, repo_id)  reposGet(client, repo_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_repo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a repo. — get_repo","text":"client Required. Instance DatabricksClient() repo_id Required. ID corresponding repo access.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_repo_permission_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get repo permission levels. — get_repo_permission_levels","title":"Get repo permission levels. — get_repo_permission_levels","text":"Gets permission levels user can object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_repo_permission_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get repo permission levels. — get_repo_permission_levels","text":"","code":"get_repo_permission_levels(client, repo_id)  reposGetPermissionLevels(client, repo_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_repo_permission_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get repo permission levels. — get_repo_permission_levels","text":"client Required. Instance DatabricksClient() repo_id Required. repo get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_repo_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get repo permissions. — get_repo_permissions","title":"Get repo permissions. — get_repo_permissions","text":"Gets permissions repo. Repos can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_repo_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get repo permissions. — get_repo_permissions","text":"","code":"get_repo_permissions(client, repo_id)  reposGetPermissions(client, repo_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_repo_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get repo permissions. — get_repo_permissions","text":"client Required. Instance DatabricksClient() repo_id Required. repo get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_restrict_workspace_admin.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the restrict workspace admins setting. — get_restrict_workspace_admin","title":"Get the restrict workspace admins setting. — get_restrict_workspace_admin","text":"Gets restrict workspace admins setting.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_restrict_workspace_admin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the restrict workspace admins setting. — get_restrict_workspace_admin","text":"","code":"get_restrict_workspace_admin(client, etag = NULL)  restrictWorkspaceAdminsGet(client, etag = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_restrict_workspace_admin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the restrict workspace admins setting. — get_restrict_workspace_admin","text":"client Required. Instance DatabricksClient() etag etag used versioning.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a schema. — get_schema","title":"Get a schema. — get_schema","text":"Gets specified schema within metastore. caller must metastore admin, owner schema, user USE_SCHEMA privilege schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a schema. — get_schema","text":"","code":"get_schema(client, full_name, include_browse = NULL)  schemasGet(client, full_name, include_browse = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a schema. — get_schema","text":"client Required. Instance DatabricksClient() full_name Required. Full name schema. include_browse Whether include schemas response principal can access selective metadata .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_secret.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a secret. — get_secret","title":"Get a secret. — get_secret","text":"Gets bytes representation secret value specified scope key.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_secret.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a secret. — get_secret","text":"","code":"get_secret(client, scope, key)  secretsGetSecret(client, scope, key)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_secret.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a secret. — get_secret","text":"client Required. Instance DatabricksClient() scope Required. name scope fetch secret information . key Required. key fetch secret .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_secret.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a secret. — get_secret","text":"Users need READ permission make call. Note secret value returned bytes. interpretation bytes determined caller DBUtils type data decoded . Throws PERMISSION_DENIED user permission make API call. Throws RESOURCE_DOES_NOT_EXIST secret secret scope exists.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_secret_acl.html","id":null,"dir":"Reference","previous_headings":"","what":"Get secret ACL details. — get_secret_acl","title":"Get secret ACL details. — get_secret_acl","text":"Gets details given ACL, group permission. Users must MANAGE permission invoke API.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_secret_acl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get secret ACL details. — get_secret_acl","text":"","code":"get_secret_acl(client, scope, principal)  secretsGetAcl(client, scope, principal)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_secret_acl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get secret ACL details. — get_secret_acl","text":"client Required. Instance DatabricksClient() scope Required. name scope fetch ACL information . principal Required. principal fetch ACL information .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_secret_acl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get secret ACL details. — get_secret_acl","text":"Throws RESOURCE_DOES_NOT_EXIST secret scope exists. Throws PERMISSION_DENIED user permission make API call.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_service_principal.html","id":null,"dir":"Reference","previous_headings":"","what":"Get service principal details. — get_service_principal","title":"Get service principal details. — get_service_principal","text":"Gets details single service principal define Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_service_principal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get service principal details. — get_service_principal","text":"","code":"get_service_principal(client, id)  servicePrincipalsGet(client, id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_service_principal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get service principal details. — get_service_principal","text":"client Required. Instance DatabricksClient() id Required. Unique ID service principal Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_serving_endpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a single serving endpoint. — get_serving_endpoint","title":"Get a single serving endpoint. — get_serving_endpoint","text":"Retrieves details single serving endpoint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_serving_endpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a single serving endpoint. — get_serving_endpoint","text":"","code":"get_serving_endpoint(client, name)  servingEndpointsGet(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_serving_endpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a single serving endpoint. — get_serving_endpoint","text":"client Required. Instance DatabricksClient() name Required. name serving endpoint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_serving_endpoint_permission_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get serving endpoint permission levels. — get_serving_endpoint_permission_levels","title":"Get serving endpoint permission levels. — get_serving_endpoint_permission_levels","text":"Gets permission levels user can object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_serving_endpoint_permission_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get serving endpoint permission levels. — get_serving_endpoint_permission_levels","text":"","code":"get_serving_endpoint_permission_levels(client, serving_endpoint_id)  servingEndpointsGetPermissionLevels(client, serving_endpoint_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_serving_endpoint_permission_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get serving endpoint permission levels. — get_serving_endpoint_permission_levels","text":"client Required. Instance DatabricksClient() serving_endpoint_id Required. serving endpoint get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_serving_endpoint_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get serving endpoint permissions. — get_serving_endpoint_permissions","title":"Get serving endpoint permissions. — get_serving_endpoint_permissions","text":"Gets permissions serving endpoint. Serving endpoints can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_serving_endpoint_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get serving endpoint permissions. — get_serving_endpoint_permissions","text":"","code":"get_serving_endpoint_permissions(client, serving_endpoint_id)  servingEndpointsGetPermissions(client, serving_endpoint_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_serving_endpoint_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get serving endpoint permissions. — get_serving_endpoint_permissions","text":"client Required. Instance DatabricksClient() serving_endpoint_id Required. serving endpoint get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_share.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a share. — get_share","title":"Get a share. — get_share","text":"Gets data object share metastore. caller must metastore admin owner share.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_share.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a share. — get_share","text":"","code":"get_share(client, name, include_shared_data = NULL)  sharesGet(client, name, include_shared_data = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_share.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a share. — get_share","text":"client Required. Instance DatabricksClient() name Required. name share. include_shared_data Query data include share.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_statement_execution.html","id":null,"dir":"Reference","previous_headings":"","what":"Get status, manifest, and result first chunk. — get_statement_execution","title":"Get status, manifest, and result first chunk. — get_statement_execution","text":"request can used poll statement's status. status.state field SUCCEEDED also return result manifest first chunk result data. statement terminal states CANCELED, CLOSED FAILED, returns HTTP 200 state set. least 12 hours terminal state, statement removed warehouse calls receive HTTP 404 response.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_statement_execution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get status, manifest, and result first chunk. — get_statement_execution","text":"","code":"get_statement_execution(client, statement_id)  statementExecutionGetStatement(client, statement_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_statement_execution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get status, manifest, and result first chunk. — get_statement_execution","text":"client Required. Instance DatabricksClient() statement_id Required. statement ID returned upon successfully submitting SQL statement, required reference subsequent calls.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_statement_execution.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get status, manifest, and result first chunk. — get_statement_execution","text":"NOTE call currently might take 5 seconds get latest status result.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_statement_execution_result_chunk_n.html","id":null,"dir":"Reference","previous_headings":"","what":"Get result chunk by index. — get_statement_execution_result_chunk_n","title":"Get result chunk by index. — get_statement_execution_result_chunk_n","text":"statement execution SUCCEEDED, request can used fetch chunk index. Whereas first chunk chunk_index=0 typically fetched :method:statementexecution/executeStatement :method:statementexecution/getStatement, request can used fetch subsequent chunks. response structure identical nested result element described :method:statementexecution/getStatement request, similarly includes next_chunk_index next_chunk_internal_link fields simple iteration result set.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_statement_execution_result_chunk_n.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get result chunk by index. — get_statement_execution_result_chunk_n","text":"","code":"get_statement_execution_result_chunk_n(client, statement_id, chunk_index)  statementExecutionGetStatementResultChunkN(client, statement_id, chunk_index)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_statement_execution_result_chunk_n.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get result chunk by index. — get_statement_execution_result_chunk_n","text":"client Required. Instance DatabricksClient() statement_id Required. statement ID returned upon successfully submitting SQL statement, required reference subsequent calls. chunk_index Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_storage_credential.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a credential. — get_storage_credential","title":"Get a credential. — get_storage_credential","text":"Gets storage credential metastore. caller must metastore admin, owner storage credential, permission storage credential.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_storage_credential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a credential. — get_storage_credential","text":"","code":"get_storage_credential(client, name)  storageCredentialsGet(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_storage_credential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a credential. — get_storage_credential","text":"client Required. Instance DatabricksClient() name Required. Name storage credential.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a table. — get_table","title":"Get a table. — get_table","text":"Gets table metastore specific catalog schema. caller must satisfy one following requirements: * metastore admin * owner parent catalog * owner parent schema USE_CATALOG privilege parent catalog * USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema, either table owner SELECT privilege table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a table. — get_table","text":"","code":"get_table(   client,   full_name,   include_browse = NULL,   include_delta_metadata = NULL )  tablesGet(   client,   full_name,   include_browse = NULL,   include_delta_metadata = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a table. — get_table","text":"client Required. Instance DatabricksClient() full_name Required. Full name table. include_browse Whether include tables response principal can access selective metadata . include_delta_metadata Whether delta metadata included response.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_token_management.html","id":null,"dir":"Reference","previous_headings":"","what":"Get token info. — get_token_management","title":"Get token info. — get_token_management","text":"Gets information token, specified ID.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_token_management.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get token info. — get_token_management","text":"","code":"get_token_management(client, token_id)  tokenManagementGet(client, token_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_token_management.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get token info. — get_token_management","text":"client Required. Instance DatabricksClient() token_id Required. ID token get.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_token_management_permission_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get token permission levels. — get_token_management_permission_levels","title":"Get token permission levels. — get_token_management_permission_levels","text":"Gets permission levels user can object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_token_management_permission_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get token permission levels. — get_token_management_permission_levels","text":"","code":"get_token_management_permission_levels(client)  tokenManagementGetPermissionLevels(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_token_management_permission_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get token permission levels. — get_token_management_permission_levels","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_token_management_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get token permissions. — get_token_management_permissions","title":"Get token permissions. — get_token_management_permissions","text":"Gets permissions tokens. Tokens can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_token_management_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get token permissions. — get_token_management_permissions","text":"","code":"get_token_management_permissions(client)  tokenManagementGetPermissions(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_token_management_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get token permissions. — get_token_management_permissions","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_user.html","id":null,"dir":"Reference","previous_headings":"","what":"Get user details. — get_user","title":"Get user details. — get_user","text":"Gets information specific user Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_user.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get user details. — get_user","text":"","code":"get_user(   client,   id,   attributes = NULL,   count = NULL,   excluded_attributes = NULL,   filter = NULL,   sort_by = NULL,   sort_order = NULL,   start_index = NULL )  usersGet(   client,   id,   attributes = NULL,   count = NULL,   excluded_attributes = NULL,   filter = NULL,   sort_by = NULL,   sort_order = NULL,   start_index = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_user.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get user details. — get_user","text":"client Required. Instance DatabricksClient() id Required. Unique ID user Databricks workspace. attributes Comma-separated list attributes return response. count Desired number results per page. excluded_attributes Comma-separated list attributes exclude response. filter Query results filtered. sort_by Attribute sort results. sort_order order sort results. start_index Specifies index first result.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_user_permission_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get password permission levels. — get_user_permission_levels","title":"Get password permission levels. — get_user_permission_levels","text":"Gets permission levels user can object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_user_permission_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get password permission levels. — get_user_permission_levels","text":"","code":"get_user_permission_levels(client)  usersGetPermissionLevels(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_user_permission_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get password permission levels. — get_user_permission_levels","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_user_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get password permissions. — get_user_permissions","title":"Get password permissions. — get_user_permissions","text":"Gets permissions passwords. Passwords can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_user_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get password permissions. — get_user_permissions","text":"","code":"get_user_permissions(client)  usersGetPermissions(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_user_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get password permissions. — get_user_permissions","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_vector_search_endpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Get an endpoint. — get_vector_search_endpoint","title":"Get an endpoint. — get_vector_search_endpoint","text":"Get endpoint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_vector_search_endpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get an endpoint. — get_vector_search_endpoint","text":"","code":"get_vector_search_endpoint(client, endpoint_name)  vectorSearchEndpointsGetEndpoint(client, endpoint_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_vector_search_endpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get an endpoint. — get_vector_search_endpoint","text":"client Required. Instance DatabricksClient() endpoint_name Required. Name endpoint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_vector_search_index.html","id":null,"dir":"Reference","previous_headings":"","what":"Get an index. — get_vector_search_index","title":"Get an index. — get_vector_search_index","text":"Get index.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_vector_search_index.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get an index. — get_vector_search_index","text":"","code":"get_vector_search_index(client, index_name)  vectorSearchIndexesGetIndex(client, index_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_vector_search_index.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get an index. — get_vector_search_index","text":"client Required. Instance DatabricksClient() index_name Required. Name index.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_warehouse.html","id":null,"dir":"Reference","previous_headings":"","what":"Get warehouse info. — get_warehouse","title":"Get warehouse info. — get_warehouse","text":"Gets information single SQL warehouse.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_warehouse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get warehouse info. — get_warehouse","text":"","code":"get_warehouse(client, id)  warehousesGet(client, id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_warehouse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get warehouse info. — get_warehouse","text":"client Required. Instance DatabricksClient() id Required. Required.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_warehouse_permission_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Get SQL warehouse permission levels. — get_warehouse_permission_levels","title":"Get SQL warehouse permission levels. — get_warehouse_permission_levels","text":"Gets permission levels user can object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_warehouse_permission_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get SQL warehouse permission levels. — get_warehouse_permission_levels","text":"","code":"get_warehouse_permission_levels(client, warehouse_id)  warehousesGetPermissionLevels(client, warehouse_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_warehouse_permission_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get SQL warehouse permission levels. — get_warehouse_permission_levels","text":"client Required. Instance DatabricksClient() warehouse_id Required. SQL warehouse get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_warehouse_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get SQL warehouse permissions. — get_warehouse_permissions","title":"Get SQL warehouse permissions. — get_warehouse_permissions","text":"Gets permissions SQL warehouse. SQL warehouses can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_warehouse_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get SQL warehouse permissions. — get_warehouse_permissions","text":"","code":"get_warehouse_permissions(client, warehouse_id)  warehousesGetPermissions(client, warehouse_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_warehouse_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get SQL warehouse permissions. — get_warehouse_permissions","text":"client Required. Instance DatabricksClient() warehouse_id Required. SQL warehouse get manage permissions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_warehouse_workspace_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the workspace configuration. — get_warehouse_workspace_config","title":"Get the workspace configuration. — get_warehouse_workspace_config","text":"Gets workspace level configuration shared SQL warehouses workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_warehouse_workspace_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the workspace configuration. — get_warehouse_workspace_config","text":"","code":"get_warehouse_workspace_config(client)  warehousesGetWorkspaceWarehouseConfig(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_warehouse_workspace_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the workspace configuration. — get_warehouse_workspace_config","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_workspace_binding.html","id":null,"dir":"Reference","previous_headings":"","what":"Get catalog workspace bindings. — get_workspace_binding","title":"Get catalog workspace bindings. — get_workspace_binding","text":"Gets workspace bindings catalog. caller must metastore admin owner catalog.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_workspace_binding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get catalog workspace bindings. — get_workspace_binding","text":"","code":"get_workspace_binding(client, name)  workspaceBindingsGet(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_workspace_binding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get catalog workspace bindings. — get_workspace_binding","text":"client Required. Instance DatabricksClient() name Required. name catalog.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_workspace_binding_bindings.html","id":null,"dir":"Reference","previous_headings":"","what":"Get securable workspace bindings. — get_workspace_binding_bindings","title":"Get securable workspace bindings. — get_workspace_binding_bindings","text":"Gets workspace bindings securable. caller must metastore admin owner securable.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_workspace_binding_bindings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get securable workspace bindings. — get_workspace_binding_bindings","text":"","code":"get_workspace_binding_bindings(client, securable_type, securable_name)  workspaceBindingsGetBindings(client, securable_type, securable_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_workspace_binding_bindings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get securable workspace bindings. — get_workspace_binding_bindings","text":"client Required. Instance DatabricksClient() securable_type Required. type securable. securable_name Required. name securable.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_workspace_conf_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Check configuration status. — get_workspace_conf_status","title":"Check configuration status. — get_workspace_conf_status","text":"Gets configuration status workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_workspace_conf_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check configuration status. — get_workspace_conf_status","text":"","code":"get_workspace_conf_status(client, keys)  workspaceConfGetStatus(client, keys)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/get_workspace_conf_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check configuration status. — get_workspace_conf_status","text":"client Required. Instance DatabricksClient() keys Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/import_notebook.html","id":null,"dir":"Reference","previous_headings":"","what":"Import a workspace object. — import_notebook","title":"Import a workspace object. — import_notebook","text":"Imports workspace object (example, notebook file) contents entire directory. path already exists overwrite set false, call returns error RESOURCE_ALREADY_EXISTS. import directory, can use either DBC format SOURCE format language field unset. import single file SOURCE, must set language field.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/import_notebook.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Import a workspace object. — import_notebook","text":"","code":"import_notebook(   client,   path,   content = NULL,   format = NULL,   language = NULL,   overwrite = NULL )  workspaceImport(   client,   path,   content = NULL,   format = NULL,   language = NULL,   overwrite = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/import_notebook.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Import a workspace object. — import_notebook","text":"client Required. Instance DatabricksClient() path Required. absolute path object directory. content base64-encoded content. format specifies format file imported. language language object. overwrite flag specifies whether overwrite existing object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/install_cluster_library.html","id":null,"dir":"Reference","previous_headings":"","what":"Add a library. — install_cluster_library","title":"Add a library. — install_cluster_library","text":"Add libraries installed cluster. installation asynchronous; happens background completion request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/install_cluster_library.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add a library. — install_cluster_library","text":"","code":"install_cluster_library(client, cluster_id, libraries)  librariesInstall(client, cluster_id, libraries)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/install_cluster_library.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add a library. — install_cluster_library","text":"client Required. Instance DatabricksClient() cluster_id Required. Unique identifier cluster install libraries. libraries Required. libraries install.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/install_cluster_library.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add a library. — install_cluster_library","text":"Note: actual set libraries installed cluster union libraries specified via method libraries set installed clusters via libraries UI.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_alerts.html","id":null,"dir":"Reference","previous_headings":"","what":"Get alerts. — list_alerts","title":"Get alerts. — list_alerts","text":"Gets list alerts.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_alerts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get alerts. — list_alerts","text":"","code":"list_alerts(client)  alertsList(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_alerts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get alerts. — list_alerts","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_catalogs.html","id":null,"dir":"Reference","previous_headings":"","what":"List catalogs. — list_catalogs","title":"List catalogs. — list_catalogs","text":"Gets array catalogs metastore. caller metastore admin, catalogs retrieved. Otherwise, catalogs owned caller (caller USE_CATALOG privilege) retrieved. guarantee specific ordering elements array.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_catalogs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List catalogs. — list_catalogs","text":"","code":"list_catalogs(client, include_browse = NULL)  catalogsList(client, include_browse = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_catalogs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List catalogs. — list_catalogs","text":"client Required. Instance DatabricksClient() include_browse Whether include catalogs response principal can access selective metadata .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_catalogs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List catalogs. — list_catalogs","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_clean_rooms.html","id":null,"dir":"Reference","previous_headings":"","what":"List clean rooms. — list_clean_rooms","title":"List clean rooms. — list_clean_rooms","text":"Gets array data object clean rooms metastore. caller must metastore admin owner clean room. guarantee specific ordering elements array.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_clean_rooms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List clean rooms. — list_clean_rooms","text":"","code":"list_clean_rooms(client, max_results = NULL, page_token = NULL)  cleanRoomsList(client, max_results = NULL, page_token = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_clean_rooms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List clean rooms. — list_clean_rooms","text":"client Required. Instance DatabricksClient() max_results Maximum number clean rooms return. page_token Opaque pagination token go next page based previous query.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_clean_rooms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List clean rooms. — list_clean_rooms","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_cluster_node_types.html","id":null,"dir":"Reference","previous_headings":"","what":"List node types. — list_cluster_node_types","title":"List node types. — list_cluster_node_types","text":"Returns list supported Spark node types. node types can used launch cluster.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_cluster_node_types.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List node types. — list_cluster_node_types","text":"","code":"list_cluster_node_types(client)  clustersListNodeTypes(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_cluster_node_types.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List node types. — list_cluster_node_types","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_cluster_policies.html","id":null,"dir":"Reference","previous_headings":"","what":"List cluster policies. — list_cluster_policies","title":"List cluster policies. — list_cluster_policies","text":"Returns list policies accessible requesting user.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_cluster_policies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List cluster policies. — list_cluster_policies","text":"","code":"list_cluster_policies(client, sort_column = NULL, sort_order = NULL)  clusterPoliciesList(client, sort_column = NULL, sort_order = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_cluster_policies.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List cluster policies. — list_cluster_policies","text":"client Required. Instance DatabricksClient() sort_column cluster policy attribute sort . sort_order order policies get listed.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_cluster_policies.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List cluster policies. — list_cluster_policies","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_cluster_policy_families.html","id":null,"dir":"Reference","previous_headings":"","what":"List policy families. — list_cluster_policy_families","title":"List policy families. — list_cluster_policy_families","text":"Retrieve list policy families. API paginated.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_cluster_policy_families.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List policy families. — list_cluster_policy_families","text":"","code":"list_cluster_policy_families(client, max_results = NULL, page_token = NULL)  policyFamiliesList(client, max_results = NULL, page_token = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_cluster_policy_families.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List policy families. — list_cluster_policy_families","text":"client Required. Instance DatabricksClient() max_results max number policy families return. page_token token can used get next page results.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_cluster_policy_families.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List policy families. — list_cluster_policy_families","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_cluster_zones.html","id":null,"dir":"Reference","previous_headings":"","what":"List availability zones. — list_cluster_zones","title":"List availability zones. — list_cluster_zones","text":"Returns list availability zones clusters can created (example, us-west-2a). zones can used launch cluster.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_cluster_zones.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List availability zones. — list_cluster_zones","text":"","code":"list_cluster_zones(client)  clustersListZones(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_cluster_zones.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List availability zones. — list_cluster_zones","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_clusters.html","id":null,"dir":"Reference","previous_headings":"","what":"List all clusters. — list_clusters","title":"List all clusters. — list_clusters","text":"Return information pinned clusters, active clusters, 200 recently terminated -purpose clusters past 30 days, 30 recently terminated job clusters past 30 days.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_clusters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List all clusters. — list_clusters","text":"","code":"list_clusters(client, can_use_client = NULL)  clustersList(client, can_use_client = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_clusters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List all clusters. — list_clusters","text":"client Required. Instance DatabricksClient() can_use_client Filter clusters based type client can used .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_clusters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List all clusters. — list_clusters","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_clusters.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List all clusters. — list_clusters","text":"example, 1 pinned cluster, 4 active clusters, 45 terminated -purpose clusters past 30 days, 50 terminated job clusters past 30 days, API returns 1 pinned cluster, 4 active clusters, 45 terminated -purpose clusters, 30 recently terminated job clusters.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_connections.html","id":null,"dir":"Reference","previous_headings":"","what":"List connections. — list_connections","title":"List connections. — list_connections","text":"List connections.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_connections.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List connections. — list_connections","text":"","code":"list_connections(client)  connectionsList(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_connections.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List connections. — list_connections","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_connections.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List connections. — list_connections","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_dashboards.html","id":null,"dir":"Reference","previous_headings":"","what":"Get dashboard objects. — list_dashboards","title":"Get dashboard objects. — list_dashboards","text":"Fetch paginated list dashboard objects.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_dashboards.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get dashboard objects. — list_dashboards","text":"","code":"list_dashboards(client, order = NULL, page = NULL, page_size = NULL, q = NULL)  dashboardsList(client, order = NULL, page = NULL, page_size = NULL, q = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_dashboards.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get dashboard objects. — list_dashboards","text":"client Required. Instance DatabricksClient() order Name dashboard attribute order . page Page number retrieve. page_size Number dashboards return per page. q Full text search term.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_dashboards.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get dashboard objects. — list_dashboards","text":"data.frame response pages.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_dashboards.html","id":"-warning-calling-this-api-concurrently-or-more-times-could-result-in","dir":"Reference","previous_headings":"","what":"**Warning: Calling this API concurrently 10 or more times could result in","title":"Get dashboard objects. — list_dashboards","text":"throttling, service degradation, temporary ban.**","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_data_sources.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a list of SQL warehouses. — list_data_sources","title":"Get a list of SQL warehouses. — list_data_sources","text":"Retrieves full list SQL warehouses available workspace. fields appear API response enumerated clarity. However, need SQL warehouse's id create new queries .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_data_sources.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a list of SQL warehouses. — list_data_sources","text":"","code":"list_data_sources(client)  dataSourcesList(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_data_sources.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a list of SQL warehouses. — list_data_sources","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_dbfs.html","id":null,"dir":"Reference","previous_headings":"","what":"List directory contents or file details. — list_dbfs","title":"List directory contents or file details. — list_dbfs","text":"List contents directory, details file. file directory exist, call throws exception RESOURCE_DOES_NOT_EXIST.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_dbfs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List directory contents or file details. — list_dbfs","text":"","code":"list_dbfs(client, path)  dbfsList(client, path)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_dbfs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List directory contents or file details. — list_dbfs","text":"client Required. Instance DatabricksClient() path Required. path file directory.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_dbfs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List directory contents or file details. — list_dbfs","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_dbfs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List directory contents or file details. — list_dbfs","text":"calling list large directory, list operation time approximately 60 seconds. strongly recommend using list directories containing less 10K files discourage using DBFS REST API operations list 10K files. Instead, recommend perform operations context cluster, using File system utility (dbutils.fs), provides functionality without timing .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_experiment_artifacts.html","id":null,"dir":"Reference","previous_headings":"","what":"Get all artifacts. — list_experiment_artifacts","title":"Get all artifacts. — list_experiment_artifacts","text":"List artifacts run. Takes optional artifact_path prefix. specified, response contains artifacts specified prefix.',","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_experiment_artifacts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get all artifacts. — list_experiment_artifacts","text":"","code":"list_experiment_artifacts(   client,   page_token = NULL,   path = NULL,   run_id = NULL,   run_uuid = NULL )  experimentsListArtifacts(   client,   page_token = NULL,   path = NULL,   run_id = NULL,   run_uuid = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_experiment_artifacts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get all artifacts. — list_experiment_artifacts","text":"client Required. Instance DatabricksClient() page_token Token indicating page artifact results fetch. path Filter artifacts matching path (relative path root artifact directory). run_id ID run whose artifacts list. run_uuid Deprecated, use run_id instead. ID run whose artifacts list.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_experiment_artifacts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get all artifacts. — list_experiment_artifacts","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_experiment_experiments.html","id":null,"dir":"Reference","previous_headings":"","what":"List experiments. — list_experiment_experiments","title":"List experiments. — list_experiment_experiments","text":"Gets list experiments.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_experiment_experiments.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List experiments. — list_experiment_experiments","text":"","code":"list_experiment_experiments(   client,   max_results = NULL,   page_token = NULL,   view_type = NULL )  experimentsListExperiments(   client,   max_results = NULL,   page_token = NULL,   view_type = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_experiment_experiments.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List experiments. — list_experiment_experiments","text":"client Required. Instance DatabricksClient() max_results Maximum number experiments desired. page_token Token indicating page experiments fetch. view_type Qualifier type experiments returned.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_experiment_experiments.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List experiments. — list_experiment_experiments","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_external_locations.html","id":null,"dir":"Reference","previous_headings":"","what":"List external locations. — list_external_locations","title":"List external locations. — list_external_locations","text":"Gets array external locations (ExternalLocationInfo objects) metastore. caller must metastore admin, owner external location, user privilege external location. guarantee specific ordering elements array.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_external_locations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List external locations. — list_external_locations","text":"","code":"list_external_locations(   client,   include_browse = NULL,   max_results = NULL,   page_token = NULL )  externalLocationsList(   client,   include_browse = NULL,   max_results = NULL,   page_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_external_locations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List external locations. — list_external_locations","text":"client Required. Instance DatabricksClient() include_browse Whether include external locations response principal can access selective metadata . max_results Maximum number external locations return. page_token Opaque pagination token go next page based previous query.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_external_locations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List external locations. — list_external_locations","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_file_directory_contents.html","id":null,"dir":"Reference","previous_headings":"","what":"List directory contents. — list_file_directory_contents","title":"List directory contents. — list_file_directory_contents","text":"Returns contents directory. directory specified path, API returns HTTP 404 error.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_file_directory_contents.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List directory contents. — list_file_directory_contents","text":"","code":"list_file_directory_contents(   client,   directory_path,   page_size = NULL,   page_token = NULL )  filesListDirectoryContents(   client,   directory_path,   page_size = NULL,   page_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_file_directory_contents.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List directory contents. — list_file_directory_contents","text":"client Required. Instance DatabricksClient() directory_path Required. absolute path directory. page_size maximum number directory entries return. page_token opaque page token next_page_token response previous request list contents directory.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_file_directory_contents.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List directory contents. — list_file_directory_contents","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_functions.html","id":null,"dir":"Reference","previous_headings":"","what":"List functions. — list_functions","title":"List functions. — list_functions","text":"List functions within specified parent catalog schema. user metastore admin, functions returned output list. Otherwise, user must USE_CATALOG privilege catalog USE_SCHEMA privilege schema, output list contains functions either user EXECUTE privilege user owner. guarantee specific ordering elements array.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_functions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List functions. — list_functions","text":"","code":"list_functions(   client,   catalog_name,   schema_name,   include_browse = NULL,   max_results = NULL,   page_token = NULL )  functionsList(   client,   catalog_name,   schema_name,   include_browse = NULL,   max_results = NULL,   page_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_functions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List functions. — list_functions","text":"client Required. Instance DatabricksClient() catalog_name Required. Name parent catalog functions interest. schema_name Required. Parent schema functions. include_browse Whether include functions response principal can access selective metadata . max_results Maximum number functions return. page_token Opaque pagination token go next page based previous query.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_functions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List functions. — list_functions","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_git_credentials.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Git credentials. — list_git_credentials","title":"Get Git credentials. — list_git_credentials","text":"Lists calling user's Git credentials. One credential per user supported.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_git_credentials.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Git credentials. — list_git_credentials","text":"","code":"list_git_credentials(client)  gitCredentialsList(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_git_credentials.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Git credentials. — list_git_credentials","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_git_credentials.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Git credentials. — list_git_credentials","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_global_init_scripts.html","id":null,"dir":"Reference","previous_headings":"","what":"Get init scripts. — list_global_init_scripts","title":"Get init scripts. — list_global_init_scripts","text":"Get list global init scripts workspace. returns properties script script contents. retrieve contents script, use get global init script operation.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_global_init_scripts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get init scripts. — list_global_init_scripts","text":"","code":"list_global_init_scripts(client)  globalInitScriptsList(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_global_init_scripts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get init scripts. — list_global_init_scripts","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_global_init_scripts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get init scripts. — list_global_init_scripts","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_groups.html","id":null,"dir":"Reference","previous_headings":"","what":"List group details. — list_groups","title":"List group details. — list_groups","text":"Gets details groups associated Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_groups.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List group details. — list_groups","text":"","code":"list_groups(   client,   attributes = NULL,   count = NULL,   excluded_attributes = NULL,   filter = NULL,   sort_by = NULL,   sort_order = NULL,   start_index = NULL )  groupsList(   client,   attributes = NULL,   count = NULL,   excluded_attributes = NULL,   filter = NULL,   sort_by = NULL,   sort_order = NULL,   start_index = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_groups.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List group details. — list_groups","text":"client Required. Instance DatabricksClient() attributes Comma-separated list attributes return response. count Desired number results per page. excluded_attributes Comma-separated list attributes exclude response. filter Query results filtered. sort_by Attribute sort results. sort_order order sort results. start_index Specifies index first result.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_groups.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List group details. — list_groups","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_instance_pools.html","id":null,"dir":"Reference","previous_headings":"","what":"List instance pool info. — list_instance_pools","title":"List instance pool info. — list_instance_pools","text":"Gets list instance pools statistics.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_instance_pools.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List instance pool info. — list_instance_pools","text":"","code":"list_instance_pools(client)  instancePoolsList(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_instance_pools.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List instance pool info. — list_instance_pools","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_instance_pools.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List instance pool info. — list_instance_pools","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_instance_profiles.html","id":null,"dir":"Reference","previous_headings":"","what":"List available instance profiles. — list_instance_profiles","title":"List available instance profiles. — list_instance_profiles","text":"List instance profiles calling user can use launch cluster.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_instance_profiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List available instance profiles. — list_instance_profiles","text":"","code":"list_instance_profiles(client)  instanceProfilesList(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_instance_profiles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List available instance profiles. — list_instance_profiles","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_instance_profiles.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List available instance profiles. — list_instance_profiles","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_instance_profiles.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List available instance profiles. — list_instance_profiles","text":"API available users.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_ip_access_lists.html","id":null,"dir":"Reference","previous_headings":"","what":"Get access lists. — list_ip_access_lists","title":"Get access lists. — list_ip_access_lists","text":"Gets IP access lists specified workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_ip_access_lists.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get access lists. — list_ip_access_lists","text":"","code":"list_ip_access_lists(client)  ipAccessListsList(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_ip_access_lists.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get access lists. — list_ip_access_lists","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_ip_access_lists.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get access lists. — list_ip_access_lists","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_job_runs.html","id":null,"dir":"Reference","previous_headings":"","what":"List job runs. — list_job_runs","title":"List job runs. — list_job_runs","text":"List runs descending order start time.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_job_runs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List job runs. — list_job_runs","text":"","code":"list_job_runs(   client,   active_only = NULL,   completed_only = NULL,   expand_tasks = NULL,   job_id = NULL,   limit = NULL,   offset = NULL,   page_token = NULL,   run_type = NULL,   start_time_from = NULL,   start_time_to = NULL )  jobsListRuns(   client,   active_only = NULL,   completed_only = NULL,   expand_tasks = NULL,   job_id = NULL,   limit = NULL,   offset = NULL,   page_token = NULL,   run_type = NULL,   start_time_from = NULL,   start_time_to = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_job_runs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List job runs. — list_job_runs","text":"client Required. Instance DatabricksClient() active_only active_only true, active runs included results; otherwise, lists active completed runs. completed_only completed_only true, completed runs included results; otherwise, lists active completed runs. expand_tasks Whether include task cluster details response. job_id job list runs. limit number runs return. offset offset first run return, relative recent run. page_token Use next_page_token prev_page_token returned previous request list next previous page runs respectively. run_type type runs return. start_time_from Show runs started value. start_time_to Show runs started value.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_job_runs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List job runs. — list_job_runs","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_jobs.html","id":null,"dir":"Reference","previous_headings":"","what":"List jobs. — list_jobs","title":"List jobs. — list_jobs","text":"Retrieves list jobs.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_jobs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List jobs. — list_jobs","text":"","code":"list_jobs(   client,   expand_tasks = NULL,   limit = NULL,   name = NULL,   offset = NULL,   page_token = NULL )  jobsList(   client,   expand_tasks = NULL,   limit = NULL,   name = NULL,   offset = NULL,   page_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_jobs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List jobs. — list_jobs","text":"client Required. Instance DatabricksClient() expand_tasks Whether include task cluster details response. limit number jobs return. name filter list based exact (case insensitive) job name. offset offset first job return, relative recently created job. page_token Use next_page_token prev_page_token returned previous request list next previous page jobs respectively.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_jobs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List jobs. — list_jobs","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_lakehouse_monitor_refreshes.html","id":null,"dir":"Reference","previous_headings":"","what":"List refreshes. — list_lakehouse_monitor_refreshes","title":"List refreshes. — list_lakehouse_monitor_refreshes","text":"Gets array containing history recent refreshes (25) table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_lakehouse_monitor_refreshes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List refreshes. — list_lakehouse_monitor_refreshes","text":"","code":"list_lakehouse_monitor_refreshes(client, full_name)  lakehouseMonitorsListRefreshes(client, full_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_lakehouse_monitor_refreshes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List refreshes. — list_lakehouse_monitor_refreshes","text":"client Required. Instance DatabricksClient() full_name Required. Full name table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_lakehouse_monitor_refreshes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List refreshes. — list_lakehouse_monitor_refreshes","text":"caller must either: 1. owner table's parent catalog 2. USE_CATALOG table's parent catalog owner table's parent schema 3. following permissions: - USE_CATALOG table's parent catalog - USE_SCHEMA table's parent schema - SELECT privilege table. Additionally, call must made workspace monitor created.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_metastores.html","id":null,"dir":"Reference","previous_headings":"","what":"List metastores. — list_metastores","title":"List metastores. — list_metastores","text":"Gets array available metastores (MetastoreInfo objects). caller must admin retrieve info. guarantee specific ordering elements array.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_metastores.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List metastores. — list_metastores","text":"","code":"list_metastores(client)  metastoresList(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_metastores.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List metastores. — list_metastores","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_metastores.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List metastores. — list_metastores","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_models.html","id":null,"dir":"Reference","previous_headings":"","what":"List models. — list_model_models","title":"List models. — list_model_models","text":"Lists available registered models, limit specified max_results.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List models. — list_model_models","text":"","code":"list_model_models(client, max_results = NULL, page_token = NULL)  modelRegistryListModels(client, max_results = NULL, page_token = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List models. — list_model_models","text":"client Required. Instance DatabricksClient() max_results Maximum number registered models desired. page_token Pagination token go next page based previous query.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List models. — list_model_models","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_transition_requests.html","id":null,"dir":"Reference","previous_headings":"","what":"List transition requests. — list_model_transition_requests","title":"List transition requests. — list_model_transition_requests","text":"Gets list open stage transition requests model version.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_transition_requests.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List transition requests. — list_model_transition_requests","text":"","code":"list_model_transition_requests(client, name, version)  modelRegistryListTransitionRequests(client, name, version)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_transition_requests.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List transition requests. — list_model_transition_requests","text":"client Required. Instance DatabricksClient() name Required. Name model. version Required. Version model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_transition_requests.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List transition requests. — list_model_transition_requests","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_versions.html","id":null,"dir":"Reference","previous_headings":"","what":"List Model Versions. — list_model_versions","title":"List Model Versions. — list_model_versions","text":"List model versions. can list model versions particular schema, list model versions current metastore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_versions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Model Versions. — list_model_versions","text":"","code":"list_model_versions(   client,   full_name,   include_browse = NULL,   max_results = NULL,   page_token = NULL )  modelVersionsList(   client,   full_name,   include_browse = NULL,   max_results = NULL,   page_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_versions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Model Versions. — list_model_versions","text":"client Required. Instance DatabricksClient() full_name Required. full three-level name registered model list model versions. include_browse Whether include model versions response principal can access selective metadata . max_results Maximum number model versions return. page_token Opaque pagination token go next page based previous query.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_versions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Model Versions. — list_model_versions","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_versions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Model Versions. — list_model_versions","text":"returned models filtered based privileges calling user. example, metastore admin able list model versions. regular user needs owner EXECUTE privilege parent registered model recieve model versions response. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema. guarantee specific ordering elements response.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_webhooks.html","id":null,"dir":"Reference","previous_headings":"","what":"List registry webhooks. — list_model_webhooks","title":"List registry webhooks. — list_model_webhooks","text":"NOTE: endpoint Public Preview.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_webhooks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List registry webhooks. — list_model_webhooks","text":"","code":"list_model_webhooks(   client,   events = NULL,   model_name = NULL,   page_token = NULL )  modelRegistryListWebhooks(   client,   events = NULL,   model_name = NULL,   page_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_webhooks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List registry webhooks. — list_model_webhooks","text":"client Required. Instance DatabricksClient() events events specified, webhook one specified trigger events included output. model_name specified, webhooks associated specified events listed, regardless associated model. page_token Token indicating page artifact results fetch.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_webhooks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List registry webhooks. — list_model_webhooks","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_model_webhooks.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List registry webhooks. — list_model_webhooks","text":"Lists registry webhooks.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_notebooks.html","id":null,"dir":"Reference","previous_headings":"","what":"List contents. — list_notebooks","title":"List contents. — list_notebooks","text":"Lists contents directory, object directory. input path exist, call returns error RESOURCE_DOES_NOT_EXIST.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_notebooks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List contents. — list_notebooks","text":"","code":"list_notebooks(client, path, notebooks_modified_after = NULL)  workspaceList(client, path, notebooks_modified_after = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_notebooks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List contents. — list_notebooks","text":"client Required. Instance DatabricksClient() path Required. absolute path notebook directory. notebooks_modified_after UTC timestamp milliseconds.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_notebooks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List contents. — list_notebooks","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_pipeline_events.html","id":null,"dir":"Reference","previous_headings":"","what":"List pipeline events. — list_pipeline_events","title":"List pipeline events. — list_pipeline_events","text":"Retrieves events pipeline.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_pipeline_events.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List pipeline events. — list_pipeline_events","text":"","code":"list_pipeline_events(   client,   pipeline_id,   filter = NULL,   max_results = NULL,   order_by = NULL,   page_token = NULL )  pipelinesListPipelineEvents(   client,   pipeline_id,   filter = NULL,   max_results = NULL,   order_by = NULL,   page_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_pipeline_events.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List pipeline events. — list_pipeline_events","text":"client Required. Instance DatabricksClient() pipeline_id Required. field description yet. filter Criteria select subset results, expressed using SQL-like syntax. max_results Max number entries return single page. order_by string indicating sort order timestamp results, example, timestamp asc. page_token Page token returned previous call.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_pipeline_events.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List pipeline events. — list_pipeline_events","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_pipeline_pipelines.html","id":null,"dir":"Reference","previous_headings":"","what":"List pipelines. — list_pipeline_pipelines","title":"List pipelines. — list_pipeline_pipelines","text":"Lists pipelines defined Delta Live Tables system.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_pipeline_pipelines.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List pipelines. — list_pipeline_pipelines","text":"","code":"list_pipeline_pipelines(   client,   filter = NULL,   max_results = NULL,   order_by = NULL,   page_token = NULL )  pipelinesListPipelines(   client,   filter = NULL,   max_results = NULL,   order_by = NULL,   page_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_pipeline_pipelines.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List pipelines. — list_pipeline_pipelines","text":"client Required. Instance DatabricksClient() filter Select subset results based specified criteria. max_results maximum number entries return single page. order_by list strings specifying order results. page_token Page token returned previous call.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_pipeline_pipelines.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List pipelines. — list_pipeline_pipelines","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_pipeline_updates.html","id":null,"dir":"Reference","previous_headings":"","what":"List pipeline updates. — list_pipeline_updates","title":"List pipeline updates. — list_pipeline_updates","text":"List updates active pipeline.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_pipeline_updates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List pipeline updates. — list_pipeline_updates","text":"","code":"list_pipeline_updates(   client,   pipeline_id,   max_results = NULL,   page_token = NULL,   until_update_id = NULL )  pipelinesListUpdates(   client,   pipeline_id,   max_results = NULL,   page_token = NULL,   until_update_id = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_pipeline_updates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List pipeline updates. — list_pipeline_updates","text":"client Required. Instance DatabricksClient() pipeline_id Required. pipeline return updates . max_results Max number entries return single page. page_token Page token returned previous call. until_update_id present, returns updates including update_id.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_provider_shares.html","id":null,"dir":"Reference","previous_headings":"","what":"List shares by Provider. — list_provider_shares","title":"List shares by Provider. — list_provider_shares","text":"Gets array specified provider's shares within metastore :","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_provider_shares.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List shares by Provider. — list_provider_shares","text":"","code":"list_provider_shares(client, name)  providersListShares(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_provider_shares.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List shares by Provider. — list_provider_shares","text":"client Required. Instance DatabricksClient() name Required. Name provider list shares.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_provider_shares.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List shares by Provider. — list_provider_shares","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_provider_shares.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List shares by Provider. — list_provider_shares","text":"caller metastore admin, * caller owner.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_providers.html","id":null,"dir":"Reference","previous_headings":"","what":"List providers. — list_providers","title":"List providers. — list_providers","text":"Gets array available authentication providers. caller must either metastore admin owner providers. Providers owned caller included response. guarantee specific ordering elements array.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_providers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List providers. — list_providers","text":"","code":"list_providers(client, data_provider_global_metastore_id = NULL)  providersList(client, data_provider_global_metastore_id = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_providers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List providers. — list_providers","text":"client Required. Instance DatabricksClient() data_provider_global_metastore_id provided, providers returned.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_providers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List providers. — list_providers","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_queries.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a list of queries. — list_queries","title":"Get a list of queries. — list_queries","text":"Gets list queries. Optionally, list can filtered search term.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_queries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a list of queries. — list_queries","text":"","code":"list_queries(client, order = NULL, page = NULL, page_size = NULL, q = NULL)  queriesList(client, order = NULL, page = NULL, page_size = NULL, q = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_queries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a list of queries. — list_queries","text":"client Required. Instance DatabricksClient() order Name query attribute order . page Page number retrieve. page_size Number queries return per page. q Full text search term.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_queries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a list of queries. — list_queries","text":"data.frame response pages.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_queries.html","id":"-warning-calling-this-api-concurrently-or-more-times-could-result-in","dir":"Reference","previous_headings":"","what":"**Warning: Calling this API concurrently 10 or more times could result in","title":"Get a list of queries. — list_queries","text":"throttling, service degradation, temporary ban.**","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_query_history.html","id":null,"dir":"Reference","previous_headings":"","what":"List Queries. — list_query_history","title":"List Queries. — list_query_history","text":"List history queries SQL warehouses.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_query_history.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Queries. — list_query_history","text":"","code":"list_query_history(   client,   filter_by = NULL,   include_metrics = NULL,   max_results = NULL,   page_token = NULL )  queryHistoryList(   client,   filter_by = NULL,   include_metrics = NULL,   max_results = NULL,   page_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_query_history.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Queries. — list_query_history","text":"client Required. Instance DatabricksClient() filter_by filter limit query history results. include_metrics Whether include metrics query. max_results Limit number results returned one page. page_token token can used get next page results.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_query_history.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Queries. — list_query_history","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_query_history.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Queries. — list_query_history","text":"can filter user ID, warehouse ID, status, time range.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_recipients.html","id":null,"dir":"Reference","previous_headings":"","what":"List share recipients. — list_recipients","title":"List share recipients. — list_recipients","text":"Gets array share recipients within current metastore :","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_recipients.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List share recipients. — list_recipients","text":"","code":"list_recipients(client, data_recipient_global_metastore_id = NULL)  recipientsList(client, data_recipient_global_metastore_id = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_recipients.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List share recipients. — list_recipients","text":"client Required. Instance DatabricksClient() data_recipient_global_metastore_id provided, recipients returned.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_recipients.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List share recipients. — list_recipients","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_recipients.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List share recipients. — list_recipients","text":"caller metastore admin, * caller owner. guarantee specific ordering elements array.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_registered_models.html","id":null,"dir":"Reference","previous_headings":"","what":"List Registered Models. — list_registered_models","title":"List Registered Models. — list_registered_models","text":"List registered models. can list registered models particular schema, list registered models current metastore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_registered_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Registered Models. — list_registered_models","text":"","code":"list_registered_models(   client,   catalog_name = NULL,   include_browse = NULL,   max_results = NULL,   page_token = NULL,   schema_name = NULL )  registeredModelsList(   client,   catalog_name = NULL,   include_browse = NULL,   max_results = NULL,   page_token = NULL,   schema_name = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_registered_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Registered Models. — list_registered_models","text":"client Required. Instance DatabricksClient() catalog_name identifier catalog list registered models. include_browse Whether include registered models response principal can access selective metadata . max_results Max number registered models return. page_token Opaque token send next page results (pagination). schema_name identifier schema list registered models.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_registered_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Registered Models. — list_registered_models","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_registered_models.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Registered Models. — list_registered_models","text":"returned models filtered based privileges calling user. example, metastore admin able list registered models. regular user needs owner EXECUTE privilege registered model recieve registered models response. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema. guarantee specific ordering elements response.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_repos.html","id":null,"dir":"Reference","previous_headings":"","what":"Get repos. — list_repos","title":"Get repos. — list_repos","text":"Returns repos calling user Manage permissions . Results paginated page containing twenty repos.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_repos.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get repos. — list_repos","text":"","code":"list_repos(client, next_page_token = NULL, path_prefix = NULL)  reposList(client, next_page_token = NULL, path_prefix = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_repos.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get repos. — list_repos","text":"client Required. Instance DatabricksClient() next_page_token Token used get next page results. path_prefix Filters repos paths starting given path prefix.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_repos.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get repos. — list_repos","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_schemas.html","id":null,"dir":"Reference","previous_headings":"","what":"List schemas. — list_schemas","title":"List schemas. — list_schemas","text":"Gets array schemas catalog metastore. caller metastore admin owner parent catalog, schemas catalog retrieved. Otherwise, schemas owned caller (caller USE_SCHEMA privilege) retrieved. guarantee specific ordering elements array.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_schemas.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List schemas. — list_schemas","text":"","code":"list_schemas(   client,   catalog_name,   include_browse = NULL,   max_results = NULL,   page_token = NULL )  schemasList(   client,   catalog_name,   include_browse = NULL,   max_results = NULL,   page_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_schemas.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List schemas. — list_schemas","text":"client Required. Instance DatabricksClient() catalog_name Required. Parent catalog schemas interest. include_browse Whether include schemas response principal can access selective metadata . max_results Maximum number schemas return. page_token Opaque pagination token go next page based previous query.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_schemas.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List schemas. — list_schemas","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_acls.html","id":null,"dir":"Reference","previous_headings":"","what":"Lists ACLs. — list_secret_acls","title":"Lists ACLs. — list_secret_acls","text":"List ACLs given secret scope. Users must MANAGE permission invoke API.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_acls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Lists ACLs. — list_secret_acls","text":"","code":"list_secret_acls(client, scope)  secretsListAcls(client, scope)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_acls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Lists ACLs. — list_secret_acls","text":"client Required. Instance DatabricksClient() scope Required. name scope fetch ACL information .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_acls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Lists ACLs. — list_secret_acls","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_acls.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Lists ACLs. — list_secret_acls","text":"Throws RESOURCE_DOES_NOT_EXIST secret scope exists. Throws PERMISSION_DENIED user permission make API call.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_scopes.html","id":null,"dir":"Reference","previous_headings":"","what":"List all scopes. — list_secret_scopes","title":"List all scopes. — list_secret_scopes","text":"Lists secret scopes available workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_scopes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List all scopes. — list_secret_scopes","text":"","code":"list_secret_scopes(client)  secretsListScopes(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_scopes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List all scopes. — list_secret_scopes","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_scopes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List all scopes. — list_secret_scopes","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_scopes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List all scopes. — list_secret_scopes","text":"Throws PERMISSION_DENIED user permission make API call.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_secrets.html","id":null,"dir":"Reference","previous_headings":"","what":"List secret keys. — list_secret_secrets","title":"List secret keys. — list_secret_secrets","text":"Lists secret keys stored scope. metadata-operation; secret data retrieved using API. Users need READ permission make call.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_secrets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List secret keys. — list_secret_secrets","text":"","code":"list_secret_secrets(client, scope)  secretsListSecrets(client, scope)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_secrets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List secret keys. — list_secret_secrets","text":"client Required. Instance DatabricksClient() scope Required. name scope list secrets within.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_secrets.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List secret keys. — list_secret_secrets","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_secret_secrets.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List secret keys. — list_secret_secrets","text":"lastUpdatedTimestamp returned milliseconds since epoch. Throws RESOURCE_DOES_NOT_EXIST secret scope exists. Throws PERMISSION_DENIED user permission make API call.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_service_principals.html","id":null,"dir":"Reference","previous_headings":"","what":"List service principals. — list_service_principals","title":"List service principals. — list_service_principals","text":"Gets set service principals associated Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_service_principals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List service principals. — list_service_principals","text":"","code":"list_service_principals(   client,   attributes = NULL,   count = NULL,   excluded_attributes = NULL,   filter = NULL,   sort_by = NULL,   sort_order = NULL,   start_index = NULL )  servicePrincipalsList(   client,   attributes = NULL,   count = NULL,   excluded_attributes = NULL,   filter = NULL,   sort_by = NULL,   sort_order = NULL,   start_index = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_service_principals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List service principals. — list_service_principals","text":"client Required. Instance DatabricksClient() attributes Comma-separated list attributes return response. count Desired number results per page. excluded_attributes Comma-separated list attributes exclude response. filter Query results filtered. sort_by Attribute sort results. sort_order order sort results. start_index Specifies index first result.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_service_principals.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List service principals. — list_service_principals","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_serving_endpoints.html","id":null,"dir":"Reference","previous_headings":"","what":"Get all serving endpoints. — list_serving_endpoints","title":"Get all serving endpoints. — list_serving_endpoints","text":"Get serving endpoints.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_serving_endpoints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get all serving endpoints. — list_serving_endpoints","text":"","code":"list_serving_endpoints(client)  servingEndpointsList(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_serving_endpoints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get all serving endpoints. — list_serving_endpoints","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_serving_endpoints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get all serving endpoints. — list_serving_endpoints","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_shares.html","id":null,"dir":"Reference","previous_headings":"","what":"List shares. — list_shares","title":"List shares. — list_shares","text":"Gets array data object shares metastore. caller must metastore admin owner share. guarantee specific ordering elements array.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_shares.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List shares. — list_shares","text":"","code":"list_shares(client)  sharesList(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_shares.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List shares. — list_shares","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_shares.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List shares. — list_shares","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_storage_credentials.html","id":null,"dir":"Reference","previous_headings":"","what":"List credentials. — list_storage_credentials","title":"List credentials. — list_storage_credentials","text":"Gets array storage credentials (StorageCredentialInfo objects). array limited storage credentials caller permission access. caller metastore admin, retrieval credentials unrestricted. guarantee specific ordering elements array.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_storage_credentials.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List credentials. — list_storage_credentials","text":"","code":"list_storage_credentials(client, max_results = NULL, page_token = NULL)  storageCredentialsList(client, max_results = NULL, page_token = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_storage_credentials.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List credentials. — list_storage_credentials","text":"client Required. Instance DatabricksClient() max_results Maximum number storage credentials return. page_token Opaque pagination token go next page based previous query.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_storage_credentials.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List credentials. — list_storage_credentials","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_system_schemas.html","id":null,"dir":"Reference","previous_headings":"","what":"List system schemas. — list_system_schemas","title":"List system schemas. — list_system_schemas","text":"Gets array system schemas metastore. caller must account admin metastore admin.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_system_schemas.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List system schemas. — list_system_schemas","text":"","code":"list_system_schemas(client, metastore_id)  systemSchemasList(client, metastore_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_system_schemas.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List system schemas. — list_system_schemas","text":"client Required. Instance DatabricksClient() metastore_id Required. ID metastore system schema resides.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_system_schemas.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List system schemas. — list_system_schemas","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_table_summaries.html","id":null,"dir":"Reference","previous_headings":"","what":"List table summaries. — list_table_summaries","title":"List table summaries. — list_table_summaries","text":"Gets array summaries tables schema catalog within metastore. table summaries returned either:","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_table_summaries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List table summaries. — list_table_summaries","text":"","code":"list_table_summaries(   client,   catalog_name,   max_results = NULL,   page_token = NULL,   schema_name_pattern = NULL,   table_name_pattern = NULL )  tablesListSummaries(   client,   catalog_name,   max_results = NULL,   page_token = NULL,   schema_name_pattern = NULL,   table_name_pattern = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_table_summaries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List table summaries. — list_table_summaries","text":"client Required. Instance DatabricksClient() catalog_name Required. Name parent catalog tables interest. max_results Maximum number summaries tables return. page_token Opaque pagination token go next page based previous query. schema_name_pattern sql LIKE pattern (% _) schema names. table_name_pattern sql LIKE pattern (% _) table names.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_table_summaries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List table summaries. — list_table_summaries","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_table_summaries.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List table summaries. — list_table_summaries","text":"summaries tables (within current metastore parent catalog schema), user metastore admin, : * summaries tables schemas (within current metastore parent catalog) user ownership SELECT privilege table ownership USE_SCHEMA privilege schema, provided user also ownership USE_CATALOG privilege parent catalog. guarantee specific ordering elements array.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_tables.html","id":null,"dir":"Reference","previous_headings":"","what":"List tables. — list_tables","title":"List tables. — list_tables","text":"Gets array tables current metastore parent catalog schema. caller must metastore admin owner (SELECT privilege ) table. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema. guarantee specific ordering elements array.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_tables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List tables. — list_tables","text":"","code":"list_tables(   client,   catalog_name,   schema_name,   include_browse = NULL,   include_delta_metadata = NULL,   max_results = NULL,   omit_columns = NULL,   omit_properties = NULL,   page_token = NULL )  tablesList(   client,   catalog_name,   schema_name,   include_browse = NULL,   include_delta_metadata = NULL,   max_results = NULL,   omit_columns = NULL,   omit_properties = NULL,   page_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_tables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List tables. — list_tables","text":"client Required. Instance DatabricksClient() catalog_name Required. Name parent catalog tables interest. schema_name Required. Parent schema tables. include_browse Whether include tables response principal can access selective metadata . include_delta_metadata Whether delta metadata included response. max_results Maximum number tables return. omit_columns Whether omit columns table response . omit_properties Whether omit properties table response . page_token Opaque token send next page results (pagination).","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_tables.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List tables. — list_tables","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_token_management.html","id":null,"dir":"Reference","previous_headings":"","what":"List all tokens. — list_token_management","title":"List all tokens. — list_token_management","text":"Lists tokens associated specified workspace user.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_token_management.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List all tokens. — list_token_management","text":"","code":"list_token_management(client, created_by_id = NULL, created_by_username = NULL)  tokenManagementList(client, created_by_id = NULL, created_by_username = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_token_management.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List all tokens. — list_token_management","text":"client Required. Instance DatabricksClient() created_by_id User ID user created token. created_by_username Username user created token.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_token_management.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List all tokens. — list_token_management","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_tokens.html","id":null,"dir":"Reference","previous_headings":"","what":"List tokens. — list_tokens","title":"List tokens. — list_tokens","text":"Lists valid tokens user-workspace pair.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_tokens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List tokens. — list_tokens","text":"","code":"list_tokens(client)  tokensList(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_tokens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List tokens. — list_tokens","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_tokens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List tokens. — list_tokens","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_users.html","id":null,"dir":"Reference","previous_headings":"","what":"List users. — list_users","title":"List users. — list_users","text":"Gets details users associated Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_users.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List users. — list_users","text":"","code":"list_users(   client,   attributes = NULL,   count = NULL,   excluded_attributes = NULL,   filter = NULL,   sort_by = NULL,   sort_order = NULL,   start_index = NULL )  usersList(   client,   attributes = NULL,   count = NULL,   excluded_attributes = NULL,   filter = NULL,   sort_by = NULL,   sort_order = NULL,   start_index = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_users.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List users. — list_users","text":"client Required. Instance DatabricksClient() attributes Comma-separated list attributes return response. count Desired number results per page. excluded_attributes Comma-separated list attributes exclude response. filter Query results filtered. sort_by Attribute sort results. sort_order order sort results. start_index Specifies index first result.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_users.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List users. — list_users","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_vector_search_endpoint_endpoints.html","id":null,"dir":"Reference","previous_headings":"","what":"List all endpoints. — list_vector_search_endpoint_endpoints","title":"List all endpoints. — list_vector_search_endpoint_endpoints","text":"List endpoints.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_vector_search_endpoint_endpoints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List all endpoints. — list_vector_search_endpoint_endpoints","text":"","code":"list_vector_search_endpoint_endpoints(client, page_token = NULL)  vectorSearchEndpointsListEndpoints(client, page_token = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_vector_search_endpoint_endpoints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List all endpoints. — list_vector_search_endpoint_endpoints","text":"client Required. Instance DatabricksClient() page_token Token pagination.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_vector_search_endpoint_endpoints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List all endpoints. — list_vector_search_endpoint_endpoints","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_vector_search_index_indexes.html","id":null,"dir":"Reference","previous_headings":"","what":"List indexes. — list_vector_search_index_indexes","title":"List indexes. — list_vector_search_index_indexes","text":"List indexes given endpoint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_vector_search_index_indexes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List indexes. — list_vector_search_index_indexes","text":"","code":"list_vector_search_index_indexes(client, endpoint_name, page_token = NULL)  vectorSearchIndexesListIndexes(client, endpoint_name, page_token = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_vector_search_index_indexes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List indexes. — list_vector_search_index_indexes","text":"client Required. Instance DatabricksClient() endpoint_name Required. Name endpoint. page_token Token pagination.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_vector_search_index_indexes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List indexes. — list_vector_search_index_indexes","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_volumes.html","id":null,"dir":"Reference","previous_headings":"","what":"List Volumes. — list_volumes","title":"List Volumes. — list_volumes","text":"Gets array volumes current metastore parent catalog schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_volumes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Volumes. — list_volumes","text":"","code":"list_volumes(   client,   catalog_name,   schema_name,   include_browse = NULL,   max_results = NULL,   page_token = NULL )  volumesList(   client,   catalog_name,   schema_name,   include_browse = NULL,   max_results = NULL,   page_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_volumes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Volumes. — list_volumes","text":"client Required. Instance DatabricksClient() catalog_name Required. identifier catalog. schema_name Required. identifier schema. include_browse Whether include volumes response principal can access selective metadata . max_results Maximum number volumes return (page length). page_token Opaque token returned previous request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_volumes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Volumes. — list_volumes","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_volumes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Volumes. — list_volumes","text":"returned volumes filtered based privileges calling user. example, metastore admin able list volumes. regular user needs owner READ VOLUME privilege volume recieve volumes response. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema. guarantee specific ordering elements array.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_warehouses.html","id":null,"dir":"Reference","previous_headings":"","what":"List warehouses. — list_warehouses","title":"List warehouses. — list_warehouses","text":"Lists SQL warehouses user manager permissions .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_warehouses.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List warehouses. — list_warehouses","text":"","code":"list_warehouses(client, run_as_user_id = NULL)  warehousesList(client, run_as_user_id = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_warehouses.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List warehouses. — list_warehouses","text":"client Required. Instance DatabricksClient() run_as_user_id Service Principal used fetch list warehouses.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/list_warehouses.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List warehouses. — list_warehouses","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Log a batch. — log_experiment_batch","title":"Log a batch. — log_experiment_batch","text":"Logs batch metrics, params, tags run. data failed persisted, server respond error (non-200 status code).","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log a batch. — log_experiment_batch","text":"","code":"log_experiment_batch(   client,   metrics = NULL,   params = NULL,   run_id = NULL,   tags = NULL )  experimentsLogBatch(   client,   metrics = NULL,   params = NULL,   run_id = NULL,   tags = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log a batch. — log_experiment_batch","text":"client Required. Instance DatabricksClient() metrics Metrics log. params Params log. run_id ID run log . tags Tags log.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Log a batch. — log_experiment_batch","text":"case error (due internal server error invalid request), partial data may written. can write metrics, params, tags interleaving fashion, within given entity type guaranteed follow order specified request body. overwrite behavior metrics, params, tags follows: Metrics: metric values never overwritten. Logging metric (key, value, timestamp) appends set values metric provided key. Tags: tag values can overwritten successive writes tag key. , multiple tag values key provided API request, last-provided tag value written. Logging tag (key, value) permitted. Specifically, logging tag idempotent. Parameters: written, param values changed (attempting overwrite param value result error). However, logging param (key, value) permitted. Specifically, logging param idempotent. Request Limits ------------------------------- single JSON-serialized API request may 1 MB size contain: 1000 metrics, params, tags total * 1000 metrics * 100 params * 100 tags example, valid request might contain 900 metrics, 50 params, 50 tags, logging 900 metrics, 50 params, 51 tags invalid. following limits also apply metric, param, tag keys values: Metric keys, param keys, tag keys can 250 characters length Parameter tag values can 250 characters length","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_inputs.html","id":null,"dir":"Reference","previous_headings":"","what":"Log inputs to a run. — log_experiment_inputs","title":"Log inputs to a run. — log_experiment_inputs","text":"NOTE: Experimental: API may change removed future release without warning.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_inputs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log inputs to a run. — log_experiment_inputs","text":"","code":"log_experiment_inputs(client, datasets = NULL, run_id = NULL)  experimentsLogInputs(client, datasets = NULL, run_id = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_inputs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log inputs to a run. — log_experiment_inputs","text":"client Required. Instance DatabricksClient() datasets Dataset inputs. run_id ID run log .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_metric.html","id":null,"dir":"Reference","previous_headings":"","what":"Log a metric. — log_experiment_metric","title":"Log a metric. — log_experiment_metric","text":"Logs metric run. metric key-value pair (string key, float value) associated timestamp. Examples include various metrics represent ML model accuracy. metric can logged multiple times.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_metric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log a metric. — log_experiment_metric","text":"","code":"log_experiment_metric(   client,   key,   value,   timestamp,   run_id = NULL,   run_uuid = NULL,   step = NULL )  experimentsLogMetric(   client,   key,   value,   timestamp,   run_id = NULL,   run_uuid = NULL,   step = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_metric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log a metric. — log_experiment_metric","text":"client Required. Instance DatabricksClient() key Required. Name metric. value Required. Double value metric logged. timestamp Required. Unix timestamp milliseconds time metric logged. run_id ID run log metric. run_uuid Deprecated, use run_id instead. ID run log metric. step Step log metric.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Log a model. — log_experiment_model","title":"Log a model. — log_experiment_model","text":"NOTE: Experimental: API may change removed future release without warning.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log a model. — log_experiment_model","text":"","code":"log_experiment_model(client, model_json = NULL, run_id = NULL)  experimentsLogModel(client, model_json = NULL, run_id = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log a model. — log_experiment_model","text":"client Required. Instance DatabricksClient() model_json MLmodel file json format. run_id ID run log .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_param.html","id":null,"dir":"Reference","previous_headings":"","what":"Log a param. — log_experiment_param","title":"Log a param. — log_experiment_param","text":"Logs param used run. param key-value pair (string key, string value). Examples include hyperparameters used ML model training constant dates values used ETL pipeline. param can logged run.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_param.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log a param. — log_experiment_param","text":"","code":"log_experiment_param(client, key, value, run_id = NULL, run_uuid = NULL)  experimentsLogParam(client, key, value, run_id = NULL, run_uuid = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/log_experiment_param.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log a param. — log_experiment_param","text":"client Required. Instance DatabricksClient() key Required. Name param. value Required. String value param logged. run_id ID run log param. run_uuid Deprecated, use run_id instead. ID run log param.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/logs_serving_endpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the latest logs for a served model. — logs_serving_endpoint","title":"Get the latest logs for a served model. — logs_serving_endpoint","text":"Retrieves service logs associated provided served model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/logs_serving_endpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the latest logs for a served model. — logs_serving_endpoint","text":"","code":"logs_serving_endpoint(client, name, served_model_name)  servingEndpointsLogs(client, name, served_model_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/logs_serving_endpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the latest logs for a served model. — logs_serving_endpoint","text":"client Required. Instance DatabricksClient() name Required. name serving endpoint served model belongs . served_model_name Required. name served model logs retrieved .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/me.html","id":null,"dir":"Reference","previous_headings":"","what":"Get current user info. — me","title":"Get current user info. — me","text":"Get details current method caller's identity.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/me.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get current user info. — me","text":"","code":"me(client)  currentUserMe(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/me.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get current user info. — me","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/migrate_permission_migration_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Migrate Permissions. — migrate_permission_migration_permissions","title":"Migrate Permissions. — migrate_permission_migration_permissions","text":"Migrate batch permissions workspace local group account group.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/migrate_permission_migration_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Migrate Permissions. — migrate_permission_migration_permissions","text":"","code":"migrate_permission_migration_permissions(   client,   workspace_id,   from_workspace_group_name,   to_account_group_name,   size = NULL )  permissionMigrationMigratePermissions(   client,   workspace_id,   from_workspace_group_name,   to_account_group_name,   size = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/migrate_permission_migration_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Migrate Permissions. — migrate_permission_migration_permissions","text":"client Required. Instance DatabricksClient() workspace_id Required. WorkspaceId associated workspace permission migration occur. from_workspace_group_name Required. name workspace group permissions migrated . to_account_group_name Required. name account group permissions migrated . size maximum number permissions migrated.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/mkdirs_dbfs.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a directory. — mkdirs_dbfs","title":"Create a directory. — mkdirs_dbfs","text":"Creates given directory necessary parent directories exist. file (directory) exists prefix input path, call throws exception RESOURCE_ALREADY_EXISTS. Note: operation fails, might succeeded creating necessary parent directories.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/mkdirs_dbfs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a directory. — mkdirs_dbfs","text":"","code":"mkdirs_dbfs(client, path)  dbfsMkdirs(client, path)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/mkdirs_dbfs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a directory. — mkdirs_dbfs","text":"client Required. Instance DatabricksClient() path Required. path new directory.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/mkdirs_notebook.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a directory. — mkdirs_notebook","title":"Create a directory. — mkdirs_notebook","text":"Creates specified directory (necessary parent directories exist). object (directory) prefix input path, call returns error RESOURCE_ALREADY_EXISTS.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/mkdirs_notebook.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a directory. — mkdirs_notebook","text":"","code":"mkdirs_notebook(client, path)  workspaceMkdirs(client, path)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/mkdirs_notebook.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a directory. — mkdirs_notebook","text":"client Required. Instance DatabricksClient() path Required. absolute path directory.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/mkdirs_notebook.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a directory. — mkdirs_notebook","text":"Note operation fails may succeeded creating necessary parent directories.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/move_dbfs.html","id":null,"dir":"Reference","previous_headings":"","what":"Move a file. — move_dbfs","title":"Move a file. — move_dbfs","text":"Moves file one location another location within DBFS. source file exist, call throws exception RESOURCE_DOES_NOT_EXIST. file already exists destination path, call throws exception RESOURCE_ALREADY_EXISTS. given source path directory, call always recursively moves files.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/move_dbfs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Move a file. — move_dbfs","text":"","code":"move_dbfs(client, source_path, destination_path)  dbfsMove(client, source_path, destination_path)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/move_dbfs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Move a file. — move_dbfs","text":"client Required. Instance DatabricksClient() source_path Required. source path file directory. destination_path Required. destination path file directory.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/patch_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Update group details. — patch_group","title":"Update group details. — patch_group","text":"Partially updates details group.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/patch_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update group details. — patch_group","text":"","code":"patch_group(client, id, operations = NULL, schemas = NULL)  groupsPatch(client, id, operations = NULL, schemas = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/patch_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update group details. — patch_group","text":"client Required. Instance DatabricksClient() id Required. Unique ID group Databricks workspace. operations field description yet. schemas schema patch request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/patch_service_principal.html","id":null,"dir":"Reference","previous_headings":"","what":"Update service principal details. — patch_service_principal","title":"Update service principal details. — patch_service_principal","text":"Partially updates details single service principal Databricks workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/patch_service_principal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update service principal details. — patch_service_principal","text":"","code":"patch_service_principal(client, id, operations = NULL, schemas = NULL)  servicePrincipalsPatch(client, id, operations = NULL, schemas = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/patch_service_principal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update service principal details. — patch_service_principal","text":"client Required. Instance DatabricksClient() id Required. Unique ID service principal Databricks workspace. operations field description yet. schemas schema patch request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/patch_serving_endpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Update tags of a serving endpoint. — patch_serving_endpoint","title":"Update tags of a serving endpoint. — patch_serving_endpoint","text":"Used batch add delete tags serving endpoint single API call.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/patch_serving_endpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update tags of a serving endpoint. — patch_serving_endpoint","text":"","code":"patch_serving_endpoint(client, name, add_tags = NULL, delete_tags = NULL)  servingEndpointsPatch(client, name, add_tags = NULL, delete_tags = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/patch_serving_endpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update tags of a serving endpoint. — patch_serving_endpoint","text":"client Required. Instance DatabricksClient() name Required. name serving endpoint tags patch. add_tags List endpoint tags add. delete_tags List tag keys delete.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/patch_user.html","id":null,"dir":"Reference","previous_headings":"","what":"Update user details. — patch_user","title":"Update user details. — patch_user","text":"Partially updates user resource applying supplied operations specific user attributes.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/patch_user.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update user details. — patch_user","text":"","code":"patch_user(client, id, operations = NULL, schemas = NULL)  usersPatch(client, id, operations = NULL, schemas = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/patch_user.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update user details. — patch_user","text":"client Required. Instance DatabricksClient() id Required. Unique ID user Databricks workspace. operations field description yet. schemas schema patch request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/permanent_cluster_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Permanently delete cluster. — permanent_cluster_delete","title":"Permanently delete cluster. — permanent_cluster_delete","text":"Permanently deletes Spark cluster. cluster terminated resources asynchronously removed.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/permanent_cluster_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Permanently delete cluster. — permanent_cluster_delete","text":"","code":"permanent_cluster_delete(client, cluster_id)  clustersPermanentDelete(client, cluster_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/permanent_cluster_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Permanently delete cluster. — permanent_cluster_delete","text":"client Required. Instance DatabricksClient() cluster_id Required. cluster deleted.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/permanent_cluster_delete.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Permanently delete cluster. — permanent_cluster_delete","text":"addition, users longer see permanently deleted clusters cluster list, API users can longer perform action permanently deleted clusters.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/pin_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Pin cluster. — pin_cluster","title":"Pin cluster. — pin_cluster","text":"Pinning cluster ensures cluster always returned ListClusters API. Pinning cluster already pinned effect. API can called workspace admins.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/pin_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pin cluster. — pin_cluster","text":"","code":"pin_cluster(client, cluster_id)  clustersPin(client, cluster_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/pin_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pin cluster. — pin_cluster","text":"client Required. Instance DatabricksClient() cluster_id Required. needs content added.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/publish_lakeview.html","id":null,"dir":"Reference","previous_headings":"","what":"Publish dashboard. — publish_lakeview","title":"Publish dashboard. — publish_lakeview","text":"Publish current draft dashboard.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/publish_lakeview.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Publish dashboard. — publish_lakeview","text":"","code":"publish_lakeview(   client,   dashboard_id,   embed_credentials = NULL,   warehouse_id = NULL )  lakeviewPublish(   client,   dashboard_id,   embed_credentials = NULL,   warehouse_id = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/publish_lakeview.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Publish dashboard. — publish_lakeview","text":"client Required. Instance DatabricksClient() dashboard_id Required. UUID identifying dashboard published. embed_credentials Flag indicate publisher's credentials embedded published dashboard. warehouse_id ID warehouse can used override warehouse set draft.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_dbfs.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a file. — put_dbfs","title":"Upload a file. — put_dbfs","text":"Uploads file use multipart form post. mainly used streaming uploads, can also used convenient single call data upload.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_dbfs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a file. — put_dbfs","text":"","code":"put_dbfs(client, path, contents = NULL, overwrite = NULL)  dbfsPut(client, path, contents = NULL, overwrite = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_dbfs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a file. — put_dbfs","text":"client Required. Instance DatabricksClient() path Required. path new file. contents parameter might absent, instead posted file used. overwrite flag specifies whether overwrite existing file/files.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_dbfs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Upload a file. — put_dbfs","text":"Alternatively can pass contents base64 string. amount data can passed (streaming) using contents parameter limited 1 MB. MAX_BLOCK_SIZE_EXCEEDED thrown limit exceeded. want upload large files, use streaming upload. details, see :method:dbfs/create, :method:dbfs/addBlock, :method:dbfs/close.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_secret.html","id":null,"dir":"Reference","previous_headings":"","what":"Add a secret. — put_secret","title":"Add a secret. — put_secret","text":"Inserts secret provided scope given name. secret already exists name, command overwrites existing secret's value. server encrypts secret using secret scope's encryption settings storing .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_secret.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add a secret. — put_secret","text":"","code":"put_secret(client, scope, key, bytes_value = NULL, string_value = NULL)  secretsPutSecret(client, scope, key, bytes_value = NULL, string_value = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_secret.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add a secret. — put_secret","text":"client Required. Instance DatabricksClient() scope Required. name scope secret associated . key Required. unique name identify secret. bytes_value specified, value stored bytes. string_value specified, note value stored UTF-8 (MB4) form.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_secret.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add a secret. — put_secret","text":"must WRITE MANAGE permission secret scope. secret key must consist alphanumeric characters, dashes, underscores, periods, exceed 128 characters. maximum allowed secret value size 128 KB. maximum number secrets given scope 1000. input fields 'string_value' 'bytes_value' specify type secret, determine value returned secret value requested. Exactly one must specified. Throws RESOURCE_DOES_NOT_EXIST secret scope exists. Throws RESOURCE_LIMIT_EXCEEDED maximum number secrets scope exceeded. Throws INVALID_PARAMETER_VALUE key name value length invalid. Throws PERMISSION_DENIED user permission make API call.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_secret_acl.html","id":null,"dir":"Reference","previous_headings":"","what":"Create/update an ACL. — put_secret_acl","title":"Create/update an ACL. — put_secret_acl","text":"Creates overwrites Access Control List (ACL) associated given principal (user group) specified scope point.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_secret_acl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create/update an ACL. — put_secret_acl","text":"","code":"put_secret_acl(client, scope, principal, permission)  secretsPutAcl(client, scope, principal, permission)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_secret_acl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create/update an ACL. — put_secret_acl","text":"client Required. Instance DatabricksClient() scope Required. name scope apply permissions . principal Required. principal permission applied. permission Required. permission level applied principal.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_secret_acl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create/update an ACL. — put_secret_acl","text":"general, user group use powerful permission available , permissions ordered follows: MANAGE - Allowed change ACLs, read write secret scope. WRITE - Allowed read write secret scope. * READ - Allowed read secret scope list secrets available. Note general, secret values can read within command cluster (example, notebook). API read actual secret value material outside cluster. However, user's permission applied based executing command, must least READ permission. Users must MANAGE permission invoke API. principal user group name corresponding existing Databricks principal granted revoked access. Throws RESOURCE_DOES_NOT_EXIST secret scope exists. Throws RESOURCE_ALREADY_EXISTS permission principal already exists. Throws INVALID_PARAMETER_VALUE permission principal invalid. Throws PERMISSION_DENIED user permission make API call.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_serving_endpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Update rate limits of a serving endpoint. — put_serving_endpoint","title":"Update rate limits of a serving endpoint. — put_serving_endpoint","text":"Used update rate limits serving endpoint. NOTE: external foundation model endpoints supported now.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_serving_endpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update rate limits of a serving endpoint. — put_serving_endpoint","text":"","code":"put_serving_endpoint(client, name, rate_limits = NULL)  servingEndpointsPut(client, name, rate_limits = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/put_serving_endpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update rate limits of a serving endpoint. — put_serving_endpoint","text":"client Required. Instance DatabricksClient() name Required. name serving endpoint whose rate limits updated. rate_limits list endpoint rate limits.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/query_serving_endpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Query a serving endpoint. — query_serving_endpoint","title":"Query a serving endpoint. — query_serving_endpoint","text":"Query serving endpoint.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/query_serving_endpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query a serving endpoint. — query_serving_endpoint","text":"","code":"query_serving_endpoint(   client,   name,   dataframe_records = NULL,   dataframe_split = NULL,   extra_params = NULL,   input = NULL,   inputs = NULL,   instances = NULL,   max_tokens = NULL,   messages = NULL,   n = NULL,   prompt = NULL,   stop = NULL,   stream = NULL,   temperature = NULL )  servingEndpointsQuery(   client,   name,   dataframe_records = NULL,   dataframe_split = NULL,   extra_params = NULL,   input = NULL,   inputs = NULL,   instances = NULL,   max_tokens = NULL,   messages = NULL,   n = NULL,   prompt = NULL,   stop = NULL,   stream = NULL,   temperature = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/query_serving_endpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query a serving endpoint. — query_serving_endpoint","text":"client Required. Instance DatabricksClient() name Required. name serving endpoint. dataframe_records Pandas Dataframe input records orientation. dataframe_split Pandas Dataframe input split orientation. extra_params extra parameters field used completions, chat, embeddings external & foundation model serving endpoints. input input string (array strings) field used embeddings external & foundation model serving endpoints field (along extra_params needed) used embeddings queries. inputs Tensor-based input columnar format. instances Tensor-based input row format. max_tokens max tokens field used completions chat external & foundation model serving endpoints. messages messages field used chat external & foundation model serving endpoints. n n (number candidates) field used completions chat external & foundation model serving endpoints. prompt prompt string (array strings) field used completions external & foundation model serving endpoints used completions query fields. stop stop sequences field used completions chat external & foundation model serving endpoints. stream stream field used completions chat external & foundation model serving endpoints. temperature temperature field used completions chat external & foundation model serving endpoints.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/query_vector_search_index.html","id":null,"dir":"Reference","previous_headings":"","what":"Query an index. — query_vector_search_index","title":"Query an index. — query_vector_search_index","text":"Query specified vector index.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/query_vector_search_index.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query an index. — query_vector_search_index","text":"","code":"query_vector_search_index(   client,   index_name,   columns,   filters_json = NULL,   num_results = NULL,   query_text = NULL,   query_vector = NULL,   score_threshold = NULL )  vectorSearchIndexesQueryIndex(   client,   index_name,   columns,   filters_json = NULL,   num_results = NULL,   query_text = NULL,   query_vector = NULL,   score_threshold = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/query_vector_search_index.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query an index. — query_vector_search_index","text":"client Required. Instance DatabricksClient() index_name Required. Name vector index query. columns Required. List column names include response. filters_json JSON string representing query filters. num_results Number results return. query_text Query text. query_vector Query vector. score_threshold Threshold approximate nearest neighbor search.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/read_dbfs.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the contents of a file. — read_dbfs","title":"Get the contents of a file. — read_dbfs","text":"Returns contents file. file exist, call throws exception RESOURCE_DOES_NOT_EXIST. path directory, read length negative, offset negative, call throws exception INVALID_PARAMETER_VALUE. read length exceeds 1 MB, call throws exception MAX_READ_SIZE_EXCEEDED.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/read_dbfs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the contents of a file. — read_dbfs","text":"","code":"read_dbfs(client, path, length = NULL, offset = NULL)  dbfsRead(client, path, length = NULL, offset = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/read_dbfs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the contents of a file. — read_dbfs","text":"client Required. Instance DatabricksClient() path Required. path file read. length number bytes read starting offset. offset offset read bytes.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/read_dbfs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the contents of a file. — read_dbfs","text":"offset + length exceeds number bytes file, reads contents end file.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/read_volume.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a Volume. — read_volume","title":"Get a Volume. — read_volume","text":"Gets volume metastore specific catalog schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/read_volume.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a Volume. — read_volume","text":"","code":"read_volume(client, name, include_browse = NULL)  volumesRead(client, name, include_browse = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/read_volume.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a Volume. — read_volume","text":"client Required. Instance DatabricksClient() name Required. three-level (fully qualified) name volume. include_browse Whether include volumes response principal can access selective metadata .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/read_volume.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a Volume. — read_volume","text":"caller must metastore admin owner (READ VOLUME privilege ) volume. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/reject_model_transition_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Reject a transition request. — reject_model_transition_request","title":"Reject a transition request. — reject_model_transition_request","text":"Rejects model version stage transition request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/reject_model_transition_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reject a transition request. — reject_model_transition_request","text":"","code":"reject_model_transition_request(client, name, version, stage, comment = NULL)  modelRegistryRejectTransitionRequest(   client,   name,   version,   stage,   comment = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/reject_model_transition_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reject a transition request. — reject_model_transition_request","text":"client Required. Instance DatabricksClient() name Required. Name model. version Required. Version model. stage Required. Target stage transition. comment User-provided comment action.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/remove_instance_profile.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove the instance profile. — remove_instance_profile","title":"Remove the instance profile. — remove_instance_profile","text":"Remove instance profile provided ARN. Existing clusters instance profile continue function.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/remove_instance_profile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove the instance profile. — remove_instance_profile","text":"","code":"remove_instance_profile(client, instance_profile_arn)  instanceProfilesRemove(client, instance_profile_arn)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/remove_instance_profile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove the instance profile. — remove_instance_profile","text":"client Required. Instance DatabricksClient() instance_profile_arn Required. ARN instance profile remove.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/remove_instance_profile.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remove the instance profile. — remove_instance_profile","text":"API accessible admin users.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/rename_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Rename a model. — rename_model","title":"Rename a model. — rename_model","text":"Renames registered model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/rename_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rename a model. — rename_model","text":"","code":"rename_model(client, name, new_name = NULL)  modelRegistryRenameModel(client, name, new_name = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/rename_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rename a model. — rename_model","text":"client Required. Instance DatabricksClient() name Required. Registered model unique name identifier. new_name provided, updates name registered_model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/repair_job_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Repair a job run. — repair_job_run","title":"Repair a job run. — repair_job_run","text":"Re-run one tasks. Tasks re-run part original job run. use current job task settings, can viewed history original job run.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/repair_job_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repair a job run. — repair_job_run","text":"","code":"repair_job_run(   client,   run_id,   dbt_commands = NULL,   jar_params = NULL,   job_parameters = NULL,   latest_repair_id = NULL,   notebook_params = NULL,   pipeline_params = NULL,   python_named_params = NULL,   python_params = NULL,   rerun_all_failed_tasks = NULL,   rerun_dependent_tasks = NULL,   rerun_tasks = NULL,   spark_submit_params = NULL,   sql_params = NULL )  jobsRepairRun(   client,   run_id,   dbt_commands = NULL,   jar_params = NULL,   job_parameters = NULL,   latest_repair_id = NULL,   notebook_params = NULL,   pipeline_params = NULL,   python_named_params = NULL,   python_params = NULL,   rerun_all_failed_tasks = NULL,   rerun_dependent_tasks = NULL,   rerun_tasks = NULL,   spark_submit_params = NULL,   sql_params = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/repair_job_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Repair a job run. — repair_job_run","text":"client Required. Instance DatabricksClient() run_id Required. job run ID run repair. dbt_commands array commands execute jobs dbt task, example 'dbt_commands': ['dbt deps', 'dbt seed', 'dbt run']. jar_params list parameters jobs Spark JAR tasks, example 'jar_params': ['john doe', '35']. job_parameters Job-level parameters used run. latest_repair_id ID latest repair. notebook_params map keys values jobs notebook task, example 'notebook_params': {'name': 'john doe', 'age': '35'}. pipeline_params field description yet. python_named_params map keys values jobs Python wheel task, example 'python_named_params': {'name': 'task', 'data': 'dbfs:/path//data.json'}. python_params list parameters jobs Python tasks, example 'python_params': ['john doe', '35']. rerun_all_failed_tasks true, repair failed tasks. rerun_dependent_tasks true, repair tasks depend tasks rerun_tasks, even previously successful. rerun_tasks task keys task runs repair. spark_submit_params list parameters jobs spark submit task, example 'spark_submit_params': ['--class', 'org.apache.spark.examples.SparkPi']. sql_params map keys values jobs SQL task, example 'sql_params': {'name': 'john doe', 'age': '35'}.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/repair_job_run_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Repair a job run. — repair_job_run_and_wait","title":"Repair a job run. — repair_job_run_and_wait","text":"long-running operation, blocks Jobs Databricks reach TERMINATED SKIPPED state timeout 20 minutes, can change via timeout parameter. default, state Databricks Jobs reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/repair_job_run_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repair a job run. — repair_job_run_and_wait","text":"","code":"repair_job_run_and_wait(   client,   run_id,   dbt_commands = NULL,   jar_params = NULL,   job_parameters = NULL,   latest_repair_id = NULL,   notebook_params = NULL,   pipeline_params = NULL,   python_named_params = NULL,   python_params = NULL,   rerun_all_failed_tasks = NULL,   rerun_dependent_tasks = NULL,   rerun_tasks = NULL,   spark_submit_params = NULL,   sql_params = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/repair_job_run_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Repair a job run. — repair_job_run_and_wait","text":"client Required. Instance DatabricksClient() run_id Required. job run ID run repair. dbt_commands array commands execute jobs dbt task, example 'dbt_commands': ['dbt deps', 'dbt seed', 'dbt run']. jar_params list parameters jobs Spark JAR tasks, example 'jar_params': ['john doe', '35']. job_parameters Job-level parameters used run. latest_repair_id ID latest repair. notebook_params map keys values jobs notebook task, example 'notebook_params': {'name': 'john doe', 'age': '35'}. pipeline_params field description yet. python_named_params map keys values jobs Python wheel task, example 'python_named_params': {'name': 'task', 'data': 'dbfs:/path//data.json'}. python_params list parameters jobs Python tasks, example 'python_params': ['john doe', '35']. rerun_all_failed_tasks true, repair failed tasks. rerun_dependent_tasks true, repair tasks depend tasks rerun_tasks, even previously successful. rerun_tasks task keys task runs repair. spark_submit_params list parameters jobs spark submit task, example 'spark_submit_params': ['--class', 'org.apache.spark.examples.SparkPi']. sql_params map keys values jobs SQL task, example 'sql_params': {'name': 'john doe', 'age': '35'}. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/repair_job_run_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Repair a job run. — repair_job_run_and_wait","text":"Re-run one tasks. Tasks re-run part original job run. use current job task settings, can viewed history original job run.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/replace_ip_access_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace access list. — replace_ip_access_list","title":"Replace access list. — replace_ip_access_list","text":"Replaces IP access list, specified ID.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/replace_ip_access_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace access list. — replace_ip_access_list","text":"","code":"replace_ip_access_list(   client,   ip_access_list_id,   label,   list_type,   enabled,   ip_addresses = NULL )  ipAccessListsReplace(   client,   ip_access_list_id,   label,   list_type,   enabled,   ip_addresses = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/replace_ip_access_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace access list. — replace_ip_access_list","text":"client Required. Instance DatabricksClient() ip_access_list_id Required. ID corresponding IP access list. label Required. Label IP access list. list_type Required. Type IP access list. enabled Required. Specifies whether IP access list enabled. ip_addresses field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/replace_ip_access_list.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Replace access list. — replace_ip_access_list","text":"list can include allow lists block lists. See top file description server treats allow lists block lists run time. replacing IP access list: * allow lists block lists combined, API supports maximum 1000 IP/CIDR values, one CIDR counts single value. Attempts exceed number return error 400 error_code value QUOTA_EXCEEDED. * resulting list block calling user's current IP, error 400 returned error_code value INVALID_STATE. can take minutes changes take effect. Note resulting IP access list effect enable feature. See :method:workspaceconf/setStatus.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/reset_job.html","id":null,"dir":"Reference","previous_headings":"","what":"Update all job settings (reset). — reset_job","title":"Update all job settings (reset). — reset_job","text":"Overwrite settings given job. Use Update endpoint update job settings partially.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/reset_job.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update all job settings (reset). — reset_job","text":"","code":"reset_job(client, job_id, new_settings)  jobsReset(client, job_id, new_settings)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/reset_job.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update all job settings (reset). — reset_job","text":"client Required. Instance DatabricksClient() job_id Required. canonical identifier job reset. new_settings Required. new settings job.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/resize_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Resize cluster. — resize_cluster","title":"Resize cluster. — resize_cluster","text":"Resizes cluster desired number workers. fail unless cluster RUNNING state.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/resize_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resize cluster. — resize_cluster","text":"","code":"resize_cluster(client, cluster_id, autoscale = NULL, num_workers = NULL)  clustersResize(client, cluster_id, autoscale = NULL, num_workers = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/resize_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resize cluster. — resize_cluster","text":"client Required. Instance DatabricksClient() cluster_id Required. cluster resized. autoscale Parameters needed order automatically scale clusters based load. num_workers Number worker nodes cluster .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/resize_cluster_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Resize cluster. — resize_cluster_and_wait","title":"Resize cluster. — resize_cluster_and_wait","text":"long-running operation, blocks Clusters Databricks reach RUNNING state timeout 20 minutes, can change via timeout parameter. default, state Databricks Clusters reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/resize_cluster_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resize cluster. — resize_cluster_and_wait","text":"","code":"resize_cluster_and_wait(   client,   cluster_id,   autoscale = NULL,   num_workers = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/resize_cluster_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resize cluster. — resize_cluster_and_wait","text":"client Required. Instance DatabricksClient() cluster_id Required. cluster resized. autoscale Parameters needed order automatically scale clusters based load. num_workers Number worker nodes cluster . timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/resize_cluster_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Resize cluster. — resize_cluster_and_wait","text":"Resizes cluster desired number workers. fail unless cluster RUNNING state.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restart_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Restart cluster. — restart_cluster","title":"Restart cluster. — restart_cluster","text":"Restarts Spark cluster supplied ID. cluster currently RUNNING state, nothing happen.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restart_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restart cluster. — restart_cluster","text":"","code":"restart_cluster(client, cluster_id, restart_user = NULL)  clustersRestart(client, cluster_id, restart_user = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restart_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restart cluster. — restart_cluster","text":"client Required. Instance DatabricksClient() cluster_id Required. cluster started. restart_user needs content added.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restart_cluster_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Restart cluster. — restart_cluster_and_wait","title":"Restart cluster. — restart_cluster_and_wait","text":"long-running operation, blocks Clusters Databricks reach RUNNING state timeout 20 minutes, can change via timeout parameter. default, state Databricks Clusters reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restart_cluster_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restart cluster. — restart_cluster_and_wait","text":"","code":"restart_cluster_and_wait(   client,   cluster_id,   restart_user = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restart_cluster_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restart cluster. — restart_cluster_and_wait","text":"client Required. Instance DatabricksClient() cluster_id Required. cluster started. restart_user needs content added. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restart_cluster_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Restart cluster. — restart_cluster_and_wait","text":"Restarts Spark cluster supplied ID. cluster currently RUNNING state, nothing happen.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_dashboard.html","id":null,"dir":"Reference","previous_headings":"","what":"Restore a dashboard. — restore_dashboard","title":"Restore a dashboard. — restore_dashboard","text":"restored dashboard appears list views searches can shared.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_dashboard.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restore a dashboard. — restore_dashboard","text":"","code":"restore_dashboard(client, dashboard_id)  dashboardsRestore(client, dashboard_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_dashboard.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restore a dashboard. — restore_dashboard","text":"client Required. Instance DatabricksClient() dashboard_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_experiment.html","id":null,"dir":"Reference","previous_headings":"","what":"Restores an experiment. — restore_experiment","title":"Restores an experiment. — restore_experiment","text":"Restore experiment marked deletion. also restores associated metadata, runs, metrics, params, tags. experiment uses FileStore, underlying artifacts associated experiment also restored.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_experiment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restores an experiment. — restore_experiment","text":"","code":"restore_experiment(client, experiment_id)  experimentsRestoreExperiment(client, experiment_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_experiment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restores an experiment. — restore_experiment","text":"client Required. Instance DatabricksClient() experiment_id Required. ID associated experiment.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_experiment.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Restores an experiment. — restore_experiment","text":"Throws RESOURCE_DOES_NOT_EXIST experiment never created permanently deleted.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_experiment_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Restore a run. — restore_experiment_run","title":"Restore a run. — restore_experiment_run","text":"Restores deleted run.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_experiment_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restore a run. — restore_experiment_run","text":"","code":"restore_experiment_run(client, run_id)  experimentsRestoreRun(client, run_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_experiment_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restore a run. — restore_experiment_run","text":"client Required. Instance DatabricksClient() run_id Required. ID run restore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_experiment_runs.html","id":null,"dir":"Reference","previous_headings":"","what":"Restore runs by deletion time. — restore_experiment_runs","title":"Restore runs by deletion time. — restore_experiment_runs","text":"Bulk restore runs experiment deleted earlier specified timestamp. Restores max_runs per request. call API Databricks Notebook Python, can use client code snippet https://learn.microsoft.com/en-us/azure/databricks/mlflow/runs#bulk-restore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_experiment_runs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restore runs by deletion time. — restore_experiment_runs","text":"","code":"restore_experiment_runs(   client,   experiment_id,   min_timestamp_millis,   max_runs = NULL )  experimentsRestoreRuns(   client,   experiment_id,   min_timestamp_millis,   max_runs = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_experiment_runs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restore runs by deletion time. — restore_experiment_runs","text":"client Required. Instance DatabricksClient() experiment_id Required. ID experiment containing runs restore. min_timestamp_millis Required. minimum deletion timestamp milliseconds since UNIX epoch restoring runs. max_runs optional positive integer indicating maximum number runs restore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Restore a query. — restore_query","title":"Restore a query. — restore_query","text":"Restore query moved trash. restored query appears list views searches. can use restored queries alerts.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restore a query. — restore_query","text":"","code":"restore_query(client, query_id)  queriesRestore(client, query_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/restore_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restore a query. — restore_query","text":"client Required. Instance DatabricksClient() query_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/retrieve_recipient_activation_token.html","id":null,"dir":"Reference","previous_headings":"","what":"Get an access token. — retrieve_recipient_activation_token","title":"Get an access token. — retrieve_recipient_activation_token","text":"Retrieve access token activation url. public API without authentication.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/retrieve_recipient_activation_token.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get an access token. — retrieve_recipient_activation_token","text":"","code":"retrieve_recipient_activation_token(client, activation_url)  recipientActivationRetrieveToken(client, activation_url)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/retrieve_recipient_activation_token.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get an access token. — retrieve_recipient_activation_token","text":"client Required. Instance DatabricksClient() activation_url Required. one time activation url.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/rotate_recipient_token.html","id":null,"dir":"Reference","previous_headings":"","what":"Rotate a token. — rotate_recipient_token","title":"Rotate a token. — rotate_recipient_token","text":"Refreshes specified recipient's delta sharing authentication token provided token info. caller must owner recipient.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/rotate_recipient_token.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rotate a token. — rotate_recipient_token","text":"","code":"rotate_recipient_token(client, name, existing_token_expire_in_seconds)  recipientsRotateToken(client, name, existing_token_expire_in_seconds)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/rotate_recipient_token.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rotate a token. — rotate_recipient_token","text":"client Required. Instance DatabricksClient() name Required. name recipient. existing_token_expire_in_seconds Required. expiration time bearer token ISO 8601 format.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/run_job_now.html","id":null,"dir":"Reference","previous_headings":"","what":"Trigger a new job run. — run_job_now","title":"Trigger a new job run. — run_job_now","text":"Run job return run_id triggered run.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/run_job_now.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trigger a new job run. — run_job_now","text":"","code":"run_job_now(   client,   job_id,   dbt_commands = NULL,   idempotency_token = NULL,   jar_params = NULL,   job_parameters = NULL,   notebook_params = NULL,   pipeline_params = NULL,   python_named_params = NULL,   python_params = NULL,   queue = NULL,   spark_submit_params = NULL,   sql_params = NULL )  jobsRunNow(   client,   job_id,   dbt_commands = NULL,   idempotency_token = NULL,   jar_params = NULL,   job_parameters = NULL,   notebook_params = NULL,   pipeline_params = NULL,   python_named_params = NULL,   python_params = NULL,   queue = NULL,   spark_submit_params = NULL,   sql_params = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/run_job_now.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trigger a new job run. — run_job_now","text":"client Required. Instance DatabricksClient() job_id Required. ID job executed. dbt_commands array commands execute jobs dbt task, example 'dbt_commands': ['dbt deps', 'dbt seed', 'dbt run']. idempotency_token optional token guarantee idempotency job run requests. jar_params list parameters jobs Spark JAR tasks, example 'jar_params': ['john doe', '35']. job_parameters Job-level parameters used run. notebook_params map keys values jobs notebook task, example 'notebook_params': {'name': 'john doe', 'age': '35'}. pipeline_params field description yet. python_named_params map keys values jobs Python wheel task, example 'python_named_params': {'name': 'task', 'data': 'dbfs:/path//data.json'}. python_params list parameters jobs Python tasks, example 'python_params': ['john doe', '35']. queue queue settings run. spark_submit_params list parameters jobs spark submit task, example 'spark_submit_params': ['--class', 'org.apache.spark.examples.SparkPi']. sql_params map keys values jobs SQL task, example 'sql_params': {'name': 'john doe', 'age': '35'}.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/run_job_now_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Trigger a new job run. — run_job_now_and_wait","title":"Trigger a new job run. — run_job_now_and_wait","text":"long-running operation, blocks Jobs Databricks reach TERMINATED SKIPPED state timeout 20 minutes, can change via timeout parameter. default, state Databricks Jobs reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/run_job_now_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trigger a new job run. — run_job_now_and_wait","text":"","code":"run_job_now_and_wait(   client,   job_id,   dbt_commands = NULL,   idempotency_token = NULL,   jar_params = NULL,   job_parameters = NULL,   notebook_params = NULL,   pipeline_params = NULL,   python_named_params = NULL,   python_params = NULL,   queue = NULL,   spark_submit_params = NULL,   sql_params = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/run_job_now_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trigger a new job run. — run_job_now_and_wait","text":"client Required. Instance DatabricksClient() job_id Required. ID job executed. dbt_commands array commands execute jobs dbt task, example 'dbt_commands': ['dbt deps', 'dbt seed', 'dbt run']. idempotency_token optional token guarantee idempotency job run requests. jar_params list parameters jobs Spark JAR tasks, example 'jar_params': ['john doe', '35']. job_parameters Job-level parameters used run. notebook_params map keys values jobs notebook task, example 'notebook_params': {'name': 'john doe', 'age': '35'}. pipeline_params field description yet. python_named_params map keys values jobs Python wheel task, example 'python_named_params': {'name': 'task', 'data': 'dbfs:/path//data.json'}. python_params list parameters jobs Python tasks, example 'python_params': ['john doe', '35']. queue queue settings run. spark_submit_params list parameters jobs spark submit task, example 'spark_submit_params': ['--class', 'org.apache.spark.examples.SparkPi']. sql_params map keys values jobs SQL task, example 'sql_params': {'name': 'john doe', 'age': '35'}. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/run_job_now_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Trigger a new job run. — run_job_now_and_wait","text":"Run job return run_id triggered run.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/run_lakehouse_monitor_refresh.html","id":null,"dir":"Reference","previous_headings":"","what":"Queue a metric refresh for a monitor. — run_lakehouse_monitor_refresh","title":"Queue a metric refresh for a monitor. — run_lakehouse_monitor_refresh","text":"Queues metric refresh monitor specified table. refresh execute background.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/run_lakehouse_monitor_refresh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Queue a metric refresh for a monitor. — run_lakehouse_monitor_refresh","text":"","code":"run_lakehouse_monitor_refresh(client, full_name)  lakehouseMonitorsRunRefresh(client, full_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/run_lakehouse_monitor_refresh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Queue a metric refresh for a monitor. — run_lakehouse_monitor_refresh","text":"client Required. Instance DatabricksClient() full_name Required. Full name table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/run_lakehouse_monitor_refresh.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Queue a metric refresh for a monitor. — run_lakehouse_monitor_refresh","text":"caller must either: 1. owner table's parent catalog 2. USE_CATALOG table's parent catalog owner table's parent schema 3. following permissions: - USE_CATALOG table's parent catalog - USE_SCHEMA table's parent schema - owner table Additionally, call must made workspace monitor created.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_experiment_experiments.html","id":null,"dir":"Reference","previous_headings":"","what":"Search experiments. — search_experiment_experiments","title":"Search experiments. — search_experiment_experiments","text":"Searches experiments satisfy specified search criteria.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_experiment_experiments.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search experiments. — search_experiment_experiments","text":"","code":"search_experiment_experiments(   client,   filter = NULL,   max_results = NULL,   order_by = NULL,   page_token = NULL,   view_type = NULL )  experimentsSearchExperiments(   client,   filter = NULL,   max_results = NULL,   order_by = NULL,   page_token = NULL,   view_type = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_experiment_experiments.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search experiments. — search_experiment_experiments","text":"client Required. Instance DatabricksClient() filter String representing SQL filter condition (e.g. max_results Maximum number experiments desired. order_by List columns ordering search results, can include experiment name last updated timestamp optional 'DESC' 'ASC' annotation, 'ASC' default. page_token Token indicating page experiments fetch. view_type Qualifier type experiments returned.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_experiment_experiments.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search experiments. — search_experiment_experiments","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_experiment_runs.html","id":null,"dir":"Reference","previous_headings":"","what":"Search for runs. — search_experiment_runs","title":"Search for runs. — search_experiment_runs","text":"Searches runs satisfy expressions.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_experiment_runs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search for runs. — search_experiment_runs","text":"","code":"search_experiment_runs(   client,   experiment_ids = NULL,   filter = NULL,   max_results = NULL,   order_by = NULL,   page_token = NULL,   run_view_type = NULL )  experimentsSearchRuns(   client,   experiment_ids = NULL,   filter = NULL,   max_results = NULL,   order_by = NULL,   page_token = NULL,   run_view_type = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_experiment_runs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search for runs. — search_experiment_runs","text":"client Required. Instance DatabricksClient() experiment_ids List experiment IDs search . filter filter expression params, metrics, tags, allows returning subset runs. max_results Maximum number runs desired. order_by List columns ordered , including attributes, params, metrics, tags optional 'DESC' 'ASC' annotation, 'ASC' default. page_token Token current page runs. run_view_type Whether display active, deleted, runs.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_experiment_runs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search for runs. — search_experiment_runs","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_experiment_runs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Search for runs. — search_experiment_runs","text":"Search expressions can use mlflowMetric mlflowParam keys.',","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_model_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Search models. — search_model_models","title":"Search models. — search_model_models","text":"Search registered models based specified filter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_model_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search models. — search_model_models","text":"","code":"search_model_models(   client,   filter = NULL,   max_results = NULL,   order_by = NULL,   page_token = NULL )  modelRegistrySearchModels(   client,   filter = NULL,   max_results = NULL,   order_by = NULL,   page_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_model_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search models. — search_model_models","text":"client Required. Instance DatabricksClient() filter String filter condition, like 'name LIKE '-model-name''. max_results Maximum number models desired. order_by List columns ordering search results, can include model name last updated timestamp optional 'DESC' 'ASC' annotation, 'ASC' default. page_token Pagination token go next page based previous search query.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_model_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search models. — search_model_models","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_model_versions.html","id":null,"dir":"Reference","previous_headings":"","what":"Searches model versions. — search_model_versions","title":"Searches model versions. — search_model_versions","text":"Searches specific model versions based supplied filter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_model_versions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Searches model versions. — search_model_versions","text":"","code":"search_model_versions(   client,   filter = NULL,   max_results = NULL,   order_by = NULL,   page_token = NULL )  modelRegistrySearchModelVersions(   client,   filter = NULL,   max_results = NULL,   order_by = NULL,   page_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_model_versions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Searches model versions. — search_model_versions","text":"client Required. Instance DatabricksClient() filter String filter condition, like 'name='-model-name''. max_results Maximum number models desired. order_by List columns ordered including model name, version, stage optional 'DESC' 'ASC' annotation, 'ASC' default. page_token Pagination token go next page based previous search query.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/search_model_versions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Searches model versions. — search_model_versions","text":"data.frame response pages.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_cluster_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Set cluster permissions. — set_cluster_permissions","title":"Set cluster permissions. — set_cluster_permissions","text":"Sets permissions cluster. Clusters can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_cluster_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set cluster permissions. — set_cluster_permissions","text":"","code":"set_cluster_permissions(client, cluster_id, access_control_list = NULL)  clustersSetPermissions(client, cluster_id, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_cluster_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set cluster permissions. — set_cluster_permissions","text":"client Required. Instance DatabricksClient() cluster_id Required. cluster get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_cluster_policy_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Set cluster policy permissions. — set_cluster_policy_permissions","title":"Set cluster policy permissions. — set_cluster_policy_permissions","text":"Sets permissions cluster policy. Cluster policies can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_cluster_policy_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set cluster policy permissions. — set_cluster_policy_permissions","text":"","code":"set_cluster_policy_permissions(   client,   cluster_policy_id,   access_control_list = NULL )  clusterPoliciesSetPermissions(   client,   cluster_policy_id,   access_control_list = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_cluster_policy_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set cluster policy permissions. — set_cluster_policy_permissions","text":"client Required. Instance DatabricksClient() cluster_policy_id Required. cluster policy get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_dbsql_permission.html","id":null,"dir":"Reference","previous_headings":"","what":"Set object ACL. — set_dbsql_permission","title":"Set object ACL. — set_dbsql_permission","text":"Sets access control list (ACL) specified object. operation complete rewrite ACL.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_dbsql_permission.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set object ACL. — set_dbsql_permission","text":"","code":"set_dbsql_permission(   client,   object_type,   object_id,   access_control_list = NULL )  dbsqlPermissionsSet(client, object_type, object_id, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_dbsql_permission.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set object ACL. — set_dbsql_permission","text":"client Required. Instance DatabricksClient() object_type Required. type object permission set. object_id Required. Object ID. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_experiment_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Set experiment permissions. — set_experiment_permissions","title":"Set experiment permissions. — set_experiment_permissions","text":"Sets permissions experiment. Experiments can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_experiment_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set experiment permissions. — set_experiment_permissions","text":"","code":"set_experiment_permissions(client, experiment_id, access_control_list = NULL)  experimentsSetPermissions(client, experiment_id, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_experiment_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set experiment permissions. — set_experiment_permissions","text":"client Required. Instance DatabricksClient() experiment_id Required. experiment get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_experiment_tag.html","id":null,"dir":"Reference","previous_headings":"","what":"Set a tag. — set_experiment_tag","title":"Set a tag. — set_experiment_tag","text":"Sets tag experiment. Experiment tags metadata can updated. Sets tag run. Tags run metadata can updated run run completes.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_experiment_tag.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set a tag. — set_experiment_tag","text":"","code":"set_experiment_tag(client, key, value, run_id = NULL, run_uuid = NULL)  experimentsSetExperimentTag(client, experiment_id, key, value)  set_experiment_tag(client, key, value, run_id = NULL, run_uuid = NULL)  experimentsSetTag(client, key, value, run_id = NULL, run_uuid = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_experiment_tag.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set a tag. — set_experiment_tag","text":"client Required. Instance DatabricksClient() key Required. Name tag. value Required. String value tag logged. run_id ID run log tag. run_uuid Deprecated, use run_id instead. ID run log tag. experiment_id Required. ID experiment log tag.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_instance_pool_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Set instance pool permissions. — set_instance_pool_permissions","title":"Set instance pool permissions. — set_instance_pool_permissions","text":"Sets permissions instance pool. Instance pools can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_instance_pool_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set instance pool permissions. — set_instance_pool_permissions","text":"","code":"set_instance_pool_permissions(   client,   instance_pool_id,   access_control_list = NULL )  instancePoolsSetPermissions(   client,   instance_pool_id,   access_control_list = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_instance_pool_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set instance pool permissions. — set_instance_pool_permissions","text":"client Required. Instance DatabricksClient() instance_pool_id Required. instance pool get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_job_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Set job permissions. — set_job_permissions","title":"Set job permissions. — set_job_permissions","text":"Sets permissions job. Jobs can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_job_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set job permissions. — set_job_permissions","text":"","code":"set_job_permissions(client, job_id, access_control_list = NULL)  jobsSetPermissions(client, job_id, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_job_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set job permissions. — set_job_permissions","text":"client Required. Instance DatabricksClient() job_id Required. job get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_model_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Set registered model permissions. — set_model_permissions","title":"Set registered model permissions. — set_model_permissions","text":"Sets permissions registered model. Registered models can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_model_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set registered model permissions. — set_model_permissions","text":"","code":"set_model_permissions(client, registered_model_id, access_control_list = NULL)  modelRegistrySetPermissions(   client,   registered_model_id,   access_control_list = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_model_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set registered model permissions. — set_model_permissions","text":"client Required. Instance DatabricksClient() registered_model_id Required. registered model get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_model_tag.html","id":null,"dir":"Reference","previous_headings":"","what":"Set a tag. — set_model_tag","title":"Set a tag. — set_model_tag","text":"Sets tag registered model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_model_tag.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set a tag. — set_model_tag","text":"","code":"set_model_tag(client, name, key, value)  modelRegistrySetModelTag(client, name, key, value)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_model_tag.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set a tag. — set_model_tag","text":"client Required. Instance DatabricksClient() name Required. Unique name model. key Required. Name tag. value Required. String value tag logged.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_model_version_tag.html","id":null,"dir":"Reference","previous_headings":"","what":"Set a version tag. — set_model_version_tag","title":"Set a version tag. — set_model_version_tag","text":"Sets model version tag.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_model_version_tag.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set a version tag. — set_model_version_tag","text":"","code":"set_model_version_tag(client, name, version, key, value)  modelRegistrySetModelVersionTag(client, name, version, key, value)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_model_version_tag.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set a version tag. — set_model_version_tag","text":"client Required. Instance DatabricksClient() name Required. Unique name model. version Required. Model version number. key Required. Name tag. value Required. String value tag logged.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_notebook_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Set workspace object permissions. — set_notebook_permissions","title":"Set workspace object permissions. — set_notebook_permissions","text":"Sets permissions workspace object. Workspace objects can inherit permissions parent objects root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_notebook_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set workspace object permissions. — set_notebook_permissions","text":"","code":"set_notebook_permissions(   client,   workspace_object_type,   workspace_object_id,   access_control_list = NULL )  workspaceSetPermissions(   client,   workspace_object_type,   workspace_object_id,   access_control_list = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_notebook_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set workspace object permissions. — set_notebook_permissions","text":"client Required. Instance DatabricksClient() workspace_object_type Required. workspace object type get manage permissions. workspace_object_id Required. workspace object get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_permission.html","id":null,"dir":"Reference","previous_headings":"","what":"Set object permissions. — set_permission","title":"Set object permissions. — set_permission","text":"Sets permissions object. Objects can inherit permissions parent objects root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_permission.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set object permissions. — set_permission","text":"","code":"set_permission(   client,   request_object_type,   request_object_id,   access_control_list = NULL )  permissionsSet(   client,   request_object_type,   request_object_id,   access_control_list = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_permission.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set object permissions. — set_permission","text":"client Required. Instance DatabricksClient() request_object_type Required. type request object. request_object_id Required. id request object. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_pipeline_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Set pipeline permissions. — set_pipeline_permissions","title":"Set pipeline permissions. — set_pipeline_permissions","text":"Sets permissions pipeline. Pipelines can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_pipeline_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set pipeline permissions. — set_pipeline_permissions","text":"","code":"set_pipeline_permissions(client, pipeline_id, access_control_list = NULL)  pipelinesSetPermissions(client, pipeline_id, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_pipeline_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set pipeline permissions. — set_pipeline_permissions","text":"client Required. Instance DatabricksClient() pipeline_id Required. pipeline get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_registered_model_alias.html","id":null,"dir":"Reference","previous_headings":"","what":"Set a Registered Model Alias. — set_registered_model_alias","title":"Set a Registered Model Alias. — set_registered_model_alias","text":"Set alias specified registered model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_registered_model_alias.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set a Registered Model Alias. — set_registered_model_alias","text":"","code":"set_registered_model_alias(client, full_name, alias, version_num)  registeredModelsSetAlias(client, full_name, alias, version_num)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_registered_model_alias.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set a Registered Model Alias. — set_registered_model_alias","text":"client Required. Instance DatabricksClient() full_name Required. Full name registered model. alias Required. name alias. version_num Required. version number model version alias points.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_registered_model_alias.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set a Registered Model Alias. — set_registered_model_alias","text":"caller must metastore admin owner registered model. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_repo_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Set repo permissions. — set_repo_permissions","title":"Set repo permissions. — set_repo_permissions","text":"Sets permissions repo. Repos can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_repo_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set repo permissions. — set_repo_permissions","text":"","code":"set_repo_permissions(client, repo_id, access_control_list = NULL)  reposSetPermissions(client, repo_id, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_repo_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set repo permissions. — set_repo_permissions","text":"client Required. Instance DatabricksClient() repo_id Required. repo get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_serving_endpoint_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Set serving endpoint permissions. — set_serving_endpoint_permissions","title":"Set serving endpoint permissions. — set_serving_endpoint_permissions","text":"Sets permissions serving endpoint. Serving endpoints can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_serving_endpoint_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set serving endpoint permissions. — set_serving_endpoint_permissions","text":"","code":"set_serving_endpoint_permissions(   client,   serving_endpoint_id,   access_control_list = NULL )  servingEndpointsSetPermissions(   client,   serving_endpoint_id,   access_control_list = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_serving_endpoint_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set serving endpoint permissions. — set_serving_endpoint_permissions","text":"client Required. Instance DatabricksClient() serving_endpoint_id Required. serving endpoint get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_token_management_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Set token permissions. — set_token_management_permissions","title":"Set token permissions. — set_token_management_permissions","text":"Sets permissions tokens. Tokens can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_token_management_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set token permissions. — set_token_management_permissions","text":"","code":"set_token_management_permissions(client, access_control_list = NULL)  tokenManagementSetPermissions(client, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_token_management_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set token permissions. — set_token_management_permissions","text":"client Required. Instance DatabricksClient() access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_user_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Set password permissions. — set_user_permissions","title":"Set password permissions. — set_user_permissions","text":"Sets permissions passwords. Passwords can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_user_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set password permissions. — set_user_permissions","text":"","code":"set_user_permissions(client, access_control_list = NULL)  usersSetPermissions(client, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_user_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set password permissions. — set_user_permissions","text":"client Required. Instance DatabricksClient() access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_warehouse_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Set SQL warehouse permissions. — set_warehouse_permissions","title":"Set SQL warehouse permissions. — set_warehouse_permissions","text":"Sets permissions SQL warehouse. SQL warehouses can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_warehouse_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set SQL warehouse permissions. — set_warehouse_permissions","text":"","code":"set_warehouse_permissions(client, warehouse_id, access_control_list = NULL)  warehousesSetPermissions(client, warehouse_id, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_warehouse_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set SQL warehouse permissions. — set_warehouse_permissions","text":"client Required. Instance DatabricksClient() warehouse_id Required. SQL warehouse get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_warehouse_workspace_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the workspace configuration. — set_warehouse_workspace_config","title":"Set the workspace configuration. — set_warehouse_workspace_config","text":"Sets workspace level configuration shared SQL warehouses workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_warehouse_workspace_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the workspace configuration. — set_warehouse_workspace_config","text":"","code":"set_warehouse_workspace_config(   client,   channel = NULL,   config_param = NULL,   data_access_config = NULL,   enabled_warehouse_types = NULL,   global_param = NULL,   google_service_account = NULL,   instance_profile_arn = NULL,   security_policy = NULL,   sql_configuration_parameters = NULL )  warehousesSetWorkspaceWarehouseConfig(   client,   channel = NULL,   config_param = NULL,   data_access_config = NULL,   enabled_warehouse_types = NULL,   global_param = NULL,   google_service_account = NULL,   instance_profile_arn = NULL,   security_policy = NULL,   sql_configuration_parameters = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_warehouse_workspace_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the workspace configuration. — set_warehouse_workspace_config","text":"client Required. Instance DatabricksClient() channel Optional: Channel selection details. config_param Deprecated: Use sql_configuration_parameters. data_access_config Spark confs external hive metastore configuration JSON serialized size must less <= 512K. enabled_warehouse_types List Warehouse Types allowed workspace (limits allowed value type field CreateWarehouse EditWarehouse). global_param Deprecated: Use sql_configuration_parameters. google_service_account GCP : Google Service Account used pass cluster access Google Cloud Storage. instance_profile_arn AWS : Instance profile used pass IAM role cluster. security_policy Security policy warehouses. sql_configuration_parameters SQL configuration parameters.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_workspace_conf_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Enable/disable features. — set_workspace_conf_status","title":"Enable/disable features. — set_workspace_conf_status","text":"Sets configuration status workspace, including enabling disabling .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_workspace_conf_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enable/disable features. — set_workspace_conf_status","text":"","code":"set_workspace_conf_status(client)  workspaceConfSetStatus(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/set_workspace_conf_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Enable/disable features. — set_workspace_conf_status","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/share_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get permissions. — share_permissions","title":"Get permissions. — share_permissions","text":"Gets permissions data share metastore. caller must metastore admin owner share.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/share_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get permissions. — share_permissions","text":"","code":"share_permissions(client, name)  sharesSharePermissions(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/share_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get permissions. — share_permissions","text":"client Required. Instance DatabricksClient() name Required. name share.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/share_recipient_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Get recipient share permissions. — share_recipient_permissions","title":"Get recipient share permissions. — share_recipient_permissions","text":"Gets share permissions specified Recipient. caller must metastore admin owner Recipient.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/share_recipient_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get recipient share permissions. — share_recipient_permissions","text":"","code":"share_recipient_permissions(client, name)  recipientsSharePermissions(client, name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/share_recipient_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get recipient share permissions. — share_recipient_permissions","text":"client Required. Instance DatabricksClient() name Required. name Recipient.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/spark_cluster_versions.html","id":null,"dir":"Reference","previous_headings":"","what":"List available Spark versions. — spark_cluster_versions","title":"List available Spark versions. — spark_cluster_versions","text":"Returns list available Spark versions. versions can used launch cluster.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/spark_cluster_versions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List available Spark versions. — spark_cluster_versions","text":"","code":"spark_cluster_versions(client)  clustersSparkVersions(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/spark_cluster_versions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List available Spark versions. — spark_cluster_versions","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Start terminated cluster. — start_cluster","title":"Start terminated cluster. — start_cluster","text":"Starts terminated Spark cluster supplied ID. works similar createCluster except:","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Start terminated cluster. — start_cluster","text":"","code":"start_cluster(client, cluster_id)  clustersStart(client, cluster_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Start terminated cluster. — start_cluster","text":"client Required. Instance DatabricksClient() cluster_id Required. cluster started.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_cluster.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Start terminated cluster. — start_cluster","text":"previous cluster id attributes preserved. * cluster starts last specified cluster size. * previous cluster autoscaling cluster, current cluster starts minimum number nodes. * cluster currently TERMINATED state, nothing happen. * Clusters launched run job started.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_cluster_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Start terminated cluster. — start_cluster_and_wait","title":"Start terminated cluster. — start_cluster_and_wait","text":"long-running operation, blocks Clusters Databricks reach RUNNING state timeout 20 minutes, can change via timeout parameter. default, state Databricks Clusters reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_cluster_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Start terminated cluster. — start_cluster_and_wait","text":"","code":"start_cluster_and_wait(   client,   cluster_id,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_cluster_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Start terminated cluster. — start_cluster_and_wait","text":"client Required. Instance DatabricksClient() cluster_id Required. cluster started. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_cluster_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Start terminated cluster. — start_cluster_and_wait","text":"Starts terminated Spark cluster supplied ID. works similar createCluster except: previous cluster id attributes preserved. * cluster starts last specified cluster size. * previous cluster autoscaling cluster, current cluster starts minimum number nodes. * cluster currently TERMINATED state, nothing happen. * Clusters launched run job started.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_pipeline_update.html","id":null,"dir":"Reference","previous_headings":"","what":"Start a pipeline. — start_pipeline_update","title":"Start a pipeline. — start_pipeline_update","text":"Starts new update pipeline. already active update pipeline, request fail active update remain running.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_pipeline_update.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Start a pipeline. — start_pipeline_update","text":"","code":"start_pipeline_update(   client,   pipeline_id,   cause = NULL,   full_refresh = NULL,   full_refresh_selection = NULL,   refresh_selection = NULL,   validate_only = NULL )  pipelinesStartUpdate(   client,   pipeline_id,   cause = NULL,   full_refresh = NULL,   full_refresh_selection = NULL,   refresh_selection = NULL,   validate_only = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_pipeline_update.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Start a pipeline. — start_pipeline_update","text":"client Required. Instance DatabricksClient() pipeline_id Required. field description yet. cause field description yet. full_refresh true, update reset tables running. full_refresh_selection list tables update fullRefresh. refresh_selection list tables update without fullRefresh. validate_only true, update validates correctness pipeline source code materialize publish datasets.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_warehouse.html","id":null,"dir":"Reference","previous_headings":"","what":"Start a warehouse. — start_warehouse","title":"Start a warehouse. — start_warehouse","text":"Starts SQL warehouse.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_warehouse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Start a warehouse. — start_warehouse","text":"","code":"start_warehouse(client, id)  warehousesStart(client, id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_warehouse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Start a warehouse. — start_warehouse","text":"client Required. Instance DatabricksClient() id Required. Required.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_warehouse_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Start a warehouse. — start_warehouse_and_wait","title":"Start a warehouse. — start_warehouse_and_wait","text":"long-running operation, blocks Warehouses Databricks reach RUNNING state timeout 20 minutes, can change via timeout parameter. default, state Databricks Warehouses reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_warehouse_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Start a warehouse. — start_warehouse_and_wait","text":"","code":"start_warehouse_and_wait(client, id, timeout = 20, callback = cli_reporter)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_warehouse_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Start a warehouse. — start_warehouse_and_wait","text":"client Required. Instance DatabricksClient() id Required. Required. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/start_warehouse_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Start a warehouse. — start_warehouse_and_wait","text":"Starts SQL warehouse.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/stop_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Stop a pipeline. — stop_pipeline","title":"Stop a pipeline. — stop_pipeline","text":"Stops pipeline canceling active update. active update pipeline, request -op.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/stop_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stop a pipeline. — stop_pipeline","text":"","code":"stop_pipeline(client, pipeline_id)  pipelinesStop(client, pipeline_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/stop_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stop a pipeline. — stop_pipeline","text":"client Required. Instance DatabricksClient() pipeline_id Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/stop_pipeline_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Stop a pipeline. — stop_pipeline_and_wait","title":"Stop a pipeline. — stop_pipeline_and_wait","text":"long-running operation, blocks Pipelines Databricks reach IDLE state timeout 20 minutes, can change via timeout parameter. default, state Databricks Pipelines reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/stop_pipeline_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stop a pipeline. — stop_pipeline_and_wait","text":"","code":"stop_pipeline_and_wait(   client,   pipeline_id,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/stop_pipeline_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stop a pipeline. — stop_pipeline_and_wait","text":"client Required. Instance DatabricksClient() pipeline_id Required. field description yet. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/stop_pipeline_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Stop a pipeline. — stop_pipeline_and_wait","text":"Stops pipeline canceling active update. active update pipeline, request -op.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/stop_warehouse.html","id":null,"dir":"Reference","previous_headings":"","what":"Stop a warehouse. — stop_warehouse","title":"Stop a warehouse. — stop_warehouse","text":"Stops SQL warehouse.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/stop_warehouse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stop a warehouse. — stop_warehouse","text":"","code":"stop_warehouse(client, id)  warehousesStop(client, id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/stop_warehouse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stop a warehouse. — stop_warehouse","text":"client Required. Instance DatabricksClient() id Required. Required.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/stop_warehouse_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Stop a warehouse. — stop_warehouse_and_wait","title":"Stop a warehouse. — stop_warehouse_and_wait","text":"long-running operation, blocks Warehouses Databricks reach STOPPED state timeout 20 minutes, can change via timeout parameter. default, state Databricks Warehouses reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/stop_warehouse_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stop a warehouse. — stop_warehouse_and_wait","text":"","code":"stop_warehouse_and_wait(client, id, timeout = 20, callback = cli_reporter)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/stop_warehouse_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stop a warehouse. — stop_warehouse_and_wait","text":"client Required. Instance DatabricksClient() id Required. Required. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/stop_warehouse_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Stop a warehouse. — stop_warehouse_and_wait","text":"Stops SQL warehouse.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/submit_job.html","id":null,"dir":"Reference","previous_headings":"","what":"Create and trigger a one-time run. — submit_job","title":"Create and trigger a one-time run. — submit_job","text":"Submit one-time run. endpoint allows submit workload directly without creating job. Runs submitted using endpoint don’t display UI. Use jobs/runs/get API check run state job submitted.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/submit_job.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create and trigger a one-time run. — submit_job","text":"","code":"submit_job(   client,   access_control_list = NULL,   email_notifications = NULL,   git_source = NULL,   health = NULL,   idempotency_token = NULL,   notification_settings = NULL,   queue = NULL,   run_name = NULL,   tasks = NULL,   timeout_seconds = NULL,   webhook_notifications = NULL )  jobsSubmit(   client,   access_control_list = NULL,   email_notifications = NULL,   git_source = NULL,   health = NULL,   idempotency_token = NULL,   notification_settings = NULL,   queue = NULL,   run_name = NULL,   tasks = NULL,   timeout_seconds = NULL,   webhook_notifications = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/submit_job.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create and trigger a one-time run. — submit_job","text":"client Required. Instance DatabricksClient() access_control_list List permissions set job. email_notifications optional set email addresses notified run begins completes. git_source optional specification remote Git repository containing source code used tasks. health optional set health rules can defined job. idempotency_token optional token can used guarantee idempotency job run requests. notification_settings Optional notification settings used sending notifications email_notifications webhook_notifications run. queue queue settings one-time run. run_name optional name run. tasks field description yet. timeout_seconds optional timeout applied run job. webhook_notifications collection system notification IDs notify run begins completes.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/submit_job_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Create and trigger a one-time run. — submit_job_and_wait","title":"Create and trigger a one-time run. — submit_job_and_wait","text":"long-running operation, blocks Jobs Databricks reach TERMINATED SKIPPED state timeout 20 minutes, can change via timeout parameter. default, state Databricks Jobs reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/submit_job_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create and trigger a one-time run. — submit_job_and_wait","text":"","code":"submit_job_and_wait(   client,   access_control_list = NULL,   email_notifications = NULL,   git_source = NULL,   health = NULL,   idempotency_token = NULL,   notification_settings = NULL,   queue = NULL,   run_name = NULL,   tasks = NULL,   timeout_seconds = NULL,   webhook_notifications = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/submit_job_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create and trigger a one-time run. — submit_job_and_wait","text":"client Required. Instance DatabricksClient() access_control_list List permissions set job. email_notifications optional set email addresses notified run begins completes. git_source optional specification remote Git repository containing source code used tasks. health optional set health rules can defined job. idempotency_token optional token can used guarantee idempotency job run requests. notification_settings Optional notification settings used sending notifications email_notifications webhook_notifications run. queue queue settings one-time run. run_name optional name run. tasks field description yet. timeout_seconds optional timeout applied run job. webhook_notifications collection system notification IDs notify run begins completes. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/submit_job_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create and trigger a one-time run. — submit_job_and_wait","text":"Submit one-time run. endpoint allows submit workload directly without creating job. Runs submitted using endpoint don’t display UI. Use jobs/runs/get API check run state job submitted.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/summary_metastore.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a metastore summary. — summary_metastore","title":"Get a metastore summary. — summary_metastore","text":"Gets information metastore. summary includes storage credential, cloud vendor, cloud region, global metastore ID.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/summary_metastore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a metastore summary. — summary_metastore","text":"","code":"summary_metastore(client)  metastoresSummary(client)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/summary_metastore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a metastore summary. — summary_metastore","text":"client Required. Instance DatabricksClient()","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/sync_vector_search_index.html","id":null,"dir":"Reference","previous_headings":"","what":"Synchronize an index. — sync_vector_search_index","title":"Synchronize an index. — sync_vector_search_index","text":"Triggers synchronization process specified vector index.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/sync_vector_search_index.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Synchronize an index. — sync_vector_search_index","text":"","code":"sync_vector_search_index(client, index_name)  vectorSearchIndexesSyncIndex(client, index_name)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/sync_vector_search_index.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Synchronize an index. — sync_vector_search_index","text":"client Required. Instance DatabricksClient() index_name Required. Name vector index synchronize.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/test_model_registry_webhook.html","id":null,"dir":"Reference","previous_headings":"","what":"Test a webhook. — test_model_registry_webhook","title":"Test a webhook. — test_model_registry_webhook","text":"NOTE: endpoint Public Preview.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/test_model_registry_webhook.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test a webhook. — test_model_registry_webhook","text":"","code":"test_model_registry_webhook(client, id, event = NULL)  modelRegistryTestRegistryWebhook(client, id, event = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/test_model_registry_webhook.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test a webhook. — test_model_registry_webhook","text":"client Required. Instance DatabricksClient() id Required. Webhook ID. event event specified, test trigger uses specified event.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/test_model_registry_webhook.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Test a webhook. — test_model_registry_webhook","text":"Tests registry webhook.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/transfer_dbsql_permission_ownership.html","id":null,"dir":"Reference","previous_headings":"","what":"Transfer object ownership. — transfer_dbsql_permission_ownership","title":"Transfer object ownership. — transfer_dbsql_permission_ownership","text":"Transfers ownership dashboard, query, alert active user. Requires admin API key.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/transfer_dbsql_permission_ownership.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transfer object ownership. — transfer_dbsql_permission_ownership","text":"","code":"transfer_dbsql_permission_ownership(   client,   object_type,   object_id,   new_owner = NULL )  dbsqlPermissionsTransferOwnership(   client,   object_type,   object_id,   new_owner = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/transfer_dbsql_permission_ownership.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transfer object ownership. — transfer_dbsql_permission_ownership","text":"client Required. Instance DatabricksClient() object_type Required. type object change ownership. object_id Required. ID object change ownership. new_owner Email address new owner, must exist workspace.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/transition_model_stage.html","id":null,"dir":"Reference","previous_headings":"","what":"Transition a stage. — transition_model_stage","title":"Transition a stage. — transition_model_stage","text":"Transition model version's stage. Databricks workspace version MLflow endpoint also accepts comment associated transition recorded.',","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/transition_model_stage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transition a stage. — transition_model_stage","text":"","code":"transition_model_stage(   client,   name,   version,   stage,   archive_existing_versions,   comment = NULL )  modelRegistryTransitionStage(   client,   name,   version,   stage,   archive_existing_versions,   comment = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/transition_model_stage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transition a stage. — transition_model_stage","text":"client Required. Instance DatabricksClient() name Required. Name model. version Required. Version model. stage Required. Target stage transition. archive_existing_versions Required. Specifies whether archive current model versions target stage. comment User-provided comment action.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/transition_model_stage.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Transition a stage. — transition_model_stage","text":"MLflow endpoint: https://www.mlflow.org/docs/latest/rest-api.html#transition-modelversion-stage","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/trash_lakeview.html","id":null,"dir":"Reference","previous_headings":"","what":"Trash dashboard. — trash_lakeview","title":"Trash dashboard. — trash_lakeview","text":"Trash dashboard.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/trash_lakeview.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trash dashboard. — trash_lakeview","text":"","code":"trash_lakeview(client, dashboard_id)  lakeviewTrash(client, dashboard_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/trash_lakeview.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trash dashboard. — trash_lakeview","text":"client Required. Instance DatabricksClient() dashboard_id Required. UUID identifying dashboard.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/unassign_metastore.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete an assignment. — unassign_metastore","title":"Delete an assignment. — unassign_metastore","text":"Deletes metastore assignment. caller must account administrator.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/unassign_metastore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete an assignment. — unassign_metastore","text":"","code":"unassign_metastore(client, workspace_id, metastore_id)  metastoresUnassign(client, workspace_id, metastore_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/unassign_metastore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete an assignment. — unassign_metastore","text":"client Required. Instance DatabricksClient() workspace_id Required. workspace ID. metastore_id Required. Query ID metastore delete.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/uninstall_cluster_library.html","id":null,"dir":"Reference","previous_headings":"","what":"Uninstall libraries. — uninstall_cluster_library","title":"Uninstall libraries. — uninstall_cluster_library","text":"Set libraries uninstalled cluster. libraries uninstalled cluster restarted. Uninstalling libraries installed cluster impact error.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/uninstall_cluster_library.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Uninstall libraries. — uninstall_cluster_library","text":"","code":"uninstall_cluster_library(client, cluster_id, libraries)  librariesUninstall(client, cluster_id, libraries)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/uninstall_cluster_library.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Uninstall libraries. — uninstall_cluster_library","text":"client Required. Instance DatabricksClient() cluster_id Required. Unique identifier cluster uninstall libraries. libraries Required. libraries uninstall.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/unpin_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Unpin cluster. — unpin_cluster","title":"Unpin cluster. — unpin_cluster","text":"Unpinning cluster allow cluster eventually removed ListClusters API. Unpinning cluster pinned effect. API can called workspace admins.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/unpin_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unpin cluster. — unpin_cluster","text":"","code":"unpin_cluster(client, cluster_id)  clustersUnpin(client, cluster_id)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/unpin_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unpin cluster. — unpin_cluster","text":"client Required. Instance DatabricksClient() cluster_id Required. needs content added.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_account_access_control_proxy_rule_set.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a rule set. — update_account_access_control_proxy_rule_set","title":"Update a rule set. — update_account_access_control_proxy_rule_set","text":"Replace rules rule set. First, use GET rule set request read current version rule set modifying . pattern helps prevent conflicts concurrent updates.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_account_access_control_proxy_rule_set.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a rule set. — update_account_access_control_proxy_rule_set","text":"","code":"update_account_access_control_proxy_rule_set(client, name, rule_set)  accountAccessControlProxyUpdateRuleSet(client, name, rule_set)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_account_access_control_proxy_rule_set.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a rule set. — update_account_access_control_proxy_rule_set","text":"client Required. Instance DatabricksClient() name Required. Name rule set. rule_set Required. field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_alert.html","id":null,"dir":"Reference","previous_headings":"","what":"Update an alert. — update_alert","title":"Update an alert. — update_alert","text":"Updates alert.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_alert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update an alert. — update_alert","text":"","code":"update_alert(client, alert_id, name, options, query_id, rearm = NULL)  alertsUpdate(client, alert_id, name, options, query_id, rearm = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_alert.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update an alert. — update_alert","text":"client Required. Instance DatabricksClient() alert_id Required. field description yet. name Required. Name alert. options Required. Alert configuration options. query_id Required. Query ID. rearm Number seconds triggered alert rearms can triggered .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_artifact_allowlist.html","id":null,"dir":"Reference","previous_headings":"","what":"Set an artifact allowlist. — update_artifact_allowlist","title":"Set an artifact allowlist. — update_artifact_allowlist","text":"Set artifact allowlist certain artifact type. whole artifact allowlist replaced new allowlist. caller must metastore admin MANAGE ALLOWLIST privilege metastore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_artifact_allowlist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set an artifact allowlist. — update_artifact_allowlist","text":"","code":"update_artifact_allowlist(client, artifact_type, artifact_matchers)  artifactAllowlistsUpdate(client, artifact_type, artifact_matchers)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_artifact_allowlist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set an artifact allowlist. — update_artifact_allowlist","text":"client Required. Instance DatabricksClient() artifact_type Required. artifact type allowlist. artifact_matchers Required. list allowed artifact match patterns.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_automatic_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Update the automatic cluster update setting. — update_automatic_cluster","title":"Update the automatic cluster update setting. — update_automatic_cluster","text":"Updates automatic cluster update setting workspace. fresh etag needs provided PATCH requests (part setting field). etag can retrieved making GET request PATCH request. setting updated concurrently, PATCH fails 409 request must retried using fresh etag 409 response.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_automatic_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update the automatic cluster update setting. — update_automatic_cluster","text":"","code":"update_automatic_cluster(client, allow_missing, setting, field_mask)  automaticClusterUpdateUpdate(client, allow_missing, setting, field_mask)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_automatic_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update the automatic cluster update setting. — update_automatic_cluster","text":"client Required. Instance DatabricksClient() allow_missing Required. always set true Settings API. setting Required. field description yet. field_mask Required. Field mask required passed PATCH request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_catalog.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a catalog. — update_catalog","title":"Update a catalog. — update_catalog","text":"Updates catalog matches supplied name. caller must either owner catalog, metastore admin (changing owner field catalog).","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_catalog.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a catalog. — update_catalog","text":"","code":"update_catalog(   client,   name,   comment = NULL,   enable_predictive_optimization = NULL,   isolation_mode = NULL,   new_name = NULL,   owner = NULL,   properties = NULL )  catalogsUpdate(   client,   name,   comment = NULL,   enable_predictive_optimization = NULL,   isolation_mode = NULL,   new_name = NULL,   owner = NULL,   properties = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_catalog.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a catalog. — update_catalog","text":"client Required. Instance DatabricksClient() name Required. name catalog. comment User-provided free-form text description. enable_predictive_optimization Whether predictive optimization enabled object objects . isolation_mode Whether current securable accessible workspaces specific set workspaces. new_name New name catalog. owner Username current owner catalog. properties map key-value properties attached securable.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_clean_room.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a clean room. — update_clean_room","title":"Update a clean room. — update_clean_room","text":"Updates clean room changes data objects request. caller must owner clean room metastore admin.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_clean_room.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a clean room. — update_clean_room","text":"","code":"update_clean_room(   client,   name,   catalog_updates = NULL,   comment = NULL,   owner = NULL )  cleanRoomsUpdate(   client,   name,   catalog_updates = NULL,   comment = NULL,   owner = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_clean_room.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a clean room. — update_clean_room","text":"client Required. Instance DatabricksClient() name Required. name clean room. catalog_updates Array shared data object updates. comment User-provided free-form text description. owner Username current owner clean room.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_clean_room.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update a clean room. — update_clean_room","text":"caller metastore admin, owner field can updated. case clean room name changed updateCleanRoom requires caller clean room owner metastore admin. table added method, clean room owner must also SELECT privilege table. privilege must maintained indefinitely recipients able access table. Typically, use group clean room owner. Table removals update require additional privileges.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_cluster_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Update cluster permissions. — update_cluster_permissions","title":"Update cluster permissions. — update_cluster_permissions","text":"Updates permissions cluster. Clusters can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_cluster_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update cluster permissions. — update_cluster_permissions","text":"","code":"update_cluster_permissions(client, cluster_id, access_control_list = NULL)  clustersUpdatePermissions(client, cluster_id, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_cluster_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update cluster permissions. — update_cluster_permissions","text":"client Required. Instance DatabricksClient() cluster_id Required. cluster get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_cluster_policy_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Update cluster policy permissions. — update_cluster_policy_permissions","title":"Update cluster policy permissions. — update_cluster_policy_permissions","text":"Updates permissions cluster policy. Cluster policies can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_cluster_policy_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update cluster policy permissions. — update_cluster_policy_permissions","text":"","code":"update_cluster_policy_permissions(   client,   cluster_policy_id,   access_control_list = NULL )  clusterPoliciesUpdatePermissions(   client,   cluster_policy_id,   access_control_list = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_cluster_policy_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update cluster policy permissions. — update_cluster_policy_permissions","text":"client Required. Instance DatabricksClient() cluster_policy_id Required. cluster policy get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a connection. — update_connection","title":"Update a connection. — update_connection","text":"Updates connection matches supplied name.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a connection. — update_connection","text":"","code":"update_connection(client, name, options, new_name = NULL, owner = NULL)  connectionsUpdate(client, name, options, new_name = NULL, owner = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a connection. — update_connection","text":"client Required. Instance DatabricksClient() name Required. Name connection. options Required. map key-value properties attached securable. new_name New name connection. owner Username current owner connection.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_csp_enablement.html","id":null,"dir":"Reference","previous_headings":"","what":"Update the compliance security profile setting. — update_csp_enablement","title":"Update the compliance security profile setting. — update_csp_enablement","text":"Updates compliance security profile setting workspace. fresh etag needs provided PATCH requests (part setting field). etag can retrieved making GET request PATCH request. setting updated concurrently, PATCH fails 409 request must retried using fresh etag 409 response.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_csp_enablement.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update the compliance security profile setting. — update_csp_enablement","text":"","code":"update_csp_enablement(client, allow_missing, setting, field_mask)  cspEnablementUpdate(client, allow_missing, setting, field_mask)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_csp_enablement.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update the compliance security profile setting. — update_csp_enablement","text":"client Required. Instance DatabricksClient() allow_missing Required. always set true Settings API. setting Required. field description yet. field_mask Required. Field mask required passed PATCH request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_dashboard.html","id":null,"dir":"Reference","previous_headings":"","what":"Change a dashboard definition. — update_dashboard","title":"Change a dashboard definition. — update_dashboard","text":"Modify dashboard definition. operation affects attributes dashboard object. add, modify, remove widgets.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_dashboard.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Change a dashboard definition. — update_dashboard","text":"","code":"update_dashboard(client, dashboard_id, name = NULL, run_as_role = NULL)  dashboardsUpdate(client, dashboard_id, name = NULL, run_as_role = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_dashboard.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Change a dashboard definition. — update_dashboard","text":"client Required. Instance DatabricksClient() dashboard_id Required. field description yet. name title dashboard appears list views top dashboard page. run_as_role Sets Run role object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_dashboard.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Change a dashboard definition. — update_dashboard","text":"Note: undo operation.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_dashboard_widget.html","id":null,"dir":"Reference","previous_headings":"","what":"Update existing widget. — update_dashboard_widget","title":"Update existing widget. — update_dashboard_widget","text":"Update existing widget.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_dashboard_widget.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update existing widget. — update_dashboard_widget","text":"","code":"update_dashboard_widget(   client,   id,   dashboard_id,   options,   width,   text = NULL,   visualization_id = NULL )  dashboardWidgetsUpdate(   client,   id,   dashboard_id,   options,   width,   text = NULL,   visualization_id = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_dashboard_widget.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update existing widget. — update_dashboard_widget","text":"client Required. Instance DatabricksClient() id Required. Widget ID returned :method:dashboardwidgets/create. dashboard_id Required. Dashboard ID returned :method:dashboards/create. options Required. field description yet. width Required. Width widget. text textbox widget, application displays text. visualization_id Query Vizualization ID returned :method:queryvisualizations/create.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_default_namespace.html","id":null,"dir":"Reference","previous_headings":"","what":"Update the default namespace setting. — update_default_namespace","title":"Update the default namespace setting. — update_default_namespace","text":"Updates default namespace setting workspace. fresh etag needs provided PATCH requests (part setting field). etag can retrieved making GET request PATCH request. Note setting exist, GET returns NOT_FOUND error etag present error response, set PATCH request. setting updated concurrently, PATCH fails 409 request must retried using fresh etag 409 response.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_default_namespace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update the default namespace setting. — update_default_namespace","text":"","code":"update_default_namespace(client, allow_missing, setting, field_mask)  defaultNamespaceUpdate(client, allow_missing, setting, field_mask)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_default_namespace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update the default namespace setting. — update_default_namespace","text":"client Required. Instance DatabricksClient() allow_missing Required. always set true Settings API. setting Required. represents setting configuration default namespace Databricks workspace. field_mask Required. Field mask required passed PATCH request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_esm_enablement.html","id":null,"dir":"Reference","previous_headings":"","what":"Update the enhanced security monitoring setting. — update_esm_enablement","title":"Update the enhanced security monitoring setting. — update_esm_enablement","text":"Updates enhanced security monitoring setting workspace. fresh etag needs provided PATCH requests (part setting field). etag can retrieved making GET request PATCH request. setting updated concurrently, PATCH fails 409 request must retried using fresh etag 409 response.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_esm_enablement.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update the enhanced security monitoring setting. — update_esm_enablement","text":"","code":"update_esm_enablement(client, allow_missing, setting, field_mask)  esmEnablementUpdate(client, allow_missing, setting, field_mask)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_esm_enablement.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update the enhanced security monitoring setting. — update_esm_enablement","text":"client Required. Instance DatabricksClient() allow_missing Required. always set true Settings API. setting Required. field description yet. field_mask Required. Field mask required passed PATCH request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_experiment.html","id":null,"dir":"Reference","previous_headings":"","what":"Update an experiment. — update_experiment","title":"Update an experiment. — update_experiment","text":"Updates experiment metadata.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_experiment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update an experiment. — update_experiment","text":"","code":"update_experiment(client, experiment_id, new_name = NULL)  experimentsUpdateExperiment(client, experiment_id, new_name = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_experiment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update an experiment. — update_experiment","text":"client Required. Instance DatabricksClient() experiment_id Required. ID associated experiment. new_name provided, experiment's name changed new name.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_experiment_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Update experiment permissions. — update_experiment_permissions","title":"Update experiment permissions. — update_experiment_permissions","text":"Updates permissions experiment. Experiments can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_experiment_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update experiment permissions. — update_experiment_permissions","text":"","code":"update_experiment_permissions(   client,   experiment_id,   access_control_list = NULL )  experimentsUpdatePermissions(client, experiment_id, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_experiment_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update experiment permissions. — update_experiment_permissions","text":"client Required. Instance DatabricksClient() experiment_id Required. experiment get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_experiment_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a run. — update_experiment_run","title":"Update a run. — update_experiment_run","text":"Updates run metadata.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_experiment_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a run. — update_experiment_run","text":"","code":"update_experiment_run(   client,   end_time = NULL,   run_id = NULL,   run_uuid = NULL,   status = NULL )  experimentsUpdateRun(   client,   end_time = NULL,   run_id = NULL,   run_uuid = NULL,   status = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_experiment_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a run. — update_experiment_run","text":"client Required. Instance DatabricksClient() end_time Unix timestamp milliseconds run ended. run_id ID run update. run_uuid Deprecated, use run_id instead. ID run update. status Updated status run.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_external_location.html","id":null,"dir":"Reference","previous_headings":"","what":"Update an external location. — update_external_location","title":"Update an external location. — update_external_location","text":"Updates external location metastore. caller must owner external location, metastore admin. second case, admin can update name external location.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_external_location.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update an external location. — update_external_location","text":"","code":"update_external_location(   client,   name,   access_point = NULL,   comment = NULL,   credential_name = NULL,   encryption_details = NULL,   force = NULL,   new_name = NULL,   owner = NULL,   read_only = NULL,   skip_validation = NULL,   url = NULL )  externalLocationsUpdate(   client,   name,   access_point = NULL,   comment = NULL,   credential_name = NULL,   encryption_details = NULL,   force = NULL,   new_name = NULL,   owner = NULL,   read_only = NULL,   skip_validation = NULL,   url = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_external_location.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update an external location. — update_external_location","text":"client Required. Instance DatabricksClient() name Required. Name external location. access_point AWS access point use accesing s3 external location. comment User-provided free-form text description. credential_name Name storage credential used location. encryption_details Encryption options apply clients connecting cloud storage. force Force update even changing url invalidates dependent external tables mounts. new_name New name external location. owner owner external location. read_only Indicates whether external location read-. skip_validation Skips validation storage credential associated external location. url Path URL external location.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a function. — update_function","title":"Update a function. — update_function","text":"Updates function matches supplied name. owner function can updated. user metastore admin, user must member group new function owner. - metastore admin - owner function's parent catalog - owner function's parent schema USE_CATALOG privilege parent catalog - owner function USE_CATALOG privilege parent catalog well USE_SCHEMA privilege function's parent schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a function. — update_function","text":"","code":"update_function(client, name, owner = NULL)  functionsUpdate(client, name, owner = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a function. — update_function","text":"client Required. Instance DatabricksClient() name Required. fully-qualified name function (form catalog_name.schema_name.function__name). owner Username current owner function.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_git_credential.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a credential. — update_git_credential","title":"Update a credential. — update_git_credential","text":"Updates specified Git credential.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_git_credential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a credential. — update_git_credential","text":"","code":"update_git_credential(   client,   credential_id,   git_provider = NULL,   git_username = NULL,   personal_access_token = NULL )  gitCredentialsUpdate(   client,   credential_id,   git_provider = NULL,   git_username = NULL,   personal_access_token = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_git_credential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a credential. — update_git_credential","text":"client Required. Instance DatabricksClient() credential_id Required. ID corresponding credential access. git_provider Git provider. git_username Git username. personal_access_token personal access token used authenticate corresponding Git provider.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_global_init_script.html","id":null,"dir":"Reference","previous_headings":"","what":"Update init script. — update_global_init_script","title":"Update init script. — update_global_init_script","text":"Updates global init script, specifying fields change. fields optional. Unspecified fields retain current value.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_global_init_script.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update init script. — update_global_init_script","text":"","code":"update_global_init_script(   client,   script_id,   name,   script,   enabled = NULL,   position = NULL )  globalInitScriptsUpdate(   client,   script_id,   name,   script,   enabled = NULL,   position = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_global_init_script.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update init script. — update_global_init_script","text":"client Required. Instance DatabricksClient() script_id Required. ID global init script. name Required. name script. script Required. Base64-encoded content script. enabled Specifies whether script enabled. position position script, 0 represents first script run, 1 second script run, ascending order.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_grant.html","id":null,"dir":"Reference","previous_headings":"","what":"Update permissions. — update_grant","title":"Update permissions. — update_grant","text":"Updates permissions securable.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_grant.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update permissions. — update_grant","text":"","code":"update_grant(client, securable_type, full_name, changes = NULL)  grantsUpdate(client, securable_type, full_name, changes = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_grant.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update permissions. — update_grant","text":"client Required. Instance DatabricksClient() securable_type Required. Type securable. full_name Required. Full name securable. changes Array permissions change objects.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace a group. — update_group","title":"Replace a group. — update_group","text":"Updates details group replacing entire group entity.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace a group. — update_group","text":"","code":"update_group(   client,   id,   display_name = NULL,   entitlements = NULL,   external_id = NULL,   groups = NULL,   members = NULL,   meta = NULL,   roles = NULL,   schemas = NULL )  groupsUpdate(   client,   id,   display_name = NULL,   entitlements = NULL,   external_id = NULL,   groups = NULL,   members = NULL,   meta = NULL,   roles = NULL,   schemas = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace a group. — update_group","text":"client Required. Instance DatabricksClient() id Databricks group ID. display_name String represents human-readable group name. entitlements Entitlements assigned group. external_id field description yet. groups field description yet. members field description yet. meta Container group identifier. roles Corresponds AWS instance profile/arn role. schemas schema group.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_instance_pool_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Update instance pool permissions. — update_instance_pool_permissions","title":"Update instance pool permissions. — update_instance_pool_permissions","text":"Updates permissions instance pool. Instance pools can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_instance_pool_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update instance pool permissions. — update_instance_pool_permissions","text":"","code":"update_instance_pool_permissions(   client,   instance_pool_id,   access_control_list = NULL )  instancePoolsUpdatePermissions(   client,   instance_pool_id,   access_control_list = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_instance_pool_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update instance pool permissions. — update_instance_pool_permissions","text":"client Required. Instance DatabricksClient() instance_pool_id Required. instance pool get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_ip_access_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Update access list. — update_ip_access_list","title":"Update access list. — update_ip_access_list","text":"Updates existing IP access list, specified ID.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_ip_access_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update access list. — update_ip_access_list","text":"","code":"update_ip_access_list(   client,   ip_access_list_id,   enabled = NULL,   ip_addresses = NULL,   label = NULL,   list_type = NULL )  ipAccessListsUpdate(   client,   ip_access_list_id,   enabled = NULL,   ip_addresses = NULL,   label = NULL,   list_type = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_ip_access_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update access list. — update_ip_access_list","text":"client Required. Instance DatabricksClient() ip_access_list_id Required. ID corresponding IP access list. enabled Specifies whether IP access list enabled. ip_addresses field description yet. label Label IP access list. list_type Type IP access list.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_ip_access_list.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update access list. — update_ip_access_list","text":"list can include allow lists block lists. See top file description server treats allow lists block lists run time. updating IP access list: allow lists block lists combined, API supports maximum 1000 IP/CIDR values, one CIDR counts single value. Attempts exceed number return error 400 error_code value QUOTA_EXCEEDED. updated list block calling user's current IP, error 400 returned error_code value INVALID_STATE. can take minutes changes take effect. Note resulting IP access list effect enable feature. See :method:workspaceconf/setStatus.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_job.html","id":null,"dir":"Reference","previous_headings":"","what":"Update job settings partially. — update_job","title":"Update job settings partially. — update_job","text":"Add, update, remove specific settings existing job. Use Reset endpoint overwrite job settings.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_job.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update job settings partially. — update_job","text":"","code":"update_job(client, job_id, fields_to_remove = NULL, new_settings = NULL)  jobsUpdate(client, job_id, fields_to_remove = NULL, new_settings = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_job.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update job settings partially. — update_job","text":"client Required. Instance DatabricksClient() job_id Required. canonical identifier job update. fields_to_remove Remove top-level fields job settings. new_settings new settings job.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_job_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Update job permissions. — update_job_permissions","title":"Update job permissions. — update_job_permissions","text":"Updates permissions job. Jobs can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_job_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update job permissions. — update_job_permissions","text":"","code":"update_job_permissions(client, job_id, access_control_list = NULL)  jobsUpdatePermissions(client, job_id, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_job_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update job permissions. — update_job_permissions","text":"client Required. Instance DatabricksClient() job_id Required. job get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_lakehouse_monitor.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a table monitor. — update_lakehouse_monitor","title":"Update a table monitor. — update_lakehouse_monitor","text":"Updates monitor specified table.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_lakehouse_monitor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a table monitor. — update_lakehouse_monitor","text":"","code":"update_lakehouse_monitor(   client,   full_name,   output_schema_name,   baseline_table_name = NULL,   custom_metrics = NULL,   data_classification_config = NULL,   inference_log = NULL,   notifications = NULL,   schedule = NULL,   slicing_exprs = NULL,   snapshot = NULL,   time_series = NULL )  lakehouseMonitorsUpdate(   client,   full_name,   output_schema_name,   baseline_table_name = NULL,   custom_metrics = NULL,   data_classification_config = NULL,   inference_log = NULL,   notifications = NULL,   schedule = NULL,   slicing_exprs = NULL,   snapshot = NULL,   time_series = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_lakehouse_monitor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a table monitor. — update_lakehouse_monitor","text":"client Required. Instance DatabricksClient() full_name Required. Full name table. output_schema_name Required. Schema output metric tables created. baseline_table_name Name baseline table drift metrics computed . custom_metrics Custom metrics compute monitored table. data_classification_config data classification config monitor. inference_log Configuration monitoring inference logs. notifications notification settings monitor. schedule schedule automatically updating refreshing metric tables. slicing_exprs List column expressions slice data targeted analysis. snapshot Configuration monitoring snapshot tables. time_series Configuration monitoring time series tables.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_lakehouse_monitor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update a table monitor. — update_lakehouse_monitor","text":"caller must either: 1. owner table's parent catalog 2. USE_CATALOG table's parent catalog owner table's parent schema 3. following permissions: - USE_CATALOG table's parent catalog - USE_SCHEMA table's parent schema - owner table. Additionally, call must made workspace monitor created, caller must original creator monitor. Certain configuration fields, output asset identifiers, updated.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_lakeview.html","id":null,"dir":"Reference","previous_headings":"","what":"Update dashboard. — update_lakeview","title":"Update dashboard. — update_lakeview","text":"Update draft dashboard.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_lakeview.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update dashboard. — update_lakeview","text":"","code":"update_lakeview(   client,   dashboard_id,   display_name = NULL,   etag = NULL,   serialized_dashboard = NULL,   warehouse_id = NULL )  lakeviewUpdate(   client,   dashboard_id,   display_name = NULL,   etag = NULL,   serialized_dashboard = NULL,   warehouse_id = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_lakeview.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update dashboard. — update_lakeview","text":"client Required. Instance DatabricksClient() dashboard_id Required. UUID identifying dashboard. display_name display name dashboard. etag etag dashboard. serialized_dashboard contents dashboard serialized string form. warehouse_id warehouse ID used run dashboard.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_metastore.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a metastore. — update_metastore","title":"Update a metastore. — update_metastore","text":"Updates information specific metastore. caller must metastore admin. owner field set empty string (''), ownership updated System User.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_metastore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a metastore. — update_metastore","text":"","code":"update_metastore(   client,   id,   delta_sharing_organization_name = NULL,   delta_sharing_recipient_token_lifetime_in_seconds = NULL,   delta_sharing_scope = NULL,   new_name = NULL,   owner = NULL,   privilege_model_version = NULL,   storage_root_credential_id = NULL )  metastoresUpdate(   client,   id,   delta_sharing_organization_name = NULL,   delta_sharing_recipient_token_lifetime_in_seconds = NULL,   delta_sharing_scope = NULL,   new_name = NULL,   owner = NULL,   privilege_model_version = NULL,   storage_root_credential_id = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_metastore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a metastore. — update_metastore","text":"client Required. Instance DatabricksClient() id Required. Unique ID metastore. delta_sharing_organization_name organization name Delta Sharing entity, used Databricks--Databricks Delta Sharing official name. delta_sharing_recipient_token_lifetime_in_seconds lifetime delta sharing recipient token seconds. delta_sharing_scope scope Delta Sharing enabled metastore. new_name New name metastore. owner owner metastore. privilege_model_version Privilege model version metastore, form major.minor (e.g., 1.0). storage_root_credential_id UUID storage credential access metastore storage_root.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_metastore_assignment.html","id":null,"dir":"Reference","previous_headings":"","what":"Update an assignment. — update_metastore_assignment","title":"Update an assignment. — update_metastore_assignment","text":"Updates metastore assignment. operation can used update metastore_id default_catalog_name specified Workspace, Workspace already assigned metastore. caller must account admin update metastore_id; otherwise, caller can Workspace admin.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_metastore_assignment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update an assignment. — update_metastore_assignment","text":"","code":"update_metastore_assignment(   client,   workspace_id,   default_catalog_name = NULL,   metastore_id = NULL )  metastoresUpdateAssignment(   client,   workspace_id,   default_catalog_name = NULL,   metastore_id = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_metastore_assignment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update an assignment. — update_metastore_assignment","text":"client Required. Instance DatabricksClient() workspace_id Required. workspace ID. default_catalog_name name default catalog metastore. metastore_id unique ID metastore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Update model. — update_model","title":"Update model. — update_model","text":"Updates registered model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update model. — update_model","text":"","code":"update_model(client, name, description = NULL)  modelRegistryUpdateModel(client, name, description = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update model. — update_model","text":"client Required. Instance DatabricksClient() name Required. Registered model unique name identifier. description provided, updates description registered_model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model_comment.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a comment. — update_model_comment","title":"Update a comment. — update_model_comment","text":"Post edit comment model version.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model_comment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a comment. — update_model_comment","text":"","code":"update_model_comment(client, id, comment)  modelRegistryUpdateComment(client, id, comment)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model_comment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a comment. — update_model_comment","text":"client Required. Instance DatabricksClient() id Required. Unique identifier activity. comment Required. User-provided comment action.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Update registered model permissions. — update_model_permissions","title":"Update registered model permissions. — update_model_permissions","text":"Updates permissions registered model. Registered models can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update registered model permissions. — update_model_permissions","text":"","code":"update_model_permissions(   client,   registered_model_id,   access_control_list = NULL )  modelRegistryUpdatePermissions(   client,   registered_model_id,   access_control_list = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update registered model permissions. — update_model_permissions","text":"client Required. Instance DatabricksClient() registered_model_id Required. registered model get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model_version.html","id":null,"dir":"Reference","previous_headings":"","what":"Update model version. — update_model_version","title":"Update model version. — update_model_version","text":"Updates model version. Updates specified model version.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model_version.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update model version. — update_model_version","text":"","code":"update_model_version(client, full_name, version, comment = NULL)  modelRegistryUpdateModelVersion(client, name, version, description = NULL)  update_model_version(client, full_name, version, comment = NULL)  modelVersionsUpdate(client, full_name, version, comment = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model_version.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update model version. — update_model_version","text":"client Required. Instance DatabricksClient() full_name Required. three-level (fully qualified) name model version. version Required. integer version number model version. comment comment attached model version. name Required. Name registered model. description provided, updates description registered_model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model_version.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update model version. — update_model_version","text":"caller must metastore admin owner parent registered model. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema. Currently comment model version can updated.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model_webhook.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a webhook. — update_model_webhook","title":"Update a webhook. — update_model_webhook","text":"NOTE: endpoint Public Preview.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model_webhook.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a webhook. — update_model_webhook","text":"","code":"update_model_webhook(   client,   id,   description = NULL,   events = NULL,   http_url_spec = NULL,   job_spec = NULL,   status = NULL )  modelRegistryUpdateWebhook(   client,   id,   description = NULL,   events = NULL,   http_url_spec = NULL,   job_spec = NULL,   status = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model_webhook.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a webhook. — update_model_webhook","text":"client Required. Instance DatabricksClient() id Required. Webhook ID. description User-specified description webhook. events Events can trigger registry webhook: * MODEL_VERSION_CREATED: new model version created associated model. http_url_spec field description yet. job_spec field description yet. status Enable disable triggering webhook, put webhook test mode.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_model_webhook.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update a webhook. — update_model_webhook","text":"Updates registry webhook.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_notebook_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Update workspace object permissions. — update_notebook_permissions","title":"Update workspace object permissions. — update_notebook_permissions","text":"Updates permissions workspace object. Workspace objects can inherit permissions parent objects root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_notebook_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update workspace object permissions. — update_notebook_permissions","text":"","code":"update_notebook_permissions(   client,   workspace_object_type,   workspace_object_id,   access_control_list = NULL )  workspaceUpdatePermissions(   client,   workspace_object_type,   workspace_object_id,   access_control_list = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_notebook_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update workspace object permissions. — update_notebook_permissions","text":"client Required. Instance DatabricksClient() workspace_object_type Required. workspace object type get manage permissions. workspace_object_id Required. workspace object get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_permission.html","id":null,"dir":"Reference","previous_headings":"","what":"Update object permissions. — update_permission","title":"Update object permissions. — update_permission","text":"Updates permissions object. Objects can inherit permissions parent objects root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_permission.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update object permissions. — update_permission","text":"","code":"update_permission(   client,   request_object_type,   request_object_id,   access_control_list = NULL )  permissionsUpdate(   client,   request_object_type,   request_object_id,   access_control_list = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_permission.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update object permissions. — update_permission","text":"client Required. Instance DatabricksClient() request_object_type Required. type request object. request_object_id Required. id request object. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_pipeline.html","id":null,"dir":"Reference","previous_headings":"","what":"Edit a pipeline. — update_pipeline","title":"Edit a pipeline. — update_pipeline","text":"Updates pipeline supplied configuration.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_pipeline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Edit a pipeline. — update_pipeline","text":"","code":"update_pipeline(   client,   pipeline_id,   allow_duplicate_names = NULL,   catalog = NULL,   channel = NULL,   clusters = NULL,   configuration = NULL,   continuous = NULL,   development = NULL,   edition = NULL,   expected_last_modified = NULL,   filters = NULL,   id = NULL,   libraries = NULL,   name = NULL,   notifications = NULL,   photon = NULL,   serverless = NULL,   storage = NULL,   target = NULL,   trigger = NULL )  pipelinesUpdate(   client,   pipeline_id,   allow_duplicate_names = NULL,   catalog = NULL,   channel = NULL,   clusters = NULL,   configuration = NULL,   continuous = NULL,   development = NULL,   edition = NULL,   expected_last_modified = NULL,   filters = NULL,   id = NULL,   libraries = NULL,   name = NULL,   notifications = NULL,   photon = NULL,   serverless = NULL,   storage = NULL,   target = NULL,   trigger = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_pipeline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Edit a pipeline. — update_pipeline","text":"client Required. Instance DatabricksClient() pipeline_id Unique identifier pipeline. allow_duplicate_names false, deployment fail name changed conflicts name another pipeline. catalog catalog Unity Catalog publish data pipeline . channel DLT Release Channel specifies version use. clusters Cluster settings pipeline deployment. configuration String-String configuration pipeline execution. continuous Whether pipeline continuous triggered. development Whether pipeline Development mode. edition Pipeline product edition. expected_last_modified present, last-modified time pipeline settings edit. filters Filters Pipeline packages include deployed graph. id Unique identifier pipeline. libraries Libraries code needed deployment. name Friendly identifier pipeline. notifications List notification settings pipeline. photon Whether Photon enabled pipeline. serverless Whether serverless compute enabled pipeline. storage DBFS root directory storing checkpoints tables. target Target schema (database) add tables pipeline . trigger pipeline trigger use.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_pipeline_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Update pipeline permissions. — update_pipeline_permissions","title":"Update pipeline permissions. — update_pipeline_permissions","text":"Updates permissions pipeline. Pipelines can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_pipeline_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update pipeline permissions. — update_pipeline_permissions","text":"","code":"update_pipeline_permissions(client, pipeline_id, access_control_list = NULL)  pipelinesUpdatePermissions(client, pipeline_id, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_pipeline_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update pipeline permissions. — update_pipeline_permissions","text":"client Required. Instance DatabricksClient() pipeline_id Required. pipeline get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_provider.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a provider. — update_provider","title":"Update a provider. — update_provider","text":"Updates information authentication provider, caller metastore admin owner provider. update changes provider name, caller must metastore admin owner provider.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_provider.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a provider. — update_provider","text":"","code":"update_provider(   client,   name,   comment = NULL,   new_name = NULL,   owner = NULL,   recipient_profile_str = NULL )  providersUpdate(   client,   name,   comment = NULL,   new_name = NULL,   owner = NULL,   recipient_profile_str = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_provider.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a provider. — update_provider","text":"client Required. Instance DatabricksClient() name Required. Name provider. comment Description provider. new_name New name provider. owner Username Provider owner. recipient_profile_str field required authentication_type TOKEN provided.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Change a query definition. — update_query","title":"Change a query definition. — update_query","text":"Modify query definition.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Change a query definition. — update_query","text":"","code":"update_query(   client,   query_id,   data_source_id = NULL,   description = NULL,   name = NULL,   options = NULL,   query = NULL,   run_as_role = NULL )  queriesUpdate(   client,   query_id,   data_source_id = NULL,   description = NULL,   name = NULL,   options = NULL,   query = NULL,   run_as_role = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Change a query definition. — update_query","text":"client Required. Instance DatabricksClient() query_id Required. field description yet. data_source_id Data source ID maps ID data source used resource distinct warehouse ID. description General description conveys additional information query usage notes. name title query appears list views, widget headings, query page. options Exclusively used storing list parameter definitions. query text query run. run_as_role Sets Run role object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_query.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Change a query definition. — update_query","text":"Note: undo operation.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_query_visualization.html","id":null,"dir":"Reference","previous_headings":"","what":"Edit existing visualization. — update_query_visualization","title":"Edit existing visualization. — update_query_visualization","text":"Edit existing visualization.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_query_visualization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Edit existing visualization. — update_query_visualization","text":"","code":"update_query_visualization(   client,   id,   created_at = NULL,   description = NULL,   name = NULL,   options = NULL,   type = NULL,   updated_at = NULL )  queryVisualizationsUpdate(   client,   id,   created_at = NULL,   description = NULL,   name = NULL,   options = NULL,   type = NULL,   updated_at = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_query_visualization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Edit existing visualization. — update_query_visualization","text":"client Required. Instance DatabricksClient() id UUID visualization. created_at field description yet. description short description visualization. name name visualization appears dashboards query screen. options options object varies widely one visualization type next unsupported. type type visualization: chart, table, pivot table, . updated_at field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_recipient.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a share recipient. — update_recipient","title":"Update a share recipient. — update_recipient","text":"Updates existing recipient metastore. caller must metastore admin owner recipient. recipient name updated, user must metastore admin owner recipient.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_recipient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a share recipient. — update_recipient","text":"","code":"update_recipient(   client,   name,   comment = NULL,   ip_access_list = NULL,   new_name = NULL,   owner = NULL,   properties_kvpairs = NULL )  recipientsUpdate(   client,   name,   comment = NULL,   ip_access_list = NULL,   new_name = NULL,   owner = NULL,   properties_kvpairs = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_recipient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a share recipient. — update_recipient","text":"client Required. Instance DatabricksClient() name Required. Name recipient. comment Description recipient. ip_access_list IP Access List. new_name New name recipient. owner Username recipient owner. properties_kvpairs Recipient properties map string key-value pairs.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_registered_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a Registered Model. — update_registered_model","title":"Update a Registered Model. — update_registered_model","text":"Updates specified registered model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_registered_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a Registered Model. — update_registered_model","text":"","code":"update_registered_model(   client,   full_name,   comment = NULL,   new_name = NULL,   owner = NULL )  registeredModelsUpdate(   client,   full_name,   comment = NULL,   new_name = NULL,   owner = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_registered_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a Registered Model. — update_registered_model","text":"client Required. Instance DatabricksClient() full_name Required. three-level (fully qualified) name registered model. comment comment attached registered model. new_name New name registered model. owner identifier user owns registered model.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_registered_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update a Registered Model. — update_registered_model","text":"caller must metastore admin owner registered model. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema. Currently name, owner comment registered model can updated.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_repo.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a repo. — update_repo","title":"Update a repo. — update_repo","text":"Updates repo different branch tag, updates repo latest commit branch.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_repo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a repo. — update_repo","text":"","code":"update_repo(client, repo_id, branch = NULL, sparse_checkout = NULL, tag = NULL)  reposUpdate(client, repo_id, branch = NULL, sparse_checkout = NULL, tag = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_repo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a repo. — update_repo","text":"client Required. Instance DatabricksClient() repo_id Required. ID corresponding repo access. branch Branch local version repo checked . sparse_checkout specified, update sparse checkout settings. tag Tag local version repo checked .","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_repo_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Update repo permissions. — update_repo_permissions","title":"Update repo permissions. — update_repo_permissions","text":"Updates permissions repo. Repos can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_repo_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update repo permissions. — update_repo_permissions","text":"","code":"update_repo_permissions(client, repo_id, access_control_list = NULL)  reposUpdatePermissions(client, repo_id, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_repo_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update repo permissions. — update_repo_permissions","text":"client Required. Instance DatabricksClient() repo_id Required. repo get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_restrict_workspace_admin.html","id":null,"dir":"Reference","previous_headings":"","what":"Update the restrict workspace admins setting. — update_restrict_workspace_admin","title":"Update the restrict workspace admins setting. — update_restrict_workspace_admin","text":"Updates restrict workspace admins setting workspace. fresh etag needs provided PATCH requests (part setting field). etag can retrieved making GET request PATCH request. setting updated concurrently, PATCH fails 409 request must retried using fresh etag 409 response.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_restrict_workspace_admin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update the restrict workspace admins setting. — update_restrict_workspace_admin","text":"","code":"update_restrict_workspace_admin(client, allow_missing, setting, field_mask)  restrictWorkspaceAdminsUpdate(client, allow_missing, setting, field_mask)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_restrict_workspace_admin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update the restrict workspace admins setting. — update_restrict_workspace_admin","text":"client Required. Instance DatabricksClient() allow_missing Required. always set true Settings API. setting Required. field description yet. field_mask Required. Field mask required passed PATCH request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a schema. — update_schema","title":"Update a schema. — update_schema","text":"Updates schema catalog. caller must owner schema metastore admin. caller metastore admin, owner field can changed update. name field must updated, caller must metastore admin CREATE_SCHEMA privilege parent catalog.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a schema. — update_schema","text":"","code":"update_schema(   client,   full_name,   comment = NULL,   enable_predictive_optimization = NULL,   new_name = NULL,   owner = NULL,   properties = NULL )  schemasUpdate(   client,   full_name,   comment = NULL,   enable_predictive_optimization = NULL,   new_name = NULL,   owner = NULL,   properties = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a schema. — update_schema","text":"client Required. Instance DatabricksClient() full_name Required. Full name schema. comment User-provided free-form text description. enable_predictive_optimization Whether predictive optimization enabled object objects . new_name New name schema. owner Username current owner schema. properties map key-value properties attached securable.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_service_principal.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace service principal. — update_service_principal","title":"Replace service principal. — update_service_principal","text":"Updates details single service principal.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_service_principal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace service principal. — update_service_principal","text":"","code":"update_service_principal(   client,   id,   active = NULL,   application_id = NULL,   display_name = NULL,   entitlements = NULL,   external_id = NULL,   groups = NULL,   roles = NULL,   schemas = NULL )  servicePrincipalsUpdate(   client,   id,   active = NULL,   application_id = NULL,   display_name = NULL,   entitlements = NULL,   external_id = NULL,   groups = NULL,   roles = NULL,   schemas = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_service_principal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace service principal. — update_service_principal","text":"client Required. Instance DatabricksClient() id Databricks service principal ID. active user active. application_id UUID relating service principal. display_name String represents concatenation given family names. entitlements Entitlements assigned service principal. external_id field description yet. groups field description yet. roles Corresponds AWS instance profile/arn role. schemas schema List response.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_service_principal.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Replace service principal. — update_service_principal","text":"action replaces existing service principal name.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_serving_endpoint_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Update config of a serving endpoint. — update_serving_endpoint_config","title":"Update config of a serving endpoint. — update_serving_endpoint_config","text":"Updates combination serving endpoint's served entities, compute configuration served entities, endpoint's traffic config. endpoint already update progress can updated current update completes fails.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_serving_endpoint_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update config of a serving endpoint. — update_serving_endpoint_config","text":"","code":"update_serving_endpoint_config(   client,   name,   auto_capture_config = NULL,   served_entities = NULL,   served_models = NULL,   traffic_config = NULL )  servingEndpointsUpdateConfig(   client,   name,   auto_capture_config = NULL,   served_entities = NULL,   served_models = NULL,   traffic_config = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_serving_endpoint_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update config of a serving endpoint. — update_serving_endpoint_config","text":"client Required. Instance DatabricksClient() name Required. name serving endpoint update. auto_capture_config Configuration Inference Tables automatically logs requests responses Unity Catalog. served_entities list served entities endpoint serve. served_models (Deprecated, use served_entities instead) list served models endpoint serve. traffic_config traffic config defining invocations serving endpoint routed.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_serving_endpoint_config_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Update config of a serving endpoint. — update_serving_endpoint_config_and_wait","title":"Update config of a serving endpoint. — update_serving_endpoint_config_and_wait","text":"long-running operation, blocks Serving Endpoints Databricks reach NOT_UPDATING state timeout 20 minutes, can change via timeout parameter. default, state Databricks Serving Endpoints reported console. can change behavior changing callback parameter.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_serving_endpoint_config_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update config of a serving endpoint. — update_serving_endpoint_config_and_wait","text":"","code":"update_serving_endpoint_config_and_wait(   client,   name,   auto_capture_config = NULL,   served_entities = NULL,   served_models = NULL,   traffic_config = NULL,   timeout = 20,   callback = cli_reporter )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_serving_endpoint_config_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update config of a serving endpoint. — update_serving_endpoint_config_and_wait","text":"client Required. Instance DatabricksClient() name Required. name serving endpoint update. auto_capture_config Configuration Inference Tables automatically logs requests responses Unity Catalog. served_entities list served entities endpoint serve. served_models (Deprecated, use served_entities instead) list served models endpoint serve. traffic_config traffic config defining invocations serving endpoint routed. timeout Time wait operation complete minutes. callback Function report status operation. default, reports console.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_serving_endpoint_config_and_wait.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update config of a serving endpoint. — update_serving_endpoint_config_and_wait","text":"Updates combination serving endpoint's served entities, compute configuration served entities, endpoint's traffic config. endpoint already update progress can updated current update completes fails.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_serving_endpoint_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Update serving endpoint permissions. — update_serving_endpoint_permissions","title":"Update serving endpoint permissions. — update_serving_endpoint_permissions","text":"Updates permissions serving endpoint. Serving endpoints can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_serving_endpoint_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update serving endpoint permissions. — update_serving_endpoint_permissions","text":"","code":"update_serving_endpoint_permissions(   client,   serving_endpoint_id,   access_control_list = NULL )  servingEndpointsUpdatePermissions(   client,   serving_endpoint_id,   access_control_list = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_serving_endpoint_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update serving endpoint permissions. — update_serving_endpoint_permissions","text":"client Required. Instance DatabricksClient() serving_endpoint_id Required. serving endpoint get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_share.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a share. — update_share","title":"Update a share. — update_share","text":"Updates share changes data objects request. caller must owner share metastore admin.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_share.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a share. — update_share","text":"","code":"update_share(   client,   name,   comment = NULL,   new_name = NULL,   owner = NULL,   updates = NULL )  sharesUpdate(   client,   name,   comment = NULL,   new_name = NULL,   owner = NULL,   updates = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_share.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a share. — update_share","text":"client Required. Instance DatabricksClient() name Required. name share. comment User-provided free-form text description. new_name New name share. owner Username current owner share. updates Array shared data object updates.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_share.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update a share. — update_share","text":"caller metastore admin, owner field can updated. case share name changed, updateShare requires caller share owner metastore admin. table added method, share owner must also SELECT privilege table. privilege must maintained indefinitely recipients able access table. Typically, use group share owner. Table removals update require additional privileges.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_share_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Update permissions. — update_share_permissions","title":"Update permissions. — update_share_permissions","text":"Updates permissions data share metastore. caller must metastore admin owner share.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_share_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update permissions. — update_share_permissions","text":"","code":"update_share_permissions(client, name, changes = NULL)  sharesUpdatePermissions(client, name, changes = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_share_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update permissions. — update_share_permissions","text":"client Required. Instance DatabricksClient() name Required. name share. changes Array permission changes.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_share_permissions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update permissions. — update_share_permissions","text":"new recipient grants, user must also owner recipients. recipient revocations require additional privileges.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_storage_credential.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a credential. — update_storage_credential","title":"Update a credential. — update_storage_credential","text":"Updates storage credential metastore.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_storage_credential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a credential. — update_storage_credential","text":"","code":"update_storage_credential(   client,   name,   aws_iam_role = NULL,   azure_managed_identity = NULL,   azure_service_principal = NULL,   cloudflare_api_token = NULL,   comment = NULL,   databricks_gcp_service_account = NULL,   force = NULL,   new_name = NULL,   owner = NULL,   read_only = NULL,   skip_validation = NULL )  storageCredentialsUpdate(   client,   name,   aws_iam_role = NULL,   azure_managed_identity = NULL,   azure_service_principal = NULL,   cloudflare_api_token = NULL,   comment = NULL,   databricks_gcp_service_account = NULL,   force = NULL,   new_name = NULL,   owner = NULL,   read_only = NULL,   skip_validation = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_storage_credential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a credential. — update_storage_credential","text":"client Required. Instance DatabricksClient() name Required. Name storage credential. aws_iam_role AWS IAM role configuration. azure_managed_identity Azure managed identity configuration. azure_service_principal Azure service principal configuration. cloudflare_api_token Cloudflare API token configuration. comment Comment associated credential. databricks_gcp_service_account Databricks managed GCP service account configuration. force Force update even dependent external locations external tables. new_name New name storage credential. owner Username current owner credential. read_only Whether storage credential usable read operations. skip_validation Supplying true argument skips validation updated credential.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a table owner. — update_table","title":"Update a table owner. — update_table","text":"Change owner table. caller must owner parent catalog, USE_CATALOG privilege parent catalog owner parent schema, owner table USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a table owner. — update_table","text":"","code":"update_table(client, full_name, owner = NULL)  tablesUpdate(client, full_name, owner = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a table owner. — update_table","text":"client Required. Instance DatabricksClient() full_name Required. Full name table. owner field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_token_management_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Update token permissions. — update_token_management_permissions","title":"Update token permissions. — update_token_management_permissions","text":"Updates permissions tokens. Tokens can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_token_management_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update token permissions. — update_token_management_permissions","text":"","code":"update_token_management_permissions(client, access_control_list = NULL)  tokenManagementUpdatePermissions(client, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_token_management_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update token permissions. — update_token_management_permissions","text":"client Required. Instance DatabricksClient() access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_user.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace a user. — update_user","title":"Replace a user. — update_user","text":"Replaces user's information data supplied request.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_user.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace a user. — update_user","text":"","code":"update_user(   client,   id,   active = NULL,   display_name = NULL,   emails = NULL,   entitlements = NULL,   external_id = NULL,   groups = NULL,   name = NULL,   roles = NULL,   schemas = NULL,   user_name = NULL )  usersUpdate(   client,   id,   active = NULL,   display_name = NULL,   emails = NULL,   entitlements = NULL,   external_id = NULL,   groups = NULL,   name = NULL,   roles = NULL,   schemas = NULL,   user_name = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_user.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace a user. — update_user","text":"client Required. Instance DatabricksClient() id Databricks user ID. active user active. display_name String represents concatenation given family names. emails emails associated Databricks user. entitlements Entitlements assigned user. external_id External ID currently supported. groups field description yet. name field description yet. roles Corresponds AWS instance profile/arn role. schemas schema user. user_name Email address Databricks user.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_user_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Update password permissions. — update_user_permissions","title":"Update password permissions. — update_user_permissions","text":"Updates permissions passwords. Passwords can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_user_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update password permissions. — update_user_permissions","text":"","code":"update_user_permissions(client, access_control_list = NULL)  usersUpdatePermissions(client, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_user_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update password permissions. — update_user_permissions","text":"client Required. Instance DatabricksClient() access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_volume.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a Volume. — update_volume","title":"Update a Volume. — update_volume","text":"Updates specified volume specified parent catalog schema.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_volume.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a Volume. — update_volume","text":"","code":"update_volume(client, name, comment = NULL, new_name = NULL, owner = NULL)  volumesUpdate(client, name, comment = NULL, new_name = NULL, owner = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_volume.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a Volume. — update_volume","text":"client Required. Instance DatabricksClient() name Required. three-level (fully qualified) name volume. comment comment attached volume. new_name New name volume. owner identifier user owns volume.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_volume.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update a Volume. — update_volume","text":"caller must metastore admin owner volume. latter case, caller must also owner USE_CATALOG privilege parent catalog USE_SCHEMA privilege parent schema. Currently name, owner comment volume updated.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_warehouse_permissions.html","id":null,"dir":"Reference","previous_headings":"","what":"Update SQL warehouse permissions. — update_warehouse_permissions","title":"Update SQL warehouse permissions. — update_warehouse_permissions","text":"Updates permissions SQL warehouse. SQL warehouses can inherit permissions root object.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_warehouse_permissions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update SQL warehouse permissions. — update_warehouse_permissions","text":"","code":"update_warehouse_permissions(client, warehouse_id, access_control_list = NULL)  warehousesUpdatePermissions(client, warehouse_id, access_control_list = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_warehouse_permissions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update SQL warehouse permissions. — update_warehouse_permissions","text":"client Required. Instance DatabricksClient() warehouse_id Required. SQL warehouse get manage permissions. access_control_list field description yet.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_workspace_binding.html","id":null,"dir":"Reference","previous_headings":"","what":"Update catalog workspace bindings. — update_workspace_binding","title":"Update catalog workspace bindings. — update_workspace_binding","text":"Updates workspace bindings catalog. caller must metastore admin owner catalog.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_workspace_binding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update catalog workspace bindings. — update_workspace_binding","text":"","code":"update_workspace_binding(   client,   name,   assign_workspaces = NULL,   unassign_workspaces = NULL )  workspaceBindingsUpdate(   client,   name,   assign_workspaces = NULL,   unassign_workspaces = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_workspace_binding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update catalog workspace bindings. — update_workspace_binding","text":"client Required. Instance DatabricksClient() name Required. name catalog. assign_workspaces list workspace IDs. unassign_workspaces list workspace IDs.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_workspace_binding_bindings.html","id":null,"dir":"Reference","previous_headings":"","what":"Update securable workspace bindings. — update_workspace_binding_bindings","title":"Update securable workspace bindings. — update_workspace_binding_bindings","text":"Updates workspace bindings securable. caller must metastore admin owner securable.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_workspace_binding_bindings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update securable workspace bindings. — update_workspace_binding_bindings","text":"","code":"update_workspace_binding_bindings(   client,   securable_type,   securable_name,   add = NULL,   remove = NULL )  workspaceBindingsUpdateBindings(   client,   securable_type,   securable_name,   add = NULL,   remove = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/update_workspace_binding_bindings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update securable workspace bindings. — update_workspace_binding_bindings","text":"client Required. Instance DatabricksClient() securable_type Required. type securable. securable_name Required. name securable. add List workspace bindings. remove List workspace bindings.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/upload_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a file. — upload_file","title":"Upload a file. — upload_file","text":"Uploads file 5 GiB. file contents sent request body raw bytes (octet stream); encode otherwise modify bytes sending. contents resulting file exactly bytes sent request body. request successful, response body.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/upload_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a file. — upload_file","text":"","code":"upload_file(client, file_path, contents, overwrite = NULL)  filesUpload(client, file_path, contents, overwrite = NULL)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/upload_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a file. — upload_file","text":"client Required. Instance DatabricksClient() file_path Required. absolute path file. contents field description yet. overwrite true, existing file overwritten.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/upsert_vector_search_index_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Upsert data into an index. — upsert_vector_search_index_data","title":"Upsert data into an index. — upsert_vector_search_index_data","text":"Handles upserting data specified vector index.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/upsert_vector_search_index_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upsert data into an index. — upsert_vector_search_index_data","text":"","code":"upsert_vector_search_index_data(client, index_name, inputs_json)  vectorSearchIndexesUpsertDataVectorIndex(client, index_name, inputs_json)"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/upsert_vector_search_index_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upsert data into an index. — upsert_vector_search_index_data","text":"client Required. Instance DatabricksClient() index_name Required. Name vector index data upserted. inputs_json Required. JSON string representing data upserted.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/validate_storage_credential.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate a storage credential. — validate_storage_credential","title":"Validate a storage credential. — validate_storage_credential","text":"Validates storage credential. least one external_location_name url need provided. one provided, used validation. provided, url used validation, external_location_name ignored checking overlapping urls.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/validate_storage_credential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate a storage credential. — validate_storage_credential","text":"","code":"validate_storage_credential(   client,   aws_iam_role = NULL,   azure_managed_identity = NULL,   azure_service_principal = NULL,   cloudflare_api_token = NULL,   databricks_gcp_service_account = NULL,   external_location_name = NULL,   read_only = NULL,   storage_credential_name = NULL,   url = NULL )  storageCredentialsValidate(   client,   aws_iam_role = NULL,   azure_managed_identity = NULL,   azure_service_principal = NULL,   cloudflare_api_token = NULL,   databricks_gcp_service_account = NULL,   external_location_name = NULL,   read_only = NULL,   storage_credential_name = NULL,   url = NULL )"},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/validate_storage_credential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate a storage credential. — validate_storage_credential","text":"client Required. Instance DatabricksClient() aws_iam_role AWS IAM role configuration. azure_managed_identity Azure managed identity configuration. azure_service_principal Azure service principal configuration. cloudflare_api_token Cloudflare API token configuration. databricks_gcp_service_account Databricks created GCP service account configuration. external_location_name name existing external location validate. read_only Whether storage credential usable read operations. storage_credential_name name storage credential validate. url external location url validate.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/reference/validate_storage_credential.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Validate a storage credential. — validate_storage_credential","text":"Either storage_credential_name cloud-specific credential must provided. caller must metastore admin storage credential owner CREATE_EXTERNAL_LOCATION privilege metastore storage credential.","code":""},{"path":"https://databrickslabs.github.io/databricks-sdk-r/news/index.html","id":"databricks-042","dir":"Changelog","previous_headings":"","what":"databricks 0.4.2","title":"databricks 0.4.2","text":"Initial CRAN submission.","code":""}]
