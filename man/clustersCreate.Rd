% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/clusters.R
\name{clustersCreate}
\alias{clustersCreate}
\title{Create new cluster.}
\usage{
clustersCreate(
  client,
  spark_version,
  apply_policy_default_values = NULL,
  autoscale = NULL,
  autotermination_minutes = NULL,
  aws_attributes = NULL,
  azure_attributes = NULL,
  cluster_log_conf = NULL,
  cluster_name = NULL,
  cluster_source = NULL,
  custom_tags = NULL,
  data_security_mode = NULL,
  docker_image = NULL,
  driver_instance_pool_id = NULL,
  driver_node_type_id = NULL,
  enable_elastic_disk = NULL,
  enable_local_disk_encryption = NULL,
  gcp_attributes = NULL,
  init_scripts = NULL,
  instance_pool_id = NULL,
  node_type_id = NULL,
  num_workers = NULL,
  policy_id = NULL,
  runtime_engine = NULL,
  single_user_name = NULL,
  spark_conf = NULL,
  spark_env_vars = NULL,
  ssh_public_keys = NULL,
  workload_type = NULL,
  timeout = 20,
  callback = cli_reporter
)
}
\arguments{
\item{client}{Required. Instance of DatabricksClient()}

\item{spark_version}{Required. The Spark version of the cluster, e.g.}

\item{apply_policy_default_values}{}

\item{autoscale}{Parameters needed in order to automatically scale clusters up and down based on load.}

\item{autotermination_minutes}{Automatically terminates the cluster after it is inactive for this time in minutes.}

\item{aws_attributes}{Attributes related to clusters running on Amazon Web Services.}

\item{azure_attributes}{Attributes related to clusters running on Microsoft Azure.}

\item{cluster_log_conf}{The configuration for delivering spark logs to a long-term storage destination.}

\item{cluster_name}{Cluster name requested by the user.}

\item{cluster_source}{Determines whether the cluster was created by a user through the UI, created by the Databricks Jobs Scheduler, or through an API request.}

\item{custom_tags}{Additional tags for cluster resources.}

\item{data_security_mode}{Data security mode decides what data governance model to use when accessing data from a cluster.}

\item{docker_image}{}

\item{driver_instance_pool_id}{The optional ID of the instance pool for the driver of the cluster belongs.}

\item{driver_node_type_id}{The node type of the Spark driver.}

\item{enable_elastic_disk}{Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space.}

\item{enable_local_disk_encryption}{Whether to enable LUKS on cluster VMs' local disks.}

\item{gcp_attributes}{Attributes related to clusters running on Google Cloud Platform.}

\item{init_scripts}{The configuration for storing init scripts.}

\item{instance_pool_id}{The optional ID of the instance pool to which the cluster belongs.}

\item{node_type_id}{This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster.}

\item{num_workers}{Number of worker nodes that this cluster should have.}

\item{policy_id}{The ID of the cluster policy used to create the cluster if applicable.}

\item{runtime_engine}{Decides which runtime engine to be use, e.g.}

\item{single_user_name}{Single user name if data_security_mode is \code{SINGLE_USER}.}

\item{spark_conf}{An object containing a set of optional, user-specified Spark configuration key-value pairs.}

\item{spark_env_vars}{An object containing a set of optional, user-specified environment variable key-value pairs.}

\item{ssh_public_keys}{SSH public key contents that will be added to each Spark node in this cluster.}

\item{workload_type}{}

\item{timeout}{Time to wait for the operation to complete in minutes.}

\item{callback}{Function to report the status of the operation. By default, it reports to console.}
}
\description{
This is a long-running operation, which blocks until Clusters on Databricks reach
RUNNING state with the timeout of 20 minutes, that you can change via \code{timeout} parameter.
By default, the state of Databricks Clusters is reported to console. You can change this behavior
by changing the \code{callback} parameter.
}
\details{
Creates a new Spark cluster. This method will acquire new instances from the
cloud provider if necessary. Note: Databricks may not be able to acquire some
of the requested nodes, due to cloud provider limitations (account limits,
spot price, etc.) or transient network issues.

If Databricks acquires at least 85\% of the requested on-demand nodes, cluster
creation will succeed. Otherwise the cluster will terminate with an
informative error message.
}
