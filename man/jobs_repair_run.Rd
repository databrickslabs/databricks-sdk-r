% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/jobs.R
\name{jobs_repair_run}
\alias{jobs_repair_run}
\title{Repair a job run.}
\usage{
jobs_repair_run(
  run_id,
  dbt_commands = NULL,
  jar_params = NULL,
  latest_repair_id = NULL,
  notebook_params = NULL,
  pipeline_params = NULL,
  python_named_params = NULL,
  python_params = NULL,
  rerun_all_failed_tasks = NULL,
  rerun_tasks = NULL,
  spark_submit_params = NULL,
  sql_params = NULL,
  timeout = 20,
  callback = cli_reporter,
  ...
)
}
\arguments{
\item{run_id}{\link{required} The job run ID of the run to repair.}

\item{dbt_commands}{An array of commands to execute for jobs with the dbt task, for example \verb{'dbt_commands': ['dbt deps', 'dbt seed', 'dbt run']}.}

\item{jar_params}{A list of parameters for jobs with Spark JAR tasks, for example \verb{\\'jar_params\\': [\\'john doe\\', \\'35\\']}.}

\item{latest_repair_id}{The ID of the latest repair.}

\item{notebook_params}{A map from keys to values for jobs with notebook task, for example \verb{\\'notebook_params\\': \{\\'name\\': \\'john doe\\', \\'age\\': \\'35\\'\}}.}

\item{pipeline_params}{}

\item{python_named_params}{A map from keys to values for jobs with Python wheel task, for example \verb{'python_named_params': \{'name': 'task', 'data': 'dbfs:/path/to/data.json'\}}.}

\item{python_params}{A list of parameters for jobs with Python tasks, for example \verb{\\'python_params\\': [\\'john doe\\', \\'35\\']}.}

\item{rerun_all_failed_tasks}{If true, repair all failed tasks.}

\item{rerun_tasks}{The task keys of the task runs to repair.}

\item{spark_submit_params}{A list of parameters for jobs with spark submit task, for example \verb{\\'spark_submit_params\\': [\\'--class\\', \\'org.apache.spark.examples.SparkPi\\']}.}

\item{sql_params}{A map from keys to values for jobs with SQL task, for example \verb{'sql_params': \{'name': 'john doe', 'age': '35'\}}.}
}
\description{
This is a long-running operation, which blocks until Jobs on Databricks reach
TERMINATED or SKIPPED state with the timeout of 20 minutes, that you can change via \code{timeout} parameter.
By default, the state of Databricks Jobs is reported to console. You can change this behavior
by changing the \code{callback} parameter.
}
\details{
Re-run one or more tasks. Tasks are re-run as part of the original job run.
They use the current job and task settings, and can be viewed in the history
for the original job run.
}
\keyword{internal}
