% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/clusters.R
\name{edit_cluster}
\alias{edit_cluster}
\title{Update cluster configuration.}
\usage{
edit_cluster(
  client,
  cluster_id,
  spark_version,
  apply_policy_default_values = NULL,
  autoscale = NULL,
  autotermination_minutes = NULL,
  aws_attributes = NULL,
  azure_attributes = NULL,
  cluster_log_conf = NULL,
  cluster_name = NULL,
  cluster_source = NULL,
  custom_tags = NULL,
  data_security_mode = NULL,
  docker_image = NULL,
  driver_instance_pool_id = NULL,
  driver_node_type_id = NULL,
  enable_elastic_disk = NULL,
  enable_local_disk_encryption = NULL,
  gcp_attributes = NULL,
  init_scripts = NULL,
  instance_pool_id = NULL,
  node_type_id = NULL,
  num_workers = NULL,
  policy_id = NULL,
  runtime_engine = NULL,
  single_user_name = NULL,
  spark_conf = NULL,
  spark_env_vars = NULL,
  ssh_public_keys = NULL,
  workload_type = NULL
)
}
\arguments{
\item{client}{Required. Instance of DatabricksClient()}

\item{cluster_id}{Required. ID of the cluser.}

\item{spark_version}{Required. The Spark version of the cluster, e.g.}

\item{apply_policy_default_values}{This field has no description yet.}

\item{autoscale}{Parameters needed in order to automatically scale clusters up and down based on load.}

\item{autotermination_minutes}{Automatically terminates the cluster after it is inactive for this time in minutes.}

\item{aws_attributes}{Attributes related to clusters running on Amazon Web Services.}

\item{azure_attributes}{Attributes related to clusters running on Microsoft Azure.}

\item{cluster_log_conf}{The configuration for delivering spark logs to a long-term storage destination.}

\item{cluster_name}{Cluster name requested by the user.}

\item{cluster_source}{Determines whether the cluster was created by a user through the UI, created by the Databricks Jobs Scheduler, or through an API request.}

\item{custom_tags}{Additional tags for cluster resources.}

\item{data_security_mode}{Data security mode decides what data governance model to use when accessing data from a cluster.}

\item{docker_image}{This field has no description yet.}

\item{driver_instance_pool_id}{The optional ID of the instance pool for the driver of the cluster belongs.}

\item{driver_node_type_id}{The node type of the Spark driver.}

\item{enable_elastic_disk}{Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space.}

\item{enable_local_disk_encryption}{Whether to enable LUKS on cluster VMs' local disks.}

\item{gcp_attributes}{Attributes related to clusters running on Google Cloud Platform.}

\item{init_scripts}{The configuration for storing init scripts.}

\item{instance_pool_id}{The optional ID of the instance pool to which the cluster belongs.}

\item{node_type_id}{This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster.}

\item{num_workers}{Number of worker nodes that this cluster should have.}

\item{policy_id}{The ID of the cluster policy used to create the cluster if applicable.}

\item{runtime_engine}{Decides which runtime engine to be use, e.g.}

\item{single_user_name}{Single user name if data_security_mode is \code{SINGLE_USER}.}

\item{spark_conf}{An object containing a set of optional, user-specified Spark configuration key-value pairs.}

\item{spark_env_vars}{An object containing a set of optional, user-specified environment variable key-value pairs.}

\item{ssh_public_keys}{SSH public key contents that will be added to each Spark node in this cluster.}

\item{workload_type}{This field has no description yet.}
}
\description{
Updates the configuration of a cluster to match the provided attributes and
size. A cluster can be updated if it is in a \code{RUNNING} or \code{TERMINATED} state.
}
\details{
If a cluster is updated while in a \code{RUNNING} state, it will be restarted so
that the new attributes can take effect.

If a cluster is updated while in a \code{TERMINATED} state, it will remain
\code{TERMINATED}. The next time it is started using the \code{clusters/start} API, the
new attributes will take effect. Any attempt to update a cluster in any other
state will be rejected with an \code{INVALID_STATE} error code.

Clusters created by the Databricks Jobs service cannot be edited.
}
