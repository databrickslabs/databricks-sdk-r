% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/clusters.R
\name{clusters_create}
\alias{clusters_create}
\title{Create new cluster.}
\usage{
clusters_create(
  spark_version,
  apply_policy_default_values = NULL,
  autoscale = NULL,
  autotermination_minutes = NULL,
  aws_attributes = NULL,
  azure_attributes = NULL,
  cluster_log_conf = NULL,
  cluster_name = NULL,
  cluster_source = NULL,
  custom_tags = NULL,
  driver_instance_pool_id = NULL,
  driver_node_type_id = NULL,
  enable_elastic_disk = NULL,
  enable_local_disk_encryption = NULL,
  gcp_attributes = NULL,
  instance_pool_id = NULL,
  node_type_id = NULL,
  num_workers = NULL,
  policy_id = NULL,
  runtime_engine = NULL,
  spark_conf = NULL,
  spark_env_vars = NULL,
  ssh_public_keys = NULL,
  workload_type = NULL,
  timeout = 20,
  callback = cli_reporter,
  ...
)
}
\arguments{
\item{spark_version}{Required. The Spark version of the cluster, e.g.}

\item{apply_policy_default_values}{Note: This field won't be true for webapp requests.}

\item{autoscale}{Parameters needed in order to automatically scale clusters up and down based on load.}

\item{autotermination_minutes}{Automatically terminates the cluster after it is inactive for this time in minutes.}

\item{aws_attributes}{Attributes related to clusters running on Amazon Web Services.}

\item{azure_attributes}{Attributes related to clusters running on Microsoft Azure.}

\item{cluster_log_conf}{The configuration for delivering spark logs to a long-term storage destination.}

\item{cluster_name}{Cluster name requested by the user.}

\item{cluster_source}{Determines whether the cluster was created by a user through the UI, created by the Databricks Jobs Scheduler, or through an API request.}

\item{custom_tags}{Additional tags for cluster resources.}

\item{driver_instance_pool_id}{The optional ID of the instance pool for the driver of the cluster belongs.}

\item{driver_node_type_id}{The node type of the Spark driver.}

\item{enable_elastic_disk}{Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space.}

\item{enable_local_disk_encryption}{Whether to enable LUKS on cluster VMs' local disks.}

\item{gcp_attributes}{Attributes related to clusters running on Google Cloud Platform.}

\item{instance_pool_id}{The optional ID of the instance pool to which the cluster belongs.}

\item{node_type_id}{This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster.}

\item{num_workers}{Number of worker nodes that this cluster should have.}

\item{policy_id}{The ID of the cluster policy used to create the cluster if applicable.}

\item{runtime_engine}{Decides which runtime engine to be use, e.g.}

\item{spark_conf}{An object containing a set of optional, user-specified Spark configuration key-value pairs.}

\item{spark_env_vars}{An object containing a set of optional, user-specified environment variable key-value pairs.}

\item{ssh_public_keys}{SSH public key contents that will be added to each Spark node in this cluster.}

\item{workload_type}{}
}
\description{
This is a long-running operation, which blocks until Clusters on Databricks reach
RUNNING state with the timeout of 20 minutes, that you can change via \code{timeout} parameter.
By default, the state of Databricks Clusters is reported to console. You can change this behavior
by changing the \code{callback} parameter.
}
\details{
Creates a new Spark cluster. This method will acquire new instances from the
cloud provider if necessary. This method is asynchronous; the returned
\code{cluster_id} can be used to poll the cluster status. When this method
returns, the cluster will be in a \code{PENDING} state. The cluster will be usable
once it enters a \code{RUNNING} state.

Note: Databricks may not be able to acquire some of the requested nodes, due
to cloud provider limitations (account limits, spot price, etc.) or transient
network issues.

If Databricks acquires at least 85\% of the requested on-demand nodes, cluster
creation will succeed. Otherwise the cluster will terminate with an
informative error message.
}
\keyword{internal}
